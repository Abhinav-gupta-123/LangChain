{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABE4AAAHZCAYAAACYZ37QAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAP+lSURBVHhe7J0FYB1V1sf/cbdKUncXqkBxd5eFXRwWl8XWgP2wxdnFfXFKodAibYFSd3f31Ju0jbvOd/533s2bvCZtqJG25we3b+a6zeSeM1eCzrzgAgeGIDjG8Mo15iYAxzjsiglTg//fjkTCiGxcEvF+ibYWaimOUs8Jko7I/xz+5+t41fuJ7c2BmJ7qXipHHt4u8Xt0A/bboGC5+D374P58o7IcEp8pzv6M93DE1s+e254+a6pNhtzXnsN49zWOQ5G61/7hiFv6SqemXuXH9K8gfw3xT+uRWV+HB2xtO2bniMiibXqwYAuwtm2N83f3z+Dhgb+cHKO7/3LEvmvfq0ludDzvKe/76LfCvn8k1Layf7E9zvZgL8HSH4OefO55j23dOqjjVPqu/ByoP7Amc6bz1yYI10xdfTLPNT24Sn3FtJh76YFNuKtSzztUMD7E0EYEV3upHFlIu9ePZqfyhDA37IwHC/sc7B3eAU119GHaPd56r72urK/afOxrLXvjP5JarKrcwe7g/cjD1kDN2D5Bdjce4vO/L4KM8vtQKW0aqDgxRtvyoGAfKY5RjyR5w47JvWU+mD2OyRrlCR+AfYB/N5QjC7fFzaf5XRR5QZOnz3Bt5F+vYmJXIdSP9yHwvgj2T9fyx2KTcfPCO0/C+wnmPTDf+z8VZX9Rs87a1ztqasgqO+NDTM3hlSMDPu/UGO8ttSsO6ooNH9gPD1afZPq7Kr53hztDhmV3w9VcB7/3LJr6DuvM1ltN9edifQVLnddUmzV9tPgt2PgDe9/hTlW5g63C8kjD1oALB381PcemX4h1re57+e7c9/fm/sWWo77l60BhPjt62m5f/w4qvw0jvPuujzTYy6i4sG/eg1kPTGt3sqyi7B7Tg6qNuzgeDho1arT87XC7sv1jQu3a7jR0NWrf9vfT4HvRVI/Wf7enP3jMoVXo/BaO5BfcoYC7RMf7Ct59a1VvT3tVQ/9Vjgj29r2wP6lZ+DA5cy8PKHt+ZnbHru9d3tu8H4z8H+rUXP9VfUJ+9vXrmLIbOLbxXR557L7kfLbNU+zz5u2ThyNH0nNWo8JQXzMHjSNVrjDvExopvPc1wrqoUmgcyIrxJqoodcT0T/dSqN5Bzd/JWTNnGlvvgHiPSolaNNV7Wj/7W9ndy2ZP2vLAB3V3MA36tWlVPdBKvYOKE7aWbS9XE+idhFod48e9VJQ6vxN2R23vv33D7dcHnn17Imr/23Cw8n+os2v9e/uTvWI9730r1Y63hQ5E/PWVqnr1mSONmj52sd/Z55n/0keV0kTM4VxPexrjHm6YtvW8Z0z5D8jfMcWL9zk6EuUKK4dZY180/KmqF9/vgeAIrHJlP2D6p6/zBI73+e4Mqqz0q933RSAwAz1GuB9fxnvzYDH1fc3BgXyQlX3D37buFbvbkTYIUvYOr6Cg7Jma3uVaf3uH7XuB9edUVpqvwfvz76ai1AZ7n/cDV2C/450xh/m78kh8jwW2tb7LDy5HYm3bHlfzwlNBrOtSL/vaV3/Lh3Tl8IG9Zm8VlgzLoFU9z/P+DJIOuW89UlEURVEURVEURVEU5TCD6xvMbn5HguKERdQvevsP22VqqtO6dCdtC0VRFEVRFEVRFOVQQWec7EdUQaMoiqIoiqIoiqIohxdH6tl8B4T6rjShYqeysrJOs0IURVEURVEURVEURdEZJ8pvRGfVKIqiKIqiKIqiKIcLXpVIbbKuKk72QEFBAcrKyhASEmIqsaKiwszaIPwNDjgfv6aKrq3yWfU0Nj6bhhfrh1g37y+NjYO/zJ91Cw0NrYozLCzMGBtWURRFURRFURRFURSXmuR7iypOAsjLy8OCBQvwww8/YNSoUcjMzDQVSAWEVZqwymis4oLG2hOvPbFuXjv+2ni84eyvdbd4ry3WP/HGw1+6sdGpPLFEREQgOjoa8fHx5rpx48Zo0qQJmjZtiuTkZCQmJprrBg0aoFGjRubeG15RFEVRFEVRFEVRDlfWrl2L1NRUM+kgPDzcyMQdO3Y8chUn5eXlWL58OWbMmIGdO3di/vz5RjFCO5rWrVvjpJNOMhVFJYSdyUEFCiuRWGWFVVRYRQbvrX/i1VpRiWKxs0G84WoKG+jH/lp/dGNc9GvTojvLSGOvi4uLjSktLTUKEc6m2bFjB7Kyssxvbm6uCUtatmyJFi1aICYmxsTPztK7d2+ceuqp5lpRFEVRFEVRFKVec0RKukc4/rkFdYbyspW3hwwZYiZRtGvXzugKRo8ejWHDhh2eihMqQtLS0pCfn28UBHbGyLZt27Bs2TKsWLECM2fOxNatW5GdnW2UDQ0bNkRkZKRRllxyySU47rjjjPLgcIZLkKgsKSwsNL/p6enmd8uWLUaRQm0bFUoZGRlGWURFDf127twZxx9/vFGscMZKly5djB2VLPuCt8MqiqIoiqIoiqLsE6o4OfLYT+Lkxo0b8de//tXIww888MChoTipSaAuKSkxgv6aNWswb948rF+/Hjk5OUYZwuU1VJLQvS6cddZZ+PXXX+sstNsqC/TvrcrDTQGwfft2o3AaO3Ysli5dahQrs2bNMjNYSK9evdCsWTM0b94c55xzjlGsxMXFGVNXqODyzs5RFEVRFEVRFEVRlIMJl+rcdddduOGGG4zcywkEh9SMExaAwvqPP/5Ytf8I9+mgwM31R5zxwL05TjjhBKM0GThwoFFmcEbEaaedZkxSUlLVkhbOuGBYLj3hVBwLw9AcSkI882uVNd7rAwXrbvPmzaYNlixZYqYwTZo0ycxIYb1xrxjmgW1xzDHHGIVK37590a1bN7PHyu44GPlXFEVRFEVRFEVRjjwCP9h75c/Vq1fjpptuwpVXXokHH3wQzz77rLE/JBQnFMi//PJLDBo0yCy/6dChA/r06WOW0nBJDZeJUPFhl4oMHToU1157rZn18Je//AVnnHHGb5r54CWwegIFeuu+t4K+N/49xUG/tfmpqRnpd2/yxzA0gYqj3aVv4dKn2bNnY+TIkWYWkJ0VZGf/cGYKl/h0797dmJ49e5p7KsAURVEURVEURVEU5WDAiQAJCQlVugKusLj77rtxwQUX4OGHHzZ23N+EkwbqteKEs0qeeeYZTJ48GSkpKbjuuutw0UUXYcCAAeZUmNq4+OKLzYwHLr/hTJS6UBelQG3sS1him2BPcdTWVPuSdiBMg6Y2DZyX2uwD4eazVJzMnTvXzBaiIoxLfNiG3FOFm9WefPLJZmYKFStUhPG3rif61DUfiqIoiqIoiqIoikKeeOIJ9O/f3+gYuL/nbbfdhssuuwz33XefcbczU/hb7xQnzBQVJq+88opZ/sGZJMz4rbfeitjYWJ+v6ngFZ2qDOMOhbdu2ZrNXxaWmZq6LssEbbn8pJ9hG3HCW+9HMmTMHixcvNpvvUMO3atUq44eKFM4oOvroo82eKVSm1DV9b3/wXiuKoiiKoiiKoigK4VIcbudB2fPyyy83EzC4EWxN1BvFCU/CmTBhAh5//HFzHDCF5kceeQQ333xznRUgtiheQVkFZz811c/uqK0+ibWrS/3WNd2ioiIsWrQIU6dOxS+//IIxY8ZUafi4LwrXmdGceOKJe730SlEURVEURVEURVGefPJJI29yX5PTTz/d6B5ITTLu76o44RIOnpPMdUMUlnnPPS9uv/12s4PtnjYRVX5/6qI42Vt4kg9nonCpFmel/PTTT8auU6dOZrPZHj164Oqrr0a/fv10dpGiKIqiKIqiKIpSZzhR44UXXjCrWzj7hLJtq1atavxIf1AUJ4HCNTdh4VKcjz76yOxxwZNwqCi55pprzIahXiH4QArmyoGnpu5VU3taf7tra+6TMnPmTKNkGzFihOlH3CuFG8tyWhU3BOYpPhbtO4qiKIqiKIqiKEpN8ACal19+2Zz8SjiR429/+5uZfRLIQZ1xUlBQgDfffNNodrhp65///GdccsklZl1R4CauZgMWn9Crwu+hCLsVDdvuwLQfj6eeNm2a2QuHs1G43Ovss8/GP//5T9OnFEVRFEVRFEVRFGV3UPdQXl7uu0ONB8wcUMWJ94s/Zwdwt9oFCxaY5RUvvfSSmQbjhf5pAo/BVfYXVpFxMGBalWKY3oFvT/avr7/+2ky14sazd955J1577bXdnr6kKIqiKIqiKIqiKHvioMw44UyAY4891gi0n332mZllYrHJ13VWCf3rDJS9xSoyDkb9WcUJlSb+9Njc+7P5AvsDj6F+7rnnjAKFJzJxg1mesKQoiqIoiqIoiqIoe8MBV5xwDxMqTTgjgMsquPkrsUtxApUgdRWsHYfhKZQ7Jq7g4BDXQXDvj+xZK5WVZVIHoXLlVqarYGBT874OFbzPMC1vevYaWL1qLfILCtC791EBbe33s68MHToU1113ndlgeNasWWjduvUuShZFURRFURRFURRF2RMHRHHiFVDPO+88jBw5EnPnzkXfvn2NnTfJKkHWIzPPnjEVn37xFbJzC8W+XPyXoWHDBrj4oktw1llnu54M/hkUv/46EgsXLsL9999vlmccbCG5UtIL9qTnV+wcfBynQtIOQVpaKr76aijOPON89DyKG97Yej949eLHzj4B7rrzHixdugyTJo839y7Mm+0Ee58/b7tzD5Q+ffogKirKKE94xLWiKIqiKIqiKIqi/BYOiGRvBde3337bKE1effXVakoTulvjWro/lm++HoR33nkb8+YtxIIFi7B44UL88P33OP/8c9GhQztMmzbF59Ovbfnss0/xj3/8Azk5Oea+Ku6DBJUmBfkFkud5yM3Nk/R/L6UJFRRuhW7fuRUvvvQClixdbu73VSmxbzBPzJvkIDgIYZ4Nd9gnvPneF7ztziU648aNQ1paGp555hmfraIoiqIoiqIoiqLUnQMm3U+fPh333nsvrrrqKjzwwAPGbrezQIJcoZrk5mRjwDHHYfnSuVi6ZAEWL1mGNavX4Ntvv0VoWKg5enbmzOnik8tzXGH73nvvM8cJJScnm3umtTtcYX3fBXVDpRvPhtT1OPusc7FI8lsz+ym9WnDrN7hKaRMaGoQGDRsgPCLa3FeHefGafWPPMbGt3HxxGVVoSJi5JpwhQ6VKeUWZa+HBG+/u4xe3GtqTCrvHH38cn3/+OSZOnOizVRRFURRFURRFUZS6ccAUJ1SaNGrUyAisxHu88C5UWftmJBj5t7rfiMhoXHrpFVixfBW6dO2CO++8A8XFheIShLKychx//Am45pprjF/eV1T4FTGBUMCurOByllryI1ghnL81CeQ10TChAYoKihEVFeuzCaT29PYGb7547S8Pm5X5rpByVqJC6r5maE+z+/LVpQ6YMpcr0eyptty4vHXhSB4rEBrCE3Cq1xHvTNxiKvYQc4W0aU35fOqpp5CQkIC33nrLZ6MoiqIoiqIoiqIodWO/KU68AuuDDz5olqxQaWKPg93zZq0M78YRJNmqKPcL+27U/vhffOF5rFu3DrNnzzX3YWGhGDtmHN54/U2UlJSZ+9DQEAwbNlyE5beNn/+8/B+cfvrpRnimgiEklBunAtnZWXj5pZdw/nnnmdN+3nvvPZSXlRk/tkxWIbFmzRrcd999OOecc3D77bebfVtISXEx/vW3f+KPV10leQ/CA/c/hHPPvwx/vvUubN2abvxYNm7YLPXzkMRxLi677HJ89NGHNc60ePk//zGzdjZt2mTqk3mbMWMmRo0ajX/845+S75yqfDGfvM7K2on/e/xfWLZiqdhKGaUeg8WLuylsTdCecQRj48ZUPC5hzz33XFx44YX497+fQVZmlvFFvO1bVFSEZ5991tTDpZdeii8GDjSKjRCTH6daeRYtXIw77rhL4j0TTz71T+TlZSMyMtrXpi6VTiVCQyLFrsLsVXPttdeauJ988kls2rzF5wsINbULvP/u+5g8YTJysnJw39334pKLLsaoX3+VNhcfvjoJhMu4hgwZUtVmiqIoiqIoiqIoilInRCDerwwfPpwisfPAAw/4bOpKhZhyc3XXTTc4fXsd7VSaOy/04ziVlSVOr149nIcf/qu5J48//qTTsmUbJy+v0GfjOP9++mmndavWzh/+cLXTtFlzJzk52bnoggud/Owc47569WrnqF69nGbNmzu9evd2evbs6SQlNXBOPOkkJys72/iprHRz8eOPPzqRkVFOkyZNnaOPOcZp27atKecH73/gVJSVOzdfc73TJrmZExEsfpq2c1q36+KcduY5zipJw/L22x9I/Mkmnz169BTT3YmPj3WOPba/s3Llcp8vUukc3b+/c8H5FzgnSV6aS/66du3i/O9/Hzhjx4426X7//Q8+v34GffWlExYe5qxcs9LcL182RcK1c74dMszcV69Q1nWpuRo37lfJR7TTsGGi06dPbzF95D7e1Ne8efOMn4oKt+6nTZvmnHrqqVKGlk7Hjh2drt26mfz8+fbbjTup8LXjl18OcoKDw5wWLVo53bt3lnpOcnr07OYcf/wpzuWXXWP8uFQ6ZeVFzp/+dLWTmJgobdtLzFEmjSbNWzhTZ83x+XM596zznAvOvdA56YSTnQaJSU671q2dp596yudaHdt+OTk5pjxXXXWVuSfWTVEURVEURVEURVFqY78qTnbs2GEEbgq8ZWVlxq7uwin9uQL3Pbfc5HTv0svJzjO3TiWtK+juV5xcdNGFzo033GTuycv/ecVp276zsy1th8/GcT56/10j1Pfq3dfZuGmzsSvMcyMtKy51Tj/zLOfs8853dma5ShKyfPkKp03bds6Df/2bz8Zxtm/b6jRq0NC55JLLfDYuL7zwgnPzTTc7xQWusmbjyjVOTEScM27CDHNPKnzKiZG/jnLCxe1vf3vM3FsWLZrntG/fxrnk4gt9Ni5XXn6Zyftf7vcroCoqSkzZjzv+aOfGG2/02fq58sornSvEWJYvm+RTnAx3LUxT2Hp063rZ8kVOVHSYc+21VzvFxQXGjuTkZEu+2jldunR28vP9yqiHH37YeeKJJ5w8Xz2S/338kRMUHu6M+PVXn43jzF8434mIjnVuuPHPToWblJOTm+b89W8POcHBUc4Vl1/nWvp49LG/O82bNzOKNy9XXH2107lXbycrz1V2kcsuvdLUze233SF9w+1fZaVuPe+Oxx9/3ChmVq50FUuKoiiKoiiKoiiKsif26x4n3NeE+0xwiQmXTZDalk7UjN8vJWO7TYlZAOIqecx9aWkZSkpKxI83bgfBwSHGWAoL8xAbG42PPv0ELVs0N3ZRse7+IwO/+Axp6dsx7Kef0DAxwdiRLl0646X//BfjJkxCema2sfvmq0GIjY7GZ599Zu4tXP7x1ltvIiTUTbOosMCcrhMS7Jad2Ar+4IP30a59B7z0UvXTXXr27IP/e/wxjBo1EsuXLPbZliMvPx+nnHE2Xn/tVZ8dlzuFS32G4+67b8eEiROwds0mnwuwY0c65s1diGuuvd5nU1lVZ5W+enOry+gc7I3ZcLdpk6YYOPBrRHg2kY2PT8Avv/yETKmDId8ONXbl5eV45JFHzBKaWF89kltvvgWt27bFyrXrfDbAlwMHIrlZS3z26Ydwm6QC8XEpePml/+KySy+V/O4w/siatUsw6Msv8frrb5plQl4+/Phj7MjMwIQJE3w2kLopQK/e/fH+B+8hKMQtR2iYf7PZ2uASoOzsbIwZM8ZnoyiKoiiKoiiKoii7Z78pTmbOnInBgwfj0UcfRfPmrpJib3EVJJ5NRqsEfhe6FxYVoXmLFj4b8RLE/S88RxwLxcUF6NCxPbp06+azIW48S5cuxrJly3DFlVfhrAsvxlnnnY/zzjsPF118KZ548ils2LgRWVnuHh9z5sxGv759kJAQZ+55ig43u62QfETHxFgdBCrKSsFTdUtLy10Lk1YYcrK3mfhuuuVW11pwHG5kWmquTzv9ZDROScbS5f5jg4NCw9CxW3ffPaEixN075Oyzz0BYaCh+/XW0uSfffjvEKD5OPOEkn42/voKrHY3srx8yS9rtjDPP8N3ZfLnptG/fFp07da46zpjKsIYNG2LLli3mqGluxnveeefi5NNOQfqOHYhNTDT+mNflUrf9jxlg7pgT7mMCuPHGxsWYzWAt69auxvbt6RLnO7j4kktxuuTn3HOlLS68BGefcy4yN21CYUmxz7fEFxyEfv2P9t15YEL+Yu8CjyeOioqStuceMIqiKIqiKIqiKIqyZ/ab4oQnlyQlJeHWW/3Kgb2HMyUqRdj2C9degkOCsXrNarRo5VecEPdUYL9iwGyKKrcFhTx9x+K6m5kKUvqc3FwUFBShsKBQfguQmZWF+IQE/PnPt6B586bGb5GEZ5pe7Ga35ZXlVbMemGcqKYKD3dkPdqJHRXm5cfMqetx8uOFCwsIQGh6Bcse9pz1n0xQUl/juCY8Z5kyWUiQ3botjjx2AH38c5joJw4f/hDPPOkvcklBh6s3GRQWUf6NdF3/a5RXliIn2zzRxZ6nQv5QlOFzcK1Be5m+HIUO+MScYffTRx9i6dSuKigqRm5uNoOAgFBVb5UYwwkPDEOJT2LjVwPTcfHC2kFfBFSJhw8PCsWPHTuRLO0hkJq6cnBzTqBddfTW6de3q8y1WErRU6tRiTg1i1FZpYn99uIo4biIchrOkjlatWmXuFUVRFEVRFEVRFGVP7BfFSWpqKsaOHYvbbrsNycnJPtu6YxUMVpivrCwXITdEhGl3CYx1D/IJ4tOmTkWFCPx9+vQ294SKBtf4i0TZvKysTIxXAeNGVlpehmMHHIvJo0di2vjRmDppAiZNmoSpkydixuQJ+M9zzyAuMtL4TWzQwCydqYJKHRHWeYpMaDDPenHhCTacSWGX7tjycKYGl7msXr3C3BPqJmy5MjJysTMrB5GcvWIIQlmF5JkR+vArhdzy/eEPf5D4VmLnzhwsW7Yc8+cvxM0332jc3CJS0RLkKp+C/EoGxuGvb6CwoADb0tJ8d+Jq6tg1PO65qLAYTZo0MW7Z2el47F//wjFHH4eJEyeb5TMTJkzCgrkL0bZNm2qn8IQwryXuDBPG5pbErZfw8AhUVvgzERIcjOjoaPzww48YN2YUxo0ejYkTxmOStMOsqZMx7Ouv0btbT1SiBBU+5UvV8iOBy6N2h1dJ06FDB2zcuBGlpe5sH6tUURRFURRFURRFUZSa8GsZ9gEePczZJjzKluy9MOoKuJxBEB8XjwTfRIggkbeDQih0ByE3Nwu33X47zjj9dBx3zDFi5wrSJqyZBeIXkpkN1/jywx/f9Uknn4zUNWuwYJldHrMrFZWuwqF3n96YMXM6Fi9eZO4hebEzTtamrjXLhkiFJEClT4Rvvw0rr8cnNkXXLh3w7eAvXAuBM1jszJRvh/yApIaNcfSAY809M8o6cGd++EoocbnKEzfdiy46D63bNMUvP/+Mkb+MRYsWrdGlc2fjVrU0J8hhVhEdFe7e+3CVI27mjj/+eIwYPgJZWRnmnm42X2PHjUNJaTEuu+wSc79g4UJs27oND//1r4iL889S2bhlPTZt2Yxw39HTpFfPHpg7YxIKct26cWfLhKK8tBSrVq5GRIQ/T506dUFZWTl+/OFHn82uVMp/wWC+gn16IX87/xaoSOOMF/5aVHmiKIqiKIqiKIqi1IZPwt435syZY4TRjh07mnvvF/664w9TXl5pNpndmVmMjIw87EzfLgL7VgwaNAhHH3MsWrVqg1dffc3n24UzECrMbrJ+IdhxglApdnZGgpGPfclccfnVOOWkk3HR+efjhxE/Y2dGNnbsyMS61I147sWX8N4H/6va5PUPV/8BDRs1wBVXXG6UREWFJVi0eAnOveB8s1Eql5kQzkyJio7Exx+9j5ycUqxZvR452a5C4p47b8Oq5Utw0UXnY+3aNcjKzsT2jO147e238M577+PRRx9D84YNjV/CPUHKy9xZEcSUSvLulq7MLKM597zT8Mabr2HEiF9w/18elLTDzEyWKoKCUVhUgG1bNiE3N8csfeGskO3bt0tZ3Vkmf7n/AURGRuGEE07EwoULkZeXh/z8fAwdOgSXXnIJzjnnbHTq3Nb47dSpKxISEqTMj2N7eoaZyTPi5xE474ILkLst3TNjBrj+hmtQVpKLSy4+F6tXrzb1MGrkcFx66aVYOH8uEuL9m8s2a9EJ//znI/jb3/+Kf/3rCWzatAmZWVI/ks9hw3/Ck/9+FhmZmeLTVZpQqcQZR3sD+xWVXlbxRfauvyqKoiiKoiiKoihHAvtFcbJu3Tqz1KJFtT089h6e0DJ91jS0btkc7dq2Qfv27dGje3c888wzuOjCizF2zDg0bdLMLJexswVKykqRV5BnFA6W0tJK5BcUGUGbcAsRKlgqfZufvvnGG+hz1FG47k9/RHLjRkhOboTu3briOUknN9s9UYc0bNwcXw8ebJbc9OvXD3Hx8ejfvz/mzZ2LSy+9DOHhXNJTiebtWuOhv96P/334BhITI3DllVci07d85aRTzsH3Q7/EogWz0alzRzRo2BApySl44YUX8Pe//R2333C9b+tUl6KCfFSU+Pc4oWhvxftK38aqf/rTlWa5zqJFi3DGGWcZOy+RUQkoK63AbbfdhYSERDRu3BgpTVKQkpKCP/zhKpSWFaJZ01ZmZgkVCb1790a8lK1JShPccccdYu7EK6/818TFem7WtCWef/4FzJ07W+JpJOUONfF0794TjVu3QkGuv85ad+yGjz/5HxYsnIdOnTohMamR5PdadO3aFTffcpPZuNdPJR56+O944onH8fbbb6J169Zo2KCh2WT4mmv+hDWrViEywl02xTooKihAabE7k6VGdqMHSUtLM8t1uEmsoiiKoiiKoiiKouyJIBGIXc3DPnD11Vdj6tSp2Lx5s8/mt8EseD/6b924HiN/HYO09AwzG4BHCnOuxQXnn492HTsYPzbbZgNYBGP2goWYs2AxrrnqKiREuzNAli1ejLUbN+HE085AYnSEUZrw1BizUqWSe2u4M0rmzp2L+fPnm+sGIrBTgdCuXVvjl1M4TN6CwpCfn4eRI381m5g2adrELBeiooFUVBSaZTHFxaUYP34Kli1bhe7deuK0009CuGQnKIjKjgikp22Sso3Fjp2ZiIiIwnHHn4D+ffuaOMokT2FGleVgyPffIyI6Fhedc7ZxIywpl+8ESebdWRKlOO6409EkpR2+/+FL14+pFtZnkOQlD99JPDNnzERkZLTYuXuMFBcXo2/f3rjuumurluWwbN9//wOWL18ufiPNJqrHHXeccSsv5yyNoKpZGoulXocMGWLKTgXSsQMGYPDQoejTvx+O6tTZzaNpz2Bs2LAO3/0wDMVFRTjpxONx4omnYPGixVi5ajWuvPJyEx83zw3xHV+9Zs0ac0ITFRwRERFGiXLeeecj1LdvDIs3ZdoMsxzphOO4VItldvsC2d3sEe4zQ2XMTTfdhBdffNHYuX2v9jCKoiiKoiiKoijKkc1+UZzccsst+OKLL7Bjxw4kVh1JW3dc4dVeV8g1BfTaJ8NwpgmFXVfg5b4iFOxdwZtzSxjSqZA4fafd2JkcwZKOUYYIPLo4JNgVxmujvKLUCOj8z2SwRgHbdwIQSlFaWoiwsAiEBvuXoZCKymI4lWVG4RESyuUsbl69VFQ6Rjlh6sIoRty8cV+Tqu08mH+jOmB9hWDVqkU47bRz8emnX+Oss062Xkx+/PVTO1V1Yeq7Zr+B3WNPcXJT29DgYCmvlMfUf+3+uSQrxHdaEfPsXT5TE24/8ccnQXzRu/ZUjJly06oGJk6ciFNPPRUDBw7Etddeu0t8St34rX1CURRFURRFURTlUGb3kmodueqqq8zX/Ndeq77vyG9BRFiqC0QI270yg1DA9gtrnAkRahQmVANUiXQ+d9pXx03DVRbUDAVDCvKckUK/jvhl/igvuoapuEoYv5IiBCFhURJ9qNh5F90wHnELCUdwiLvcpCZ4JC9zzP1YvHXAc4JMetSgyP/0Y8v+66+jERMTg86d2pt7ky3BVUDY+qkdNx1XWRMoDNcE092TP540RHg8sZsH+g80LqGhbEdeUWm05664i4BuZhu58VWPuWY++eQTc0LQOeec47NRFEVRFEVRFEVRlN2zXxQnPE3nmmuuwVNPPWU2T7XURRgPxA3BbP22sFZMryqQR8Y2agRfdFRiuMqJACFcsPl1FSEU6r0KmurQr+vPp0xwQiTucIQEcZlQYP6DxQ/9cqbJnhVDFm8aZtpJVV6CUVJSiF9/HYO7774brVo399n7qfIq7K4ddlfO6vXhlrMmf4Ruu3P3U929stINZ/er8ZrdYZ1t3iy1hePRyZ999pmZHdWoUaM6pVETexNGURRFURRFURRFOXTZL0t1yNatW3HiiSeisLAQs2bNQqtWrXwue4ZZCBR+9yyA+4XY3fmlD5raNESBxa8tLtebmy9//mxY2ongb64YR02pWVc3zT2Vr7ay2bDFxUWYPn06+vU7GvHxcWJPvz5PvwM2XzbfxJ93a7drBmsr556w6XnhUh3/CUrV3Y8++mikpqZi6dKlZnNcKmrIb033cKCmMtfcbjXj9UuOxDpUFEVRFEVRFOXIoTZ9Qp2hEEXTrFkzs4cEr4866ihs2LDB52NXQSuQQMFrfwtijK22HDAtr6kNOll3vz/+WrtgEdrd2Rs1Y8N4w9dOYH5sPdtrHiF82mncnPb3V5qQ3ZeJbjW7B5azrtQUxipNiHXnMdncz4RHZj///PNGaUJ2l+6e+uvhTmD5ee89wUpRFEVRFEVRFOVIYp8VJ14BtGXLlkZ5QiFrwIAB5phiUpuAejBgytYcyth69ta3pbbq3Z+CLuOyZk/UlMeDTWlpKSZPnmyOPx40aBDeeOMN3HbbbcYtsAzecjHfddlv5XDDttnu2q4uba8oiqIoiqIoinK4sd+W6jAaK3CtXbsW/fr1MwqUJUuW/KZlO4qyt5SVlZnlS1SYcD+T1atX4/TTT8eTTz6Jk046yedL2Rv4LFulSuArozZFi6IoiqIoiqIoyuHAflOcEK/yhEIr95XgqS/z589HcnIy0tLSqvaW2I/JGqxA581DTRwoIY+zFGzZArFuNeXN1kNN+aprHTEsjzLmRquW3zJrorZ8kz3VpyXQX23pe9PaXflqcmP8blmDkZCQgKioKONv2rRpGD16tDG8Jh07dsR9992HP/7xj4iPj0dOTo6ZhUJqK4+N2+bRW4bAOvL6qyt7E+ZgY+udv6zfhg0bVmsL1lFg29RWn4qiKIqiKIqiKIcD+1VxQhidFaQ48+T88883p5j8+c9/Nssm8vLyqgTY3VHXbP2W7O+rgOctGwm83194y+SNv6a0rF/rVlPY2vLt9VsbdfVnqSkfu6Mmf4zDGqtooD8K8lSChIaGGiUJl4WxL9GuRYsWaNOmjfkNCQlBYmKisedmxQUFBbvNT01u1s6W35bLy+7i3B01xVVfYJmo4GG9cqZY//79TRt42yMw//W5PIqiKIqiKIqiKPvKflecBMKZJ8cffzwqKirMEoru3bv7XPYPBzj7NQqFgWlaP7Sn4f2RLkzurl32pW7Yhz788EMMHDjQCPFXXnklbrzxRjO7yW78quw/ysvLjSKKbebt316O9L6uKIqiKIqiKMrhzQFXnBAqT0477TRs27YNTz/9NP7+978jLCzM56ooe4ZHXD/22GMYM2YM2rdvb5QlPC2nXbt2Ph/KgcC+HqxypKbXhSpOFEVRFEVRFEU5nDkoihOSlZWFBx54AJ9//jn69u2Ld999F8ccc4zPtXaYvd9bMONSDyp6wsPDzb3Nk626uubvt5alPpQ9EFtmizd/+zu/nO1Ahcl///tffPfdd2jSpAkef/xxXH/99YiNjfX5cqmPdXW44K3bwPYnWu+KoiiKoiiKohzOHHDFSaBAO3jwYDPrZNmyZbjqqqvwyCOPoHfv3j5XIDc3F3PnzjXLA3r27ImkpCRs2bIFa9asMXHRPjo62uxnwY0rLTNnzkRkZCR69erls3HhqT6kR48e5ndvYJ7btm1bpeixVWbLlZ+fj1WrVpkNSCncd+nSxbhR8F+0aJHZ16Vbt25o3LixCbtgwQJjxw1OO3XqZPbu2Lx5M1JTU82SJipouKSJ7gcDLnlZsWKF2YuGm/gStgM39aUbZ3W0bt3a2Ftsu+7cudPMKKIf71IZ7m/DMh511FHVNlmtjezsbCxdutT8sr3ZPyZMmICFCxeatO+66y7ceuut1dr8QBHYZxVFURRFURRFUZQjl7ofvbKXBAqgV199tRGQn3nmGfz444/o06cPTj31VDOjYMOGDUapQKUIZxQMHToUJSUlRuFAgZlKCQry48ePN8oKC+0o5P/www/Yvn27zxZGkfH9999j+PDhKCoq8tn6Ybi6wLSo0LAElokKD6YVERFhNixlXsjYsWONEoCKg2+++cZcM3+cfUMlD4/OnTFjhvFLBQHLzrLGxcXVSdkQCAV+q9SpKxkZGfjqq6/MviFWycTyDhs2zGwQynz+9NNP2Lhxo3Gz2DqYPXu22W9k3bp15p6wrhme4bx1xbyxntj+o0aNMrOOOHuESiJuHHzeeeeZ5Td33HEHPvroI6PsYh9YvHgx/vGPf1QpTX5rGX8rqjRRFEVRFEVRFEVRLAdtqY5NxiuU8njiTz75BF988QWWL19u7I499lhccsklZiYGZx3wVJ5+/foZN8KZJ7S/+OKLfTYuVMJw1gYVMdyMlnDmChUZnKHCPTE4s4NKF86EoL+WLVsaRcb69evNDIvmzZsbAZ4KBMbFU0U4U4RKD4bljAzOeOFJI17FRllZWdWeLSNHjjTxH3fccfj555+NoogzR7799lujBLrmmmuqwo4bNw5bt27FddddhxEjRhj3K664wrgRKnb2RoHyW6Ayh0oPKm44U+bMM880sz4+/vhj3H333UZxwvY58cQTzawbzqzp0KGDydemTZswZcoUowjiPctJpUtmZqZpTyqBeKoNZ97QbseOHVi5cqVpd8K42Q5UiFBZxHbmrB6ejMO6pp2XmvqQoiiKoiiKoiiKohxIDqrixAq83mtCYZtKBM7UoAJhzpw5RjjnEh4qSahAaNCgATp37myWknCWBIVuzvDgTBAeT8trKh4ocFMBQGUJlR+cuUJ7Kico6HPGA/cs4RIaKlOoJOFSHCpsqBzg0hPmhQI8/XG5EJUtxcXFZgYE80hlyoABA3y5rw73cKEigMI/l79wORLLytklXNJyww03+HzCzMjg7AyeDDNp0iSjlOAxuizLWWedZcpYV1hGpsO63VOT2vqnsoMKH14PGjTI5Pucc84xZWU70B/zw3KcfvrpRrnE2SX0R3vOtOFmrTzRhsoWLq1h+3Ts2NHUI5VfLBcVK2wX1huX7tAPlSpsT842iYmJ8eWsZgLL4+07iqIoiqIoiqIoinIgOWiKE2KT2pPgy5kJFNCpDKBwzlkmnNURHx9vhG7un8GZIpz9QXvOBqHwTnsqWCjoMyyVF4QzRCjEc3YD90zhEhTOUKESg8oRzhK57777jF/OgOFsESpfLMwLZ1uccMIJZrNSpvOnP/3J5+qHS0q418pNN91klh1xNgaVM4QzOlgOO6OEZaTSgTNnvIoDKok+/fRTozg6+eSTfbYwCiTOTmEcdrkPlUAsPxVPPLGICiQqQ2pbghRY//RroYKIM2U4A4X+WH5ec8ZI06ZNzbIjpk8YjnvJUNnEfLBOWbfc44RLephvxsEwl1122W86+cbbHVVBoiiKoiiKoiiKovzeHFTFSV2hkoHKCQrdXuH5l19+MTM0eLQxl38w65wxQYH/119/NTNCeG33DeFSDyoZONNk8uTJRvHBpSJcUsLZLFw2QwULlRy8JlyWwtkVXLZj4VIdKjJ4GhAVI1zaw5kkVMBQSUHlBRUhzAOVNZyNwjJwNsspp5xiFAtcNsQZMJzJwtkwXELEcCeddJKZrcFycoYJDeNneJaVihrGzXxyJgiVJPRLpRHLTmUFy8kZHIEnzQQqUGpSSjAe1iUVUoyfaVKZxH1mCOuK+WeeeAwwFSpMc968eSZPVChRUcUZKJxNwnbj7BLGQz+sc68CyOLNW20KktrsFUVRFEVRFEVRFOVgUe8UJ1R6cHkHZ2pQQKdQz5kUXJ7DmR9c1uJVali4uSlnc1Ap8frrr5tZKPfee69RnFDxQaGey0+4DIdCO5eTUJnBuKhwodKASgsqDbikhMoMu2cH7Wh4TUUDBXouReE9lR8My/g444QKBioUqMygAobLVxjXueeea/b9oJKGygQqQ7g0x0KlB+MjdGc5uPcIy8HZMtxAl3uucA+WZs2ambyyXvYX7733nlGWMJ+cNTJ69GhcfvnlRmnC/VmoDKHCim6cgWL3dGH5OUuHbcW9aSxclsSNXW+++WbjZhUlXmUIr233UyWJoiiKoiiKoiiKUt+gzFqvFCfMCjcl5UwOKge4TIQzQ6gs4B4hnLHBWShWYWAKIAI3/XO2B/1yw1cuw6HC4eyzzzbLTThLhOE4i4P7p3CpCcPak2I4S4VKFc5M4b4b3F+EMya4FIbLaS699FKzRIaKFsbD44Kp9GAa3O+DhooFKmmYZyoVuPSFZWC+uTEq90+hEoJpcnNUKlOsUoinydCOs0uYN85S4dIXuzTpQCoVmB/ODGE9cW8VzpahIokzYuwmroR7n9CdM3K4lInls/DUIi6RosLHQsUQlS9UpnApki2rxbadoiiKoiiKoiiKotRn6t2MEwrYhLM2OEuBwjoFbl7bX6/AbQVwGlsU615X4dzOhmDaNh4qR3jPzWc5y4RQqWHdvfuD/N7YcpO6lJfYurEzZqjc4DWNXfJDeyqleG+VTYR+2Ba11QH92TpiHdpZOoqiKIqiKIqiKIpyKGFkZ/mnXilOasJk1CN4e7OsAnn9QdtFURRFURRFURRFOZww+gj5p94rTpQDj+kMquxQFEVRFEVRFEVRlCooK9ef9SZHOGyM31OHVZvSRPVqiqIoiqIoiqIoypGMzjhRFEVRFEVRFEVRFEWpAapMdMaJoiiKoiiKoiiKoihKLajiRKkzOjlJURRFURRFURRFOdJQxYlSZ3TzWEVRFEVRFEVRFOVIQxUniqIoiqIoiqIoiqIotaCKE0VRFEVRFEVRFEVRlFpQxYmiKIqiKIqiKIqiKEotqOJEURRFURRFURRFURSlFlRxoiiKoiiKoiiKoiiKUguqOFEURVEURVEURVEURakFVZwoiqIoiqIoiqIoiqLUgipOFEVRFEVRFEVRFEVRakEVJ4qiKIqiKIqiKIqiKLWgihNFURRFURRFURRFUZRaUMWJoiiKoiiKoiiKoihKLajiRFEURVEURVEURVEUpRZUcaIoiqIoiqIoiqIoilILqjhRFEVRFEVRFEVRFEWpBVWcKIqiKIqiKIqiKIqi1IIqThRFURRFURRFURRFUWpBFSeKoiiKoiiKoiiKoii1oIoTRVEURVEURVEURVGUWlDFiaIoiqIoiqIoiqIoSi2o4kRRFEVRFEVRFEVRFKUWVHGiKIqiKIqiKIqiKIpSC6o4URRFURRFURRFURRFqQVVnCiKoiiKoiiKoiiKotSCKk4URVEURVEURVEURVFqQRUniqIoiqIoiqIoiqIotaCKE0VRFEVRFEVRFEVRlFpQxYmiKIqiKIqiKIqiKEotqOJEURRFURRFURRFURSlFlRxoij1HMdxfFfKkYS2u6IoiqIoiqLUD1Rxoij1HMepVCH6CIPtXVFRYX617RVFURRFURTl90UVJ8ohAoXHI1OArKioRKWYQxvbfgeyDQ+vPlJZ6W9zVZ4caA6vvqMoiqIoiqLsX1RxohwCuAKN998DjpWj9mT2J7XE58rMQeZaqQ1P5dWmZKC1Nb+FQP/eePZkdkdd/IiHIF/T10l5UgcvdcFmjaobr9lP0deMTXR3iXj90Eim9l2n5I2gDpHtc3rKYY32D0VRFEU5LFHFyWFO/Z7qX9d8UXJ0jYiRxsaF4QPNoYgv70Fu/gNL4ThBCAn1P6p7356+dKpMdSpNX6F4vD8ITMPfhnVml2zuYlGF2y98xmoadsEb1sZVc3xV7MF5v1BLGpxxEhTEdg+S3zrUm42nKr66Zb5SvLHtA6lb6P1EXRLz+vFdM9tlZVzS5N7vHf6+Uz0e3tRgzHNKcyjiKcdeluFQLflBwVZOtUqydb07UxO12SuKoiiK8ntQrxQndREIA/3Ub8VAzdRUhgNFRWWFX+gSCWnv06p7OPqs2fBfi7X1/9QFK9YzLhtDdVM9FRJ4vyveGHz4ZalajPj1KTos3ljqagKhnffLvmmuoBD3xkeQwwyQgFjsrcfKj3Xwx86aqvQoSnjt+vLXraHqgsiNpw9Zf4HGV0nmavd4U/MY/lTDaxHo6L/nFUtEZYAf3oip1m5u3lyXXXNQRU3FsHZ1MR6qxUuq3HdNuVIcK6vaWVx2CUzfvjDVg/qoIYAPulR4nCucCrGrXgcWb1FcFQ7x+fB6DMQTkefS/DJta7z23sSoyGEb0t762QXxZx4N/uNmrBZ2bV+v8faVigoqZ/1uu8K43Oekito9V6OO3qpjA9XJ1GDpva3CfQdUs6oF/9vBpUKMfYMEhrd2rgl0tVgfXr+7Us0+wJO99VjVirdtzXvBvazCG9e+mBrZraOiKIqiKIca9UpxQgGfgn12djbWb9iAdevWIW3bNp+rjEHEjX6Ki4uxdetW5OXlmfs6fY2tR9j8FhUVoaSk5IDmPzQkFHPnzsX99/0FO3fsMGntnfLEn8eyskrk5xehVH6JHR/SeAfVuxpXKHHxunjZ1Y5bPcxasBITZy+pGvjShytYMc5A44/BXtt8Wbx+3LJZUxPVfbtU92991MUwL9bw3sbl2lHk8Isd7BobNm3Fl4N/REZOsbFzqp7a6nmoHRub/5dX8uQgOCgY4ydNxOixY4wN7906pI9AY5E0PbdeH16BnNjbjKwSfP71L1i/Nctn42LbkIoCbxvyf0NV8XjhGq8w5BKEnMJy/DJ5LpavT3N9yT/s5sZ4wpq0qsK791YY9BtXNLbtYI3Fa7cnY+G1jbuaC7Nl/rHGdQ2Ryx0ZOfjiq6HYnJbhlscTzo3FH8aPjb8mN9eFMP6pc+bi57FjUSmRc2YL3WwduDEwBb/xuxD5DbL31s6DL3m68DKvsBKDh47Guo07TdqVYvhL9zL5h21QKtFlynslt6QMjsmTP3YaA+3E8C/X0mXr8N2wUSi1jtU8emEAv3OgCRbnTVuyMXDQcOzIzDW+XTe35FWF8d27dkR82XepWPmuBP8VceMKtD0QVM+rMd5bwc07XyDuc+5twZryx3YZNWMxnnnzQ7zx8UBs2Z4J6qq8PcFeee/d5AJjtPdubwo0gdRmF2h2B9t2y44sDB89CWlZhSZfeYWFyC/IN+68dxXG1Z/1vTL8hxFaQ+TX9hmvqe6RpiZqsyc2VUVRFEVRDhb1SnFC5s+fj2OOOQYXnH8+rr/+egwYMABXXn4ZVq9eJQNpdyBBpUnz5s3xzjvvmPuaoHLAKgh+q6Jg7drVGCOCZGlpqc9mV2qK87emc9/d9+CD997z3e2e2uKuPR/WAKlr12PQoK9RUuIrDyVHjnyJ35tL4L1QKdIwg1iheMPmzXj86X9j4dJl5p6Da+pQ+GuFUHrlbzUjlq6hq2fA6PvxRV+FFXBLyoEJU+dj2Zo0yKXBDD6lP9CLDWevOQzmN3Sbrmvn5r/avc9YN8Jf36XUo+/Cm9cAbFze+GkIf+ke2ETGTgxjpJ8lK9eJQLkV5RKBG8YdWlsycnIxddYc5Pnaj3kulwi8eTXYbAZklbFRMVEhj7s13hSWL1+JJYuXiY1rZ/+199UjlV/PrS0L24pWFIZzCh3MX7kJ2wv8oYpKHEyduQjpma7AwjK4Yf2KC9seVW4SmNdlYrZlZGPB8jXILiiRSN1297IjIxdjJ83C5rRMX7yChLfx8tcaht+RX4Ap8xciv6wcoeKR/r3pW2Pj8hpSIY1qr80SJ9+vva5yE0O//HXtXMHJfy9IPsVLtXAkr6AYE6dMQ2aeW2cICq4qAw0x/lnJtqJ54XtPkqr45IIKSN5b16XS7jNmzUWZFJR2dKOxeTPtby2ZQfNA0qeNgffVqfIuxjzrtBRKSh1MmzEf6TuyzX2+NOOEWeuRXSyxSXShYtZs2IhnX3wJs6VdrHBezYgftomNc93GzZgxV/LvuzcKFTE2bZsPL9bOa0h2dgmmTF2AgkKm4MLo7DNtYds58vy4z4YYZl7+D/RnsXmvSs8m6IGTvqomfnncTZ/gvS8pmqp7i8fNa09vNsoieW0sX7MFWYWV5v1ZLh75S3dvPyf21zJvyQZ888MoIDwOodFxUk43Ef7LvzNub6/+PFZ/b3hxlZa2PrxUlZXXPkPMr0TDZBnOGuunNuNlzbpNGD95OnKLykyOvvl2KN5+x/93l3Ymff76DHHVKbv+mmv5h7+2hLxnV9iZW4iFq9ZjZ16JUejZ/urWu6++ffY05j3uu6Y/g9iZPuFemufWQHtaVLmYG0VRFEVRDhL1TnGybds2ZGVl4dVXX8WgL7/Ed0OHoKy0BEf37YslS5YYPy1btMC4ceNw8803m/uaoJLFKlpqmtGxOyXH9OnT8cy/n0Z4eLi5p9/ycg5t/HjjtHHVlA7hXgU1pZe2eQuyd2b67vy4ioXqMG4zUA1wqz0f1gDxcXFISU5BWJhbnqBgaXbb8n5vcCiBeO4tQfxs54NjuJRmzXHNdTegXYfOrqUQLvGFym+YGCv0EObIGgpHFKyDq/InQryMOI3A6fNjZx4QmyyzXR4Sjcrw+KoBP52YjjW2OBzA0pWiDe2sO/PGwTftGJbeGBeLbPyJJdUSzBrdiadqq8pj4T2/lPMrOXsG0wgT/0yHhnFZgc6UWX6toTvTNumKGT1pNsZNW4CwkGCpPwoXtq3FkQSHIjw6Bo5UBNNlWFuXniwae1s/XuiHeaHYQFMhhmlbQiNiERoZa2YeEJs3FzcRlrGk+iNgoD9a27riT35RBQZ9OxxrN+9wLYWg0EgRvBIRFB5r7llu+qUwbIUHWzc0vujcsopJTcvA4B9/RZ5ULP1Zd9OQQkhoFILDYxAcFmWsaOiHdcTwDEM7W7aM3FKMGDkRJWXMiV9w4RMSLiHZDuxDrDfjJoHpzjhpWFe8p1uIXJt6YCX4rm0e+Uu/TJfpW3vG5z0oiUHdvuvHCQpBZGw8QiIi3HsxNm7mmr/EzNgQYxUHXmjP+qXxPvYkIjpJTIL4cWebWMNsubUi+MpEpQ2CQ0w5XJhQiJTf9ikX2pbL+46Gz6+Np0KuIiUthLjvoFAp0keffoO1qTurnofkZi1w4623o2PXniiQe4alG39teRm/fa6CpF7C5Llg+ZlnW530Y98d7LMsO8tlYT3z3sZLQiPiJH8N4QS7zxjTYBR8zrx1ZtOx5bXYZ9ytHzdxXpv+IYZuTJN5DYRVWzWLzJdvYsKLvTcM/boXkg8xgfmwMBr7vGbkleHzwd8jM7/cvJ8LJWKWj3XLe/t8MD0m5e2HBcUOmrXuhNvuuBo3//ESNE5JMvb0z37P/9x3i2vnz757xSx48893P/1Zv3Qy93LDbkb/Hu/m2vRf+TVpirF9gtA90Nh+YAmNikdoTLy8PyNMPs8482xcetkVVWlxlh3L4u0PdAuRHLrpub8sq21/Zt618+WLF0JWQQk+HjQE27ILzN9E/h2gYTrMtzG+sIynTK7pxnvGYxA7trMvStM3bJqsI9fFuiqKoiiKcrAIeVLwXdcL1q9fj6lTp+If//iHmVXSVIT0P11zLQZ+/pnYT8MNN96IkJAQs+ykcaPGiIqKwpo1a7Fhwwaz9OXrr79GcXEJWrduZeJbunQJvvrqKyxZshQtW7ZGTEy0EUqpXOAymR9+GIZvvx2KrVu3oGu3LpJ+Kt59712sXbsWWVnZJs527dsjVNLMycnDD9//gF9++QWbN29Gx44dERoqwyqJa8WKVVi9eo2ZpfL990ON+/LlyxEVGYWkpCTjh4z4aQQKCwvRpEkTDPtmCJo2bYKTTj/NuBF3IOfmbeTIkRgxYoTEswJNmjZFXFxcVTxZmVnGbdSoUWbpUtu2bY29dV84fx4+/N/7iI6Kxo4dOzF9+gxc+6drERMfY8Zc61atweBB32LMqPFIadwUDRtLHmX0VpJfiskTpyApIRFjRo/B3Hnz0KNnDzOwZTgO7vKLyo3Co0GDBFT6BnVzF64UsxQ7MnMQl5CEmPDQqkE9B+IcLBbKCHLR0pWYv2gZtu7IREKjFESHBZuBsc034+IgkotSps1egIXLUpFVGIwN2zNRIdJWv+4tQDGS+Vi7KQ/zF6/C2vVbkVdYhiaNE0w+WYccaKduSMeCRYuwRtoyNCISDeJisTF9J9Zs2IyEhg1dgUX8LVy5GjnSZxrEx2GHCBcrU9NRFhKLxas3Y9HK9QiPTUJSdCgWS3wzFy1FoUgsSUnxZtBL4ZcCzIa0TEyaMx/LUzciJDoeKbGRpvw78yuxcPka8RuBtRvTsHDpahGGItAwIdqUc+nmfMycvwybt+cgtzxCBJUyJDeMR5gUxFcl2LgtA4uXL8OA445HXFSoKfumHUWYK3GtXr8ZO3MKkJTSwNjbQfqy1K2YuXAJlqxZj8jERkiKCkeR2DNKkYUwY760w9I1KHHCkLpxK8qkIXv362nSpDC1amsOJkyZhfWb0xEd3wAJ0WGmvjZnlWHlhu0oqQzD9FlzUVxeIflNrBIi0jIKMWHqLCxdmSpuDvKl0ZMbN0aBZGzKrPnoeVQP7JDnaOaCRagMi0Ss9EfWH9tUql7aex3mL1mFnbklSJAyUSBZmZaBcZNnYuPWDFO2AmmrxMQERNgNc8VPYVkoZi9chjatWqFD80S3LkqAOYuXY+HqddiWkYPkpsmIEvvUjGJMmDQDaek5cKSRikscNG4cjwiJZ/Yy1ttypG7Zhih53uKjXKUF88G+ybxl5RejobQ/02B+lq7eIOXORnOpB/pZLe08V/rJ4hVrkFdcihbJDY3AxD45b/FK6aulkv84MPuZmUVYvHgFkqnYlEqUZAz8Tc/Ix+x5c9D/mKORkhBr0suWRpw9fzmWrV6PrTuz5RlKhjxqruAqgTZszcaM2YuxYvVWsYiXfEaYdOUxQ5GUddKM1VJPK1EiNbFN6rWosAh9+/SCdCtTFqbBvG7JLMSaTTsRFhOHyTOXYfnarYhJagLp1pizcgdmSRxOaDQaJ4iFwK/rC5aIXXAwEmLlHSt52Zq+AyvXpaJhSrJ5FiZPXYD27dsgpXkSxoyXOt6QJu/CHClTHuKSGkriYSgoLEGS9P8oyciKDTtF6C9BYUUYps1dilUbt5t3Rox0UNbP8g1bsWnLZnkujkWM1B374PasIkybNQ8r129CfHJTJEQEm+eU/mlYSKvk2Jwu7w/Tng5ypb+tWbsBvfv2QEJimHHPzJPnZM5KrFi7BZvkGYxvkIzYcHlHihufEf7OX5KKOQuWYfvOXKRI/2Kbzl+2Vp7xUMRGR5h4tu7INn0hsWEyi4gFyzYbYT5H2nLMpNlIyypGSrMG8g6SOpqxGOu37ERicrL0b7fPML98NhYtXyvtniplzEOD5EZGUUv3NRt3YntmAQpKgjB5+jzpiwWIb9gI0ZLXLVmVGDt+mnnvllY4KHRCpX4TECuVtWhtutTrAmnnTQiLjESj2BjTB/gOYF3tyCzBL2Mny/tnu7zD5fkPC0N0RBRSU7cjODIG0xeswMa0LCQ2amjqf0NmqTzXy+WdmWqeX743WF4+36vWb0dGbjGCwqMxdso8pG6VMqakoFLCzVqwXupE3tGR8WgUF27KxPRpTJ+UC/bJDdtz5V25FMvXbUJIRIz4jXQVCh54T79rNqXJ+2smdhY65nldk7pe2rY3UqTzZOcVITwiAolx0abOF61YL2mGYsmK1dLO29FI6j5SGk66HqbMmIPlq6Q9xT1ZnnnGzzwxjdWbMjBb/pYtXrUOYdFxCI+LwtRZi6Wd1kgahfI3RN42wZFIigk14Zatz8KcRSuxesM2E3fjhnFuP5M88O9chbwFl0vdbdi4zSifU+W3aVN5LgT62yHv1uUrVso4QN5V4WEmH6wl1pOiKIqiKAeeeqc4WbNmDYYNG4YrLr8CDWVAZiQbkdjCRar49LMvcNONN6G8rBynnnoq2ndojx49emDw4K/w3HPPYeKEiSYsFRr9+/fHV199jccee8woSqg4ePXV13HSSSejSZMUk9Y9996D1159DZs2bsSYsaMRESGDquwsfP7Z59ixI0OEmkzExcbhlFNOEfsc3HTTzXj3/XdF4NiK77/7HpMmTcLpp52GWBGwRo8ahYceehi/yu/kKVMQFhqGr7/+CtNnzMBVV11l0svLzcXlUq5kGZhxCdIPX36NxKQknHr2mcadJ5pQgUCl0F133WWWIu3cuRMjRgzDFInzwgsvRKQMcKmQue8v92Hs2LFYsmQxPvjgAzRq1Ah9+/Y18QwZ/A3+/OdbkJeXi5EjR0k9DEakDHhvkfxHxUZh9rTpuPaa61BYUIgVK1bgow8/RI+uPdC6XSvkZ+Xi+uuuw88//4zxE8cjdf16XHrZZQgNDTEKEA5iOaB757330bV7LxHoovDjL5Ok3O7SpmVLl0peGqBFk2T3S6P4DxdTIMLp0O9+xOix480+ApOnzxbBZQc6d+8ugpD7xdoOSCmQfDFoKH7+dRS2yEB2xuy52LwzC8ktW2NAtxZmEDp20hIMGfIDtm/fgUVSB5MmTzEDz66dWhsBaub81fjsiy9NW82fPxsrV61C/wEniACxAoO+HSrC6LFIigwzfj+XeNasW4eT+/bBis078OnAb0UIFIF7yRLMW7QYy6RP7iwqx4iRv2Dl2lRMnDJdBvyR6NiupRGUFq3cgA8++gh5Up+Z2XmYOHk6Ihs0QbuURKzdnoOvh3yHGTPnYI0I8MuXrcCcOfPQqm0nJDWIkXLMw7yFKyTvIcjOyUXDhBj0aOv2Tzsg3pyeJQPqleh39NFIjg7D8o25GPL995g+cybWb94sdTkTaZn5aNmpM6KlAqeIUDv0h+FYJgP+5TKoX7YqFe2690MjkXGzpKK/GTIaQ4b+iM1b07Bi5Tps3JyGpi1a4aheHUExeMycdRj8zffS3iHYuGkb5i9cglYdu6FBdAimLVmPT7781vSbVSuXIUaEw26d2puBPckrKMX4KTOwJT0TuQVFSF2zAj27dpb6isC0OQuxedN6rFi6CPMXLxaBaRHatOuCZonRSMutxJdffYuZs+Zg/cYtGDtxslHO9OjcGptEOBs/cZrUbYE8Q9koLy1Cn549EM7GI1JRWfkVmDV/MVq1bIlOLZKwXYTeT774WgTfuciRduGSFyoiuvXohKUiKI0dP8UIUDvStxvBtFOndpi3ZCt+GDYCRSVl2CjvhMS4WLRtluwKb2KY3ODvR5j2P/m0E0xd5Uhn/fjTQUbg7ytxj5+5FAMHfY0tW9KQLv173PhJCI6IQ5d2LYxy46NPvjbKtqO6tDNxrl6zGZ9/PlDeV0cjNjbCzBCxguu2HTmYPW8u+h3TH00S47A5R/rq54PkPTdJhKtNmD13PtZL+3TodpQIUsDU2Svx9bffyzOzA8tWrJOwS9Cmw1FolMSnCvh+xCJp9++RnZuDWbMXIkPqI0We056SbypfmCYN8zVv2Qa8//nXIuSlSX2twrhJM0WozEVqWr70txmYLwLy3LlL0Kh5K7RsHIt8KdP7H3yGhIQ4dGjVXOoqCD/9OgbjJK8nn3ayqbwJUxegrTwz8YlJGD1mMtLSpS3L8pGTsw09ex6FHTuz8cXAwWjXriuaJ0Rg2JhpEn6i9GERaDdvw6Sp07Fq7Ua079ZLnl1ghbyHNkn/P+54V3Eyf+Vmedd9YxRriyTPE6ZNQ8sOnZEkAjJnbxmFia9uFyzdgE8+/9IsV9q8datZzlFRGYw+fXvKOy0EO/PZrkMwU56xrKwMTJG45s5fiDZde6JJbDjW7izFh59+K8/1bOTly/ts4kQkJTZGk1ZN5Pn4ESVlFTiqQytTl7PnLTDK+X5Hn4AYEaI/HTQC3/34k6S5Qcq2Br+MGYft2WWYPX+Z9NfZ5r2zcUs6evTtbvpYntTtwEE/SPqLJd5Kee7nyPtsPVq374b4yCAMHz0Nw4b/gs3S51au3SBtNRWbtqaj79E9kL6jCBMmTkFWTp70izR55+ejR8/uWLBsPX74cQTKykqxeeMGxLOvt2hm+rrtB2tTN2P4yDHIkucnJ3sHGsRHSR9tgM8+G4zFy1ZL312HNHn/du/ZE1ki/L/13v9M3nPy8yWPs7F2/Ra069TDtNWUeSsweOgwrFqzEUuWr8LkaTOxXd5ZS6WfjZc+slDetwuWLke7zt2REhde9cwR/k6eu1jeecMkzs2S9hrznmjepgOaNYg1fw+YX/5SobUuPQ9vvfsBFixeImnkYvnqVLOvyXHHD0ATeUEO+3ksfvn1V5x16onIKZX3hJRn+cpVWCx/RzIystDzqD5mps+gr4Zi2bKV2Cl2/PtSVB4kY47WRsk7bsYSfPX1YHk/bsGChRIuMxuNk1vKM7nIKD4cecizZfzQo3sPJEYHY8jP08z7bfMW9tktGDNuAhKSW6FjswZYuy0f773/IdasXY/F8l7cvj0djox5hv38i/xN7oImSTGmfF99NwLT5G/36aedglDpzJyZaetIURRFUZSDgFPPGD16rNOhQydnzaq15r6itMj8/jpyuBOfkODMnDnbydiZ6XTt2tUZPPgr4/bZp+87DRsmOS+/9IpTUlJi7NasWeP07z/AGTH8Z3NPjj32OOeOO+80159++rHTqlVLZ/bsWeZ+06aNzrJlS8z1hx9+4JxzzrnOtq3bzT257dZbnQHHneCk7XDtZJDlpKSkOI8+8k9zP/irr53g4FDnv6+87pSUVhq7od8NcZo2b+6sWpdq7n8e9pPTpXM3Z8uWbeb+T2df4Dz113+Ya8dxw5AXnvm3c0z/o51Vq1ab+7Stm5y+fXo7Dz30V3O/aNEiZ9iwH52KijJz/9e//9U5qncvc52Tk+M0SU527r/vHnO/M32Hc/WV1zrtWnV0cnbmGrszTjzZefLR/zPX5B8P/dPpf1Q/c12eV+o0iGrg9O7e29m5I8OpKK90ysrcvFWYfx1n2dotzt0PP+4sWrfdKZX7vz39mjP4p0nGjbWfUVBifoskGE25XO+UeKfMXeHsKKQvxxk5a7Vz3g0POPNT08x9icTulsZxJsxc5tzx0OPOjKWpplYWbsxzrn/0deepryY5xXK/bFOxc/O9TzmffTPRYe8oEPPdyOnObX953FmXXuRkS4L3PfaC8/bAYSZ/+eXlztxly5wcSeDXmQudW/72lLOlwC0T8/ns/750Xv50kLlfnF7oXPXAs86jrw101uY4Tqrk994X33Gue+RpZ8aWnU66+Hl54Ajn3n+/7myRwDvk/p//edv5YMhPJjx55+uxzoPPvO2wpyzLLHduf/I155XPhjtbsiUvUonP/OdT5/+efdvUBavj/W9nOM+884OzOd8x5bP1bLvE9PlrnPv/9YKTKgVgCz7y4v+c1z/7XsrgmPsRM5c7V9z7L+ezcfOdDLkfsyzVmbMtx8mUa/a0v7/+lfPusKkmrfErdziX3/OkM2rBFnM/dXmmc8/j7zgvfTjc1OVqCXTzwy85Q8e6zwL9PPXqIOfl94eYeh6zKM35wwMvOx8Pn2buGca2m2Xx+lznjkffcEbM3uJkS2FYnmWbipwb/vaG89rnIx2pVidd2ujhVz5xnv7gWxPPsrRC59fZK5ztvsi+n7xU/D/nrMly+8WPkxc79z39jrNie6lpUxpf9Rg27Ch3/vbSF87PM9ab+88kf4+8/rmzXuqUzNuULeV+xBm/ZIPJz8Rl25z7n/vAWba1xMS1Vcp9919fdN78xG1Hppkn/xRLInS3ZZyxeI1z432POdMWbzT305dsdu555EVndVqps3JLsfOXR//jDB01y2GyTOebEVOdBx571dkg8WdIJM+++YUz8MeJVfHNX7ze+fs/XnTSpN8RedyqyjV36Qbntof/5SzczF7mOG8M/MX5x7PvOis35TpZ0vcWpWY6Nz34tPPukAmmDict3uJMW+a+n/LEPP6f75yX3vvJxDd/TYVz4z2vO2NmbjFpL9lY6Tzy3DfOv1/7xjwvhP4qfImPn7/BufDOp5y3v5vs7BC7ScvznfNufs75+3+HOquy3PZ78o0Rzgvvf2/KKl3b+dd/P3QmzltswjOa/33xjfN/L7xm8sZ+efdTHzjfjptn0t8qlvc/8ZkzetY68wyyjmevTnf+9sw7zpLNxabu3pbn6Nq/POn8MmuVSWOxtNVtj77q/G/YdOM+dOIc5+HnXzfPGdN/XNIfPnG+XLn99sVPhzhPvPWR6W9M01dMZ7tU3r9fft957o3PzDNIt6+Gz3Vue/BNaUPG7DjD5Fl68uXPndR09122RSK55aHnnGffcd8TA3+Z6fzhriec5WlFJt6N27OcrTvznXR5gB9/+1tnyLgFxh+ZOHWW8/A/n5Q+Vmael8de+co599qHnJmrs0w7vT5ojHPRrY86X45bZJ7nX+aucS6746/OzPU7TT0OHjvPufeJN5xNvr48f0Ohc929Tzg/TXaf0XcGjXKuuv1RZ9w8t0+OX5juXH7rI85E6Q9k4rwtzn3/etOZumynuWd9PPjUG86/33L/fpJCSYjvHtYFa4Dp8vrLn6c4d8tzsmhHiSPN7izZVuxc/8ALzr/f+MpJk/xkSMNtlwr61yufOo+++ol53hg/n7db5Vn4csxsUz9fjpnr/FHeq99PXmHacsT0Nc6Ff/6n88L/vnPSJGGpCucWeWf877vxJn23Fdxf5mPUtNnOwtTtJq4dYvmgvAuef+dLejF5pT39Mu4X3v/aefbtzx1pGtP33vpmnPOnB5+RdzF9Os7HX49wHvzX86a8m6QB7nvibefR/3zorMssMnlnmE+HjnLe+GSoaS8ybvoy56FHX3Q27SxzMiTgrff/n/P1sAkm3Z3iaZ08v/S7amuZ8zfp56OmrKx6ly9MzZXn9BnpM7PMc5Am9fXp8CnOn//5krNaElwqj/eVdz/u/N+rXzjbpT/yWdicVeb89d9vOJ9+N1bu5HkT+0efe9v5drh7T8or3fIoiqIoinLgqZS/u/XogwW/MQF2P43gEPcrqT0utayiHNzws3FKCmJi48zsB7u8o6Q4Hz179MCdd91TtS8JvxRu3rQZhQXFmDB2IiaOn4z4uHjMmT3buI8ZMxo9e3Y3X3pJixYt0bVrd3MdFR2FkJAwSSfe3BflZGPypEk457xzkdKosbHjl6TLLnM3rSUlJcXo06cvbrr5FoRzDrVw4gnHI6lBEubPX2Duvxg0CP2P7o9mzZqYey7JkXYw17b8nGEzacxYnHf2OejYsYOxSmnaApdLWnNmz8X27Tsl3z1x7rnnShlnYOSon5GTl41ESYfwNCJytW+WS8PkRrjnnnvRsGEjU1+b1q7H3LlzUFpahgmjJ2LyuKnIzc3F0qVLjX/mPDoyEnfffQ8aNmog7RAkdeGWx/1XMDNjuC9CsJkdktKsBcZK/U6Zn2q++jWIDnf3XBB/bE7uA5IYG4Zj+nZGRnYxJi3ZjPTMHIRExkol8Bsh43bMF32ybOkKaasE9O3WxtRKm5axaN2qBSrKSswXNs5CSWjQCH2PPcZ8/YsW069fX4RLvtPS07Bu/TbsyMzCUX37mS+Q0SEh6Nu1K+KlSzlBoQiOiDb5I/zhyv/gYLfflJZXIFL6Sb/+fdBOmr9NFNBY6rBRSiP0aNbQpNWhfSfEcM28ZHhjWg62bE839TFq2hKMmbEGJQUlKC4oRHZmiZm9wKU5ffoejWYJQIwUoJm0Z4nYV1ZIX5P4wqRdQp0yJMW4S1bMxoCsyKoKl7qWfhIZGSpplUr509G9ezc0kczwKTnumC5o27ETdmbnmVkN/bq2QXxCPGYv347lawqQlVOCgqJyU9ZVa1PRrHkrDOjVzNR3jy5J6Ny5GyrKHRPXxg1pKCoukT6Rj/Gzt2DqzE0IDYlATpZ72khZmYMmTVrgWKl71gVrrSqbPsLD3WU9MTFRSJBftlmFPL8REREYMOAY8KmKk8QbJjeTvOWbZVmdUqJwUv/O2LwlB2PmpmJtaqq0ZxRCQ4NNvkIl92HBDuKkH7FNWRZfExrsuyA4ONTsj7FBnpMC6VuLV2+ECN5YIn0qIioGGVlZJj9BQdyHoxRRXHshJMrjc8Lxx2Hd2rX4bvQCZORLv5GEGS1jtmkd1aM9mkr9LV+Vau7nzF+EuIQGaJMShrXybHGj1a49ekGa0qTTt08/5lzKtZ1b1UiaQWLcPUVIsTwcFex/oW4+vNAP9yMKC49ArlwvWbUOPXr3RqcWcUgU793bJOGo3v2xVPLC5Q39ezRDm/aNMWV5NmbOy0ROvtSstB3zzz7DJXRduzUz9dm9ZRA6tOsifdDdsJNpGeO+blEsBUlqnIwTTzoejSSC7l1i0Kp1W/Tr2xcdE2GWeiRIuSu4DkNgsDLpUZVseN99iKQNed4IZ5RJj4cjbch+FyLegiSxqMgI04fYpsFmkxY6hJi8VEo9te/QCX37djThOzQNR+vWbZAl/Zz3fHVyySBTzCwC1koZ06U/jZq/DhMXrDMbVu/MyBDX6uQVFMg7MxfHHHssGskDyBx26dxF0pR3ndQ363LypIno3qML2iSHG/dm0mn79jsGmdm5Zsnbpq3b0Evatk2Ku1ykZeNENG0YY9IslGckyNOeZfJO4Yw9vkeZb86A6t2nD47pkAju+NOtc0e0a90c/fr0QJzcd+3aXl6LwcjIcTc6Xr1ug/SPKCxatgXDpq3HytXrEBEZbdqNFBaVmFkfx/dpae5bt0lGtPyNtO+4qMgQeQ+XIyGebxtpOzH9+/ZGeloavv5pGtJzHURJ1ZuW9IUhrNfKynKzN068PCfMW5C0TVBYBPr274cU6eQNpOHyC8qxYdN6ecf1RTPfO6xbiwQkN21mZkYxypy8fDRt2QqnnNjZPBtt2so7KilRwhyFFAnQSPpUg+QUs7yP7zALy8hlYGce1x/JTRpj6pJ0zJi5HkWl8tRExZr6dHugm99tO0rNzJ3+xxwPaRrzjurdq7e0a6h53xL2mGDfXjumjuQ90f+YY9A8KdLkrVgysHDxErNscs6izZg8bz0K+E4vKjNm9aqtEkmw2cSefTlR0mmbHGVmB8VGhSJC0uKcGdYD87RC3j0JcYk4++yjIcVEiiTNmbIl8je4sFiefykAs8GxQWNpIj4LDRND0bp9O6zduAWFcp+6ORP5JaXoLX/nCPucu1eYoiiKoigHC/5dr1eEiKRNIdTCgSxZvWaNUZg0a95CBnIyQDeDIA5bRBgpK0dSUgMZLPoHq+XlpcjPyzMKjyFDhuLLgV+iUcNGuPaaa8S1AuUiyDGMpay8ROwowrnxlZeXizDJ4Ym4lZbKdYUIvM3MfXGZ6y86OhrFxRxGcxAvgmdImAh27uaXJDm5KU464XgM/moQ8gqLMW/xQpx13jk+15pxioqxduUqs8eIl4SEBMmfCPUiTK5evVoGYWfh2Wf/jfETxmHpsqUI8e33wOU38fHxZvmQJT8/z+STeczKzESs5JFLooYP/wlffPGlDAZL8Pe//d34rZSKjYqKRksZ5BJ3A0sO6/wDVI5mec19WDhYvPmWK3D8CSfj088H4bW3vsS67YVmv4gwaTv6434FXEv+9Etf4IOPP8eKteuwev0GGYCHi4zktiH9uanIr5SzUgzv7SDabN4nFvSdnZOH0AgZqMZFIk/uKejwvIgSaXOeexAqUllMbAzCxQ8VO9zTw8ZTXFYGJ0TEcOln7EKML0gGzq745Q7Uy6T9mRu6M3xUZKQIDrFm/xDeO5WVkp5UjAQukjooLi1Hevp2rFu9BmtWLEeYCOVnnHwC4qMjzKbClTIU9m4Eyk16Q0M5PHYHwBROqGQSr6YsLDdlSCv4uBvmMl8Vps4TpW+ES3ibP0KlBAU0PgHTF27Ea699gBkz5mIZ9zEpYgzc8lSEROmj7B+MjwoGnlhEoSJEnimWj309KTERmTt3Yrn0qzUrV6JxwwY4/ZTTjJKnUiTDCBHkY6O5MSifJEEiM3nmtVBRLrUt7VUqv7bew8xeQFK+CqbilpMW7JPMM89a+c97g/Htt9+ZU6C2imAanxhvwhA+z2XSdsXF8my6VtWwihPGZ+pFnt0iyevqDZuwXATN9B0ZGCCCMvdAYZ3l5efIM8N6rUSJZJztfs1lJ+GSiy/C1MlT8O9nXsaEmStM/2DPoGG8rINu3Xti49Z0rMoUYX39RnTv3t0I1xUVZdIvuRdEiBGuSbA8A8Ui8FBYZw6NoM/+5zpLnGFSnmDYjXm90A/Ds28wvkrxF+bbKJZ1wJqMjIqR/uz2hRkrCvD6O4MxduIErFy7FgXyXIu0bvxVBjkI4XocKQjbhG8wtjk3xjSnCvuwr14+R8HSZuG+9wp7a1JCHEKDTIub9Cqk34aGuWXhPZcX2D7rlkb6iFjQylhLXqxiXKqKt/IslZtngJh3vbhTxcf4uJcV+7V9Lphv1keFvBsI/w7QMMY1G9NRIHW7JTMLy+X9snDJUpPWcccNMG1HfFkzxMXHGeWihSeO8blkXDuyKs2SzchIdx8JkW0NCXzupO8z9cysTPn74b6jeW/7OftRhbxDWTdVyPuFbcdTWUzpJV/RUVFV5UZpIcqLchBa6f5doUIpjJvSSJvRT2lxKQoLS7AudQNWyXs7LX0b+vTrg1ZtWpl2rJQKio2OrCoflV+REaHmXU67oGC+g8pQIn+32BdorrzwJFx19ZWYt2AhHnv8aYyavMD0YZ+O3OSTaZfLc1QmAn6ZdCXa0YSEsl3cuqHJyslEuaRl3yusD6YRERFplogS1gkDWzeWLy4+1vQ3W39sWx6fzzRsWfhYcKnnqGlL8eRTr2DKlBnYtHkrSkqljuUladOzsL9RuRvlU6rwWa+Uv+X8+2EjZZl43LUJK4UMln6WkJRg/Jr6kX/Me1s8rJG+tHzFankfpaN//2ORmBSFAvkby/e3iUf8M+98Pk0flbCOlLWcHdzHpg0bzd+QhlJWt4XFn3gMovZQDLPGssfHUc3jxsN3Yu/ePbFtewbmry6Sd9h6yWMDNG3q9jm+51hPiqIoiqIcPOyYsh7gjmpcAciRQao7qOVpHLz/3/8+Ml9vI0TYoWKDozDHfKGkszuYrvCMVSnoNmvaFE8/9RTeevcNfPDRexj09UA8+NBD4hoiA7QQrFq10vUshIVGiJ2reDGDGok6MdEd9EXIoIeKi9R168x9ZBjzBHPKD/crIRUiTHOPjzIRogkVM+TKyy7B4gVz8eTTTyAxuTHOOvdcY084mIzg5gQGtymComPQrl1brF3jzmSxTJ02HU1SmsjgNAYvvvgiGjdujBEjRuLF5/6Da6919ysh/NrP45q3b99u7smOnekoKqa7YxQi3Lfl/r/cj/++8RI++OQdfDzwQzz5/FPGL7+CsSU4I4WY5jA2bvsQDtqsHX3FiZ8bLz0Rr7z6GDKyCzD8p5GmNDQUPMmc2cuwdt0mXHv9Lbj9kpNx/oUXy0BehCapN8JkbGds0aIZymSwy4Gs/WpXVFCEYJ7UIdcctHK9Po+2pZqKA/5iGQGXOxVonNIYMTExyM0rREZOgQlvv6Yyx1SS8Gt6uVjYvOUWlsrA1W0HEcWMIELNHMMwd2EiwHBzW96bclEykbxwbMy0KFCddMoZuPPGS3HnTRfhnpsvwBXnHme+RHKtO2dBsF/bGjR91aNJoULAqDYkQyyLW+e7QpmTX+hzc/OQl1dghFkOo7PzxWRlS/9INsqQLwcNQfduffB/N5+Hmy7pJf2po6ThCqihYdI/tm3DjjwRhOVe5F4UFhUa4dwtm4PSomKccuppuPvGs3HnDWfitmtOxQkD2hhBhOWhYoS6EatUsPmtKp/UXaHEERIWYfJIGD9nU9nPvqb3SB3SnmWeMGkxNm7aiquvvga3XXUaLrrgEvMsURAinHlGQSM6NtQV8MSYbuiDQjL7pVXEJMUnIrlhMq655ATcdu25ePC683HnlWeiVfMkk3aF1DmVUNGxEqfkv0TKQ+HnlKPb4Nmn70XDxk3wy6jRRpAyZRRjW6x3r17IyMrBDz+PN5sOH9Wrh4kzJiYCefn50p+49apLbmGxCGmcpZNi/JRJxfG0HPvUZ0nCufKsVVKICoDhXLWDW172oB0Z7iwE1gHztn7TRrRt28rkbdBXXyE6Jg6P3Hkp7v7D0ejctSvyC9kjxH9EMLalb0F6RqlpE5YnMz/HCLy+R9Dt37YtfW1FxQZhM5SWSFwOU3XrJDhI3MWfm0MqH4pFCHXfjXQvc0S49MyuIRRGDZIOldWR8vzYklMxzP5g2poW8l7Pz5d+JJWVILd8H7DvUuFi64OCN7MfHRstbViK088+C3dLOz943UX4542X49KzTjKKF/rxFQ2R4RHIzMwxey1ZyiqDUEaFjHiKiwsWgT9M+uMmEybal0Hue5PcuJFJOzw0FOvWrjHXLDHbwyij5KK0pBCp4tdSGhSFEomff66YD7d87vNGgipKzewm98wXqTupc6olqWphvHFREUiIj8WNVx2Pe+V5vOMaeSb/eBbatIwzaTIvVDr5mlHqjfHz/eW+wyiKl8nLKkL+trDfSfc0m82e1KMF/v3YXejUrTcGfvUttmcV+/z7qToFR54r8yv3rPdK+ftm/SbER5uNmrlXGOOn+E+3/NxcxEZFmTLwxDBXVe17x8l1udEysJyuf1MncuFeu7B+s/IqMWrUePTpMwCP3nkJ7vrD8WjZpo2ZGUesXxIpf08T4uOQnp5uwvL9n5eXb97BdoYGayVQ7UBFPXHbhMqXEvk71Bw3XXYybr/mLNx+/Tm4Weq9UYzkXxqZHxuoBGUsbH+mxfrnOKBExgFBvpmvpHWblsjLzcEm6W6uagTIkXc4/1bL69z8DWR9VPqULTZnXTu1MjMbx0+ZjWUr15pN2hmeOa2ee0VRFEVRDgZ27FZv4Fey7OxsjB49GvPnL8TXXw3COedw89RgvP3WW8ZPvgyEOJvEfnmkAJ6fz0G9uTWcKoJfUmICLrr4IkyZOBWzps/FG6++jV9/GWncH3zwQWwT4fvRRx/BjBkz8N577+KbbwYbt0aNGmLW7Ol49733sXL5ckTExeOGG6/HG2+8hnfEHzcjfeTRRyV/83HdddeZMBQaCgrzZVDsZoJfkMjJJx6PJskN8cqLL+HEk09B0wb+WS78KsUN6RbPm49pU2aYPIqkgQcefhDffT8Ezzz9FObNm4MXXngek6dMxW233WrCccYITx9avHgh5i2Yi7fffAe5OXlGAOcGsccc0x9PPPE4FsybjS8//wJPPfWkOwtAKiipSSMMGHA8/v73v2PUT2MwZcJ0fPT+pxj4yUATd7AMcHPzck1ctVEucVERExUdYU75+M+7gzB67kYsWZmPYhECY2SAbrEdLCIsXAauHFxvwbJMYNLEySgrFkHLoyWwzde+fVvkFhTi04E/YeqyHfj8y/FYs2qt+RrKoXK/Ph0RI3Xw9bc/YMzSTIxeko5PvvwGjZumoEVyLFo3jjXT6Ef8Ohbjl+/ET9OW4OOvhmFDRjG6du0mwlkoPvniB4xfnYn3hs/G7LkLEOybAcKvveUlUijph8w7B8T8BBkkAi6Hwhz0U0PniF1FUQU6yki6fbv2+O7HERizMA0TJT+DRy/ChDmrzaA9ROKTCOXKr9UrKS1CYZEr0DKNxKQErFq7DiMnLsXCNemuAOQZGVN4LZH6pqDfvnE4eh3VA7+MHINfZ6zC7LU5+OrrH83g++i+3Ux8YeE8MWUnFmcA4+fnYfXqFeLuzv7odVR3M7gfPPQXTF6Zha+GTpX+P136oisQH3dsazRv0gSffPYZxi3OxOTlOfj4x5kYPmqhr30qJC8FIu+4Ij2zabNqfxs1SDSzGUb89CumLE1DgSQcGhqE0qJ88eN7PsSUlxdLv81045B+UFBYiG3csHYLl0pMQW52HmVAQ5MmTY3CaObMxVi0MQMFUp326zihMFpUWGCWc7El+3bvjsWzZmPoiPlYmQZMWLEd7w3+GdmZhUbQaZKcbDZhHj9+LhZvzBMhugyffTUG349bhoUr81AsQl1CkntaEHNsn2tmp23LaDSk4vLX0YhPSkLLFnGmdbt162iWCn49eAhGL9qOOeuL8MVX35iZCe1bRxvhv2/vXpg1Zy5+nrcV38/eiEHff4+ysGBIl9wFClKcKcZ3C5VcZ5x8PGZOn4mvR8/HlOWZEvcEbN+ehlNOONrUZ3BQpQireVjHdl9chFUrV0rmy41w3a1bE8TERWDQ199g5sYifPT9XMxdthBlQdI4Uo+eqjRUyjNeKu0c5FN00b24KE/sXWGV6TnSfqXFIpTKNeu0ccME/CLP3JRlafhp+lpMm7PAnNbEmuPJWTxWnoo3xhUuD0dQSCXGTJiEsYtTsVUyGSLvCPZx6aAmzvDQSCxdvhZfDp6C6etL8dV305FXkI8Tjz/GuLOty+RZYr/u3jwOndq2xvtvvoWf56Zi6srtGDJhLiZNX2BOFHKFdbctkxtHo0OHzpgwaRbGL8nCmAU7pC1/leey2Ci2YsXzaaedak5i+37sPMxbm4FPvpuE9anrcNKJJxil2HHHHI3UdWsxeOQczNmQi4E/T8bIyfPcJVRd2mOG9L2fFm7FDzM34Jsff0VRqSN/E9zZMXx+yiQtC9u5tKgQQVbzL32vRP6ehfj63PHH9MH2bZvw+ZApmLkiA+PnrsXAYeOwM6vECNJU2JdIP2GdGCS+EnnWWD8kjkqlklKMHT8JM1fvRH4hN+oeh0EjF2DRulLk5ZegRcvWiIhylV4W5pX9oEzyW6WWkf5QUlRUpVAj3KC1S4f2GPvrKHw/aQHmpO7AN8MmImN7Onr3dJe/VkpeaIKkSMwnZxWWyjuQSmqmQ/syee+ybgjtLFGRwfI3JRbZOflYlQP8sjALy1YsBzdINWHpScKztholBaFLx9YYOfJnjJ2XKv1wFYaPGI4iqV/jQSgqlfep/TAghn+HHKkz06fENI4JwplnnI5fJY7BP0/DvBXpGDNlKUaOX4BcabYundrI3+AYs/n8bOlno2atweAfJ2On5C0pSWKRfE2ZOQsTF2/A5jzgxJOOQVhYEN5/72NMlb9HU5Zn4OdffkWPbt3QPFayVV6KUik7Zx56aSgvn+OOPRbT5JnPzMxC7149TX7ZLfQsHUVRFEU5+NSzU3WCjNJkxoyZWLJ0KUaPEeFw1Cizd8gH//sQrVs0N74qyipE4BmH008/HR06tjcCQlZWDi686CKEiODviLAQJQMtnoYzfcY0fPTRhyYeKgQuuvhCpKSkoGnT5ub0nTfeeMMcYcxTQi699FK0bdtOTGssXLAA7777tvl6zv1Ejh1wnFnC8NJLL4tg9A3y8vLwwQfv4+STTzV5WrNmNTZt2oyr/ni1OQFIhooiDJaJMBArA8Q8jPhlFP7v6WfQ0XdMMlkyfz5mzZ2LCZMn46efRmLa1Jlm7fmxp56CJimN8P7/PjBLF7Zs3Ybnn3sBF1zgzlbp16+fUfZ8+tmnWLF8Jdq17WBm15xxxhnm1J2zzzoTEydOxFtvvm2EzVtuuQWRERE4/oTjEJcQj7PPONsc0fvmm2/gh2E/InXDelMvbdu1kcFrudTVaJx93rlo16GNSc+dBeQfqOXmFyJ1fSp69+mFNg1iUFheiVGjR2PSpAlo3aoprvvjZYgW6dVMJ/YpRlq3aCCD0ghMnzbZnLyTEBeNZBG0ussgNCUhxgzLORikbx7TGB6dgNlSN0sWLUazlGR0lLzERIWiZ9c2aCjj+w7d+pgjn7lnzdy5s9A0pQFuvvEaxEeGGmG3ZbsuWL9xM2ZNn4olC+ejaZNk9O/XA40oaYTHYdGCRZgrAmxL6VOtWzRFcnwUenVqjYKiMmxYtw5dJb12LZLNWDs1dQMipF/169nNCGA7ZYScl7UTfWUgGxMeLPXWCdvTt0v7TcfSpYuRnrZZBImWaNOsMXIKirFx4wYpZ1s0l3KxfDw+uVgEzqOP6YdIGVA3TGkiaazHrFkzZBBdIsJGVzPDhWnzCyz3AdmydRP69++LxMgwtO/SCXlix5MeeNw2j6a8+srL0So50giw7dv3wJwZ07Fw3hKkbdkg9RyLFs0aSn9vjhZS/oRGzTF10ngsmDcHsVERaNOyGRomxqJH93ZmjX+zZu0kvS1S/7OwePF86RP5OP7YviIgxWBLehYyd27H0b27IS7SbdsqgU1gO8aLUBwak4T58+ZizfKFIkh2QHxcnDyny9G7RxekiKBFsWVt6kaEi7B/wtG90axlMrKz8iTfM5C6epUIJlHyvMWhV7fOSIgKRkLjeGzclI6Z8jxn7khHt85dECttzTpiLorLHKyVduvUrjVaNk2UZzxa+kQrTJ441igQebpPs5SG0n86IUok6QTJg+OEYfzYseakkX59uqNAhNVpU6di9pxZiJf+cPGFZ6Npgzj3C68kwt7JawrOuU4k5ixahHPPOAm9Wyeb8sSI0NymQ1dskPfA9GnT5J3C45Gb449/uBRJPPZFaN+uKTZuFmFr9Chk52ahU+f2iI4MQb+juiE+3FUruWlJP8vOlXfKRhwt7d5YhN8WrZtIBYdg1szZUrfzUSjC8XXXXIFebZNMv2zfvisWyjMzd8ZcafdN8hzFo1mTBHTt1hrJ8jy27tAF8+fPxfSp04yiqYu0C48X7tOzEyQLJk3LDmmL9PQ0HN+/FxLEkSdkLV++RvpKMjq0TDF+ufdGVEQwevXsbJSKTVq0wry58zFX8lYkwuhRR/U0+2p0696BEwuwasVqdGzTEh2aN+Y2JmIiMH/BPGzYsBHdu3aXuELl2VuL7t26oHF8OOYsXG+OyY2Jj8OkieOxfdtmXH7xeTi+ZwvzjKft4HHKuejTuzfiJUPcN2T96rWm7y9dNA+5mTswQOquWQPf7EWfYdi2nbpI/jeJUCr9Sfx1l34REVqJrp1aomF8pDz/rVAkcuwY+Ru0SNo5OysTf7r6CvTtkmL6eNuWjdCkSUuMGPETFixcaGajDDj2aLRrnIh2ndtg29btGD9hojwrmRJ3F/Oe6NWjs7R1KBYtWY4UefcdfVQXiQnYKn6zMrbjmH59ER0VaT4E8B3Zr488Fw2T0CIlwcyAminvssUL52L1isVIaSTh+3Y3y1jWpW4xRx/3lnrms1hYyvCLJN3OaNWkEeLiuR9KKKZPn4VcKcex/XuissLB1ClTMW/ObISJoH/VlRejbXKM6cdGuSHxsq44CyxYbo47+ihEScUVllaamZq9u3dGi+RE8zxQQdy1G4+sD8b8ufNM38yTv+PXXn0lenVuYuqbp/xQIX10355mGWdxWSXWyt/MbvL3u1WThmYfrOXLVqKJ1EvPzr6j9cXweYiQCBIbNsM8+Xs5e460Rc5ONJHyt2zaSOJvYxQJ9MgZPcxLj97dsUne/TyBhsvnuknbhsv7u1vnzmgcF4L1aTvhVJbh+KN7obgUWC99rn27Fmguf0MIn6UO7VsiLCIaUyZOkL60AOvWrJK/s63Qvm1zJMg7tFPnHli4cB5mSf3NnT8PDeRd1a93VyRKBvi3a/rM6Vgm77uWzZugW8sG6Cj9e7XUG98Lqamp0jYdcdUVZ5s9oDhDbc2KpdL+fdBMysVyWxOZmIxpsxaiY7vmOOekrqYu2cZ2ZpiiKIqiKAePIBFuOeyoBzAbIjjID6fB8ssf1xnzCy6FKOPDJ4hXljuo4LrlUG7yBpSa2RFBZgmHK6jTn1+cozKGX66TGrjrg/l1nXs9MAy/1GdkZIigFGfWZLuinxs2OzsDISGh5utSkG+Tw4KCIjMdPyVZBv++wQsVNdwPpUQGrNyDgGvzuVKfU9k5lHvvzVfwyaAh+Hn0BDSMDTdflkwKFZUilJYiv6hYchwi5eHU3RBERLvl5Re6rJxcGfhybb07I4J1wo0GyU4RYLnnSWxsnFmKQaURpxxzOQbJknKZfVw4yqrk9OoKUy9Bvn0L8rLzzPT4pIZuvfDjMr1K1Zop8KxbYstJ2ErcALOUsy6Cg80eCEwtR5qAbcdtZlhT9OcLXg0eW0m3GKlqfoWWqjMD42AZrbPtGIedSZAn6VDgokDKfSi4uoVuzD4FNX6z5WwGhknwpcuewHQZBa/zStz7BBnQMl3asYW54SA/8BoFj9wzHa7a55IPGdMbS6Zj6sO3XIQCBGF6JgxvxIpftHldJHGwDtlU7tIe6RMV0ibizjwwf7RndFwawvrlxCTuJcz88GO7Xa4T6nY3Ey/922n+TJsbzPLbZC4nxvBeBvJsA+7FQPdoiaNIrqWbIlLcGBeXK/i6jREuGF4eMbO0wHQPcacUZNIXw1vGz3T5Ido+GQzHj7VRkiCjE+dqML/0R/LEH/fPiJS2MRNvxFCnyDJTUGJc0mVNPMy/KZM0epg8P9GSbwrrFOJYFeyPdC8orJR2cRAtGWVdMu9Mk/Fxq2vmR5zNBePMlYIUiXDEdCjUiDfTj4y7GM5cYRhu/2H6lITlarsYtqHcs13Y5yR4Vdnob9isVRg87Cf866G70alBhMkr3alLYr/MlzTl1YIEqThbNt6L/OyWgx1RCs58mbaRwDFMUH5tmUyTMF65Zz1QSCYmjxI/25X6GOaJhumwD+TlSR+QV4hdBcj+zGfY5k1eYfKecu1YV6xHCrPyv0mX7cT6ZP5YbpafdtYD72kKJQ/sw+wv7IesT5M3KRvTZj8TL8bN9DGJg+X31mWxVAavWf/GQmC+GNfHX45DVn4e7rjjEsgrElHiJ04MvTEM+zoV25wlR6UW+yjrlnXPZzYyKsgoAlnVLKdEWVVGpmHqQp65KMko37gss6kTMVIkY/IlMPPNuuZzxfgZnnVNeBx1flG5vINDTfsxLRaFv/nixqUYzBefd9aT6fvSsMwHnzXWhcj2cu8gTBqCcbPNyzgzR8rFvLAu6K9E8sG+ydlPsdJgrAe2C58t5ptLmAjrm4aJsJ0YqWkvSZf7RPF9RcO+zWeDeaQ3a8yyHImQ1+yDLLNrL/HKBd/bfK4Zh4nf506YBvsmV6rY+mL9sUzMr3kHyTUD8Dk0/U/iYR9n+VjOCPmHvzZOC9siX/puuDQW/8Rxggb/drBfs7ysA6bBJJhuofjnn3nGQ3vWE2EfZBZYNl6wjpkX1n6o3Ni0bTuWS3mYnnl3MwjzLNd0y2OHlzDRUl76pyF897F+ac++QnuG5d8j5jWenULge4N1KH++Td8wZRc788yL+8rt5fjvWx/jvLNOxKUndTNxcCaS/SChKIqiKMrBwegh6o/ixJeh3QwITFZl1GGUIhzdCFzawynQxOyjEDDcqik+sy5forLh/HDPBbc6eHqAxaz3l3i9ypjqMIybDge1JkmnXH7DkJuZjm7deuCWO+7B0089aQZwQXRjkGA7zKoOl9VwA76qQgq2bvZUR5UyAjTLXzgarAnJgInDjnh9cDBn47ZBKUyzyIHJ2dJyNxSG48Z7tiR0M3Ug//mz4PrzLsux0L943yWNXX26VNjRr2SMteP1x1aydvQV2FpGCJVfFt2G4wCbNW3CSL3bJTuEeaa9zZtZiy7p2nvGx0vWGzc1DoTbmPJMGOaD+XHrwJ8vDrxtKJsHQr+skyrEge5WCOEA2+ufMC+kUhJhXrzZoRNvGZ6/to4sVe7iwdQNI6+BmurUj41lVxiO2LBGYSZebU2buhHjLZOvOObXGrrZOEx5xVDxYN0Znr8mPV7IQ+YujnCx6XCRUaVkwAqkxNsPLNa/N0/s5/nS0R575mU0atoMf7/9WlMObr9p+ouvX9p4GcZkxWeIdSMmr75f64eGYaw/9mv2PbtRtjc8/Vls/IEwX/RZ0/NHqtL2ZdQIkZII06EVy0YHd9tOt+65/0rVvRj2O4axdgxHwzioPGG92XwzNl9SVXY19c03//cttmfsxD/+eZdRgNCeYV0JmH3c9Wn2DJHE+cwEPobmGRN7a+2qJlylg/dZtNBeWlLCBJn3mhc3VX9cgbhhGT/7HZU5LiYtMa6LP03ak8A8WIx//iMElou4abmCPrHvAGbbpmd2oDHvBH9++Hcy8G+Zybt4ZTr0F5ic27fda/4wbsbj71OBIVw/xOvCdPi+ZLhdQ/hhuauithUYAONnv/G/sV0CvdMfo2Jf4DJFblpOqLQgVIKYfJm/nawDd3lZTf2D9swb/8ZRybInGC9hOO9r1Y2Hrv5+XAUdfVbvfvEDFi5bjaf+729IiXadmIHdjQEURVEURdn/GHmvPi3VqTYYsCMOjxXdq259fr0DQOMeYIg7UPQqHfxu9tfFXlM45rUZpsi1va8N180/IGT8tAlGhgz8uYntdTfciIYNuFyF6/d9+TADJBuvGUqJkYGUUegwv26+CX8Zv722uCH4S2WFXJuRr7+s/jK7mJ0pfLdee166d27emZSx83vxI44mJ+LmDrStJ18e7G9A/CaA4M2T68/36zG74iu7GNYxxYBAf+wJXjs3hN/OHRS798wDB7/8usr80rfdl8YW2uTLvTS4efbn3VfVJj+BmBqidsDj33pzU/Pn14Zmfqo8Cf5Lt3V56yp9vEIwY7Jl43GqbD8jBlQZtx8xPQnni8eLvWfxA+QpH4xn13DVqcnVzTfhvzQmJrlgMr7cyb9u7IExGHdfX6kKL/e0N8+ZJ4Bxcy/NNff7YD25abgtTIyb/OtVntHNe20Nob11I8x3fmEJ8nOycPpJx6FpUnyVoBbC9wTbUDBxMA25MHn1pWLjs/EbfGG85aSxMG5z73H3pSDXbg27LjXDevLX1a7+3PD2H/eH/cnC+nLz4Fqavua7Zi7cslsBvnpd09DedWMfFzsT3nWrQuy9bUI4m6Rp08bmKHKTf/aFqvJa3Hs3TU/aYmmeb14bGybsV4y7z67r3yh65drcmTTctgyE9m4IN8ZAaOuvKxc3NpfA8O59Dfi8uG7eZ92LrQe/m/sOqB4nFYdu//PjRic58ZXbvfOHdW1cmBVCb143c13lserCEPjMeuF9YH4s3vyY8tlrj2f7/NtTcXbpR4K9Z1+zbwBmln7ZT+lujPxDV2Lc5M7a059NqyofJkIJ77tlLL7Yq+Lxu7l2vGXcNPae8fKO19a7v9yC75LJ5WRnmWVk3ds1NXGQan4VRVEURTko8O9vvZpxsgt2pBFIbfb7CVbJvg9OZJDOpTH8JO771sQvsxxqcQNSM3yjZMXRqsEdTLlDLJfAfBjBWvAOpG1VcIjnt/VfHUjc/Lh5rFvabv53paYwe/Ib6B4Yh2/Q697UjKfru1fWtzeUz6Wqzm2sNqz32g9bw/q0YQJ9eW1s21pFIG/9zSxlMc6sZwqDXkd/HP7hOQ3drQKF8N5rasMbnxeGCXSrLZ7q/lgXXp/ud1bizVtt6bougSn5fbsu3rp0/fNfG9Ib2l77/e8J25Ze7NdjPtPEpuJ+RZZrzzNK7NPpfsuuGevHH9Lm36X6M043627fGd64q4d1r73ugXj9Wlz//vaz4a1fb3ze8IFpV/cX6Lqrfz4PfM+516aO5QEwfV/q1fPYVuGtGS51MHGaKTRu7Kw7/tJXMKdUSTs5NhGJ0x/af+WGDMS629i81Gznt+W/Ns5Afz68SRovXgsvNYXfXdyBEdcU767h3Ny7VH+Ka2JPcdI98L4mAtOpOd7A2OoPgfm1uayLvadEViMruD52fUMoiqIoinLwqP+KE4uMFeztoTFskNxWZdjNMadS86uqCx09JbHN4BW4PF5sVIRWgfeHBt5cW3aX+z2Vcm/d/fb+7l9zPrzKK/5awZgETnf3wlg9qYhxxY6aU6lfeGc67Zrj3ddXddxye/HWy64xWBc/foHYKxq7QlMgtPXaV49/19R2zUlgrLWlU/3X9gje26N0vYqT6jmn2V2sdWHXfLrUFC8JjLu28IHQn9ct8P63Ulv4Xe0DlcbEr5QKNu6/Db8Cw6+c3F0/91I9f/681VYeLzafe/DH8pjoxJ/P6651UJf0dkdgeJs3srt4f2u6e5vP3YXz5pXU7K96nfnjY1tzGaurKPO/t61f/mtDKYqiKIqi1EQ9UZx4B0ye7JilDj7kksNma3NIDHK8NevJf21532WgbKrFjURcqqKjDysiVk2Z32WQrRxKlFdWoryszMw8qagoN4N8LlXg3hbcEJjty/18+Cm+vLzcHEFKw2OjedQoj4g2x5KWFBtj3blxcnFRke+XbiUSvyuA8jhvxk9Dvz169MAf/vAHxMXFGfcD0afYa20/PlTxqsvqUh73efU/9/qU7hm/UqPm2gr8s0W/+g5UyNChQ/HQQw8hOjp6l/7gfV7dPhYiv+xl9i+sYzaEDwuzG80DERERiI6KMsfsN2jYEBGRUYiPj0dMTIzZvD48LBxJDZIQJX6ioyORkJBgwvI3MTHRDS95iY2NRajd9bvOePu59m1FURRF+T2pB4oTd7DiHxQEDA5s7nzWgb4t1WOoyUcgrh9usEef/hB7npBcI4yAeAN77HgZ6IU1b6ej082Is2JZtSmhCLRVHiSUjYObF3ILSipPmFvjwyc02DSIDan8dvhYUEFhDe+pYCBWYVFc7CoieF9UVGTu6dfYlZUaRQXteV8ovwX5+SgoLEBFeYUJQ//8pbu9poKDv1SiuPelxj/T43VZaZmJk0oPwn1ZODDngJyG1xzMc+AfFkY795fCAE9j4i9PfbJh2Wd4z7LxCOszzzwT33zzjXGv7dUQKIz8FmwfPhxgLeypLPRzpD2H7Df70kcUZV8YNWoU3njjDaPIMH8TpT/aPmmuxQ9nhhGz8FEsuDeRu2Wwi33vl4nhCX7cOJYKbZ5eV+pTUpfKu5jv4fz8QhQWyru9oEDi8sdBIiMj0bhxY6NASUlJMcoU3rdp0wadO3c2yuoOHTr4fNcGc6zPk6IoiqL83tQzxYkMbGTc4V0BYXQHNayIqDADId+NF4nKuwcIv+C7ugfXzr33u7snEfgH+vzX7+rFVpM7+PIKBqzBamF4Y70Tube3/LXO/PDPIx0t7h4oYnzxB+aVMByNPU2ArkaElri8J4yYeGhfDetSv+GA9YILLsCSJUvQtm3bqq92HAjTUEHAASkVBPYLHtuEygWv4aA2Ly+vSsFBhQMHvLwvLCysCkfDNKk8sPeECgXGb3+paKAbf3nPNmI4Qnsb3sK0OOgmQRImTMLExsWZGR00LENMdDSiaKRc5j4mBhHhEeZUJfM1k0oQCccjpq1ShGnzl/5tOObRGhuOShMqS+wvlSlVG+DWwPPPP48ZM2YYxQnTOhDY2vm9eqK/dQ4ONT+Hyp7gs8RnlM8wn1c+z1ZJSTte22ecAivtGSY/P7/KncY+13z26Z/+7HuA93Snf+vPpu19D9hnm/6I153X1vDeCue8rw0+o1ZpyWfXPJdyTWOWkxD5o8UTYIgtK+O1eeBzb59RupH/+7//Q79+/cz1kQrb0f5N2O9Id+DsvRIqsE3fYf9zUJBfgKLiIumvBaYvMg/8Zf/NyMjAzp07kZubi/Xr12PhwoXGju3P9mNf4GyUCy+80LQflSq2HymKoiiKUr+oJ4oT4g4UOC6kbLdsxXo0a9YESQmRxn7j5nTMmzdXBhaN0K9vH0RGhElIhqXxDzI4/6K8vBLzFy5Ck6ZN0bJZCmRI63OVuOkuA9L58xciuXEyWrdqXuXKmrDjlerDFn9467IzMwubNm9Gn6N6mvsqvF7tjS8yGQ75fl0owuYUFRthNVYE5j59+iA6zB30cchM3xm5BZgzd74MqCtw3IBjkRQbaY73JNk5hVi1cpX5apUQbQ8CZfw2pd2xZx+/J3fffTemT59uFABWuLGCEIUeYgUe24WtcsOdbeEqCWhnB9JWWLGKByvc8J7+rUBDNyowOLWaU7L5a5UadONAl4Neq7jgtTWMKzYmFgmJCVXx0v5QgPXBOmWeD1fYU6o9ovsJ+zTx1xt//X7KamdvhDcKk1ZJaQVH/tKez2x2dnY1Q792RpYNY6/ZF+mHdlSG0F9OTo5x2xN8Ru3zzGfP9mevsGqfdT6j/LX3hOW2ZWc9eOuC116lCMNY48Vbd/b9ZPHe22v69q3KNHa85N8txsO4a8qTDcs64bvoueeew6mnnmrsjkSq6tJT9/UR/s2iMoWKlHfffReff/45mjRpgnHjxqFr164+X4qiKIqi1DfqgeLEDzPCIc/cxRvx5Zdf4P7770HrponYvL0Ar73+qgyeCzDg6H64+MLzkRAXbWaLBI6ROHwtKK7EE08+jTPOPBPnnXmimZ1hY6cInV9ajn899i/0PqoXbrr+T8aVQ2GOhxmfMXIfELXB5nFV6ka88eYbuOuuu9C9Y3sT3gydvbXpi8BauWGp1HAdMgrL8MGHH2Lz5k04+YTjcdH55yE6LMTMIGFcuWXAh58MxOo1qejYrjXOOfM0jJ8wERHRMbj9msswb6Wbh3899hg6NE0ycRIzrHcqEbybzUvrK+yONQ18Odi0hsK9+8XPvbbCBYUfKwjZXxoKTvw9XNjTI7s3goO33mtrg8OB6s/ivmPjORxri8Ldd999Z76Sb9261Xw1p/KCs7j43FkFgr2mAM9nks8aFSV8HtmX6E5oZ5WehH2MzyaVj1RKJiUlmZlYzZo1MwoOLmugvTV0oz2VBDRUYDINKkqsssQqRrzKE/sesNd8N/DaviOYD957+/zh2v+VumPfs6Yv8Jq3vqWz1dyEwPu6wnDbt283v1SeKIqiKIpSf6k3ihNmgkMOKjmee/UzxESH4S93XGOWo3w3ZjbGjR+HZ5/5BxLEU7F4kjGvUYJ4scqLvBLg5ZdfN1/fTj+xF/iNkn7tEaK8f/zJZ9CzR3dcc+VlVYoK606YH06VtlOoA2Far7z9AUIkI3+545Yq4cn4lMCmPLsG8ytYhCG/TsaosWPx3PNPopEkTpGCQXhKZoRcrEjPx0uvvoXbbr8bx7WLR2YxMHnKVMTEJ+LMY7pj/tptePf99/H3v/0NHRrH7LLMibhKI0Hi5Ik+3iwxj6SGbCr7yJ4eq90NsL1hVYA7hKitydmEdDvEmvKnn34ySwhatGiB1q1bGzsqGGgs9t4qKuwsrYYNG6JRo0bmmvZUfNCOy+6sosQbRtk/8N1xpL8z7PuzrvWwN/6t39rC/tY4FUVRFEWp/9STpTqch8F/gY1ZDl549U1cetHZOP3oLuBOFJ8MHIltaWm4/ro/IaVBBGLD3f09sov5VTRHroLRoEEcGkTLpZAj9v/5z9s49bRTcPYJPYyihEP9LTuKUFJajmgZtL/x5mvo1q0T/nTFhUaZwfgyJbG0tHQzsG/dVAb8YkdlRkFhGULC5U4iSU/LQlJCHBpHh2LMjAX4eeRIM+ukfUqCUXhYvQXjDJUC5cvFzswisza6ebMGiPPJHOkFDt7/ZCAKSkrxp2v+iEaxkUiKDkEoI5BwDD9y6lJ89uUQ3HHX3TiqY2MkRgIlZRKvZIVFXbBmGz746EM8/PDDaNco2uSXipLN6Znm62/TZk0RHyr5l8jCJV5GvXl7LkqLC5EgZUhMiDFh7NDOdgQd6inKXlCXN+kh9HBxdsmUKVPQsWNHs7ElZ2hwRgd/6xMqpCqKoiiKoigHmpAnBd/174hfcbI0NR0Tps/GlZddjEZRQRg2egZ+HTcJeQWFWLVqNeJi49C5WUPMWbEZH332FabNmo8585dg5tz5iEpsjBbJCSiuAKZMn4tWbVujQ8tko/wYNnoO3v/fF1i8aAVWLF+L9B0ZaNOhI7p0bm2UKlMXbcDnAwdj07YMCTsHO7Jy0aFzG6Og+HnSLEyYMQ/zl63Fp59/hsjICPTs2BbBUQn4ZdQY9OrbF82SYv1Lh8RQaTJraSo+/vwbjJL8T5kxC3MXLEN4bBJaNGmAOeI2Z8FibEnbjpVr1iLYqUS3Di1MOCozFq3aiu9HjEJuYRHWrl2LgtwCdJE0P/1kEDZt2Y4+ndshLbMAc+bNx4Djj0ej6DBklgBfDR2GMeMnYfb8xZg9bzHad+uFhlHBKJI4vxkxEeMmTMWSpSuxRtLs0aM7Io2mxlXUsA28sh+LoijKfuQQeaiojOBymE6dOpmZIrzmDBEubakLDE9Fhv21djWxrwoPhleliaIoiqIoinIgqdso+ABjBte+6x07M5GQ2BAhItBzmH3CiQPQq28vJDVMws233Iyj+3fCirQ8fPblIDRp0QJ333MvHn74LjRMaYohw37GjhIgPEIG0yHBKCopNfHOWpKG8ZNn4A9XX4O/PHAfWrRoI+nkojJYhAFx35wPfDZoKHr2Ox533H45brr1Vixcsho/jZ5uZqsUO8GYNH0GwqKi8de//hUnn3S82AIx0VGIiUvAhs3b3BNxJDEaKmLS8ivwxTffo0wKceufb8ajjz6K1m3b4fW3P8CyzYU4rk87DDjuOCQkxOPGG27AWWcOMOXlrBXG1bljM5x5+imIjAjG5ZdfiEsvPQ2cIV9cWoyycvcUhWCHu6UEITQswig+hvw8Hht35OGOB/6CBx95EA2atsPHA79BrrgtWpGOCZNm4tIr/oh/PnwrLrr0cpMWZ6jUpDRRFOU3Yl9i/A00xP4eAuwPZUYgVsFhjaIoiqIoiqIcKtQLxYmX0tJyhIdHgh82qUBoFgU0b9EUsXFRaNMkFIlit2zZcrMU5dxzzkKXxkCrWODkE45Dfn4hNmzMNIXiHJbKcsYALFq4FHFxsTjhhHZoL/6vuuoEtGrTGsWSFpUGkycvQkVFMDp16YZtmUBRMZUIIVi+fKXJQ2lZGRo3bIizTz0ZA9o3RdPYSGMfEU7lSRy278gwCgjuIWJZv2kriiuDcMmll6B/h8ZoJxn/w2VnoWnTFKxYtRo8ZyU2KgKxkWFomRINrvJnvrn3HPMUK79NkxMRLR5bt05BcqjYi0NwSIg52pYEOwwThJCwYGwTt7mLVyGlVSdk5vM0Hqm3Np2QumErMrKkHlNSECSZnDV9JgolXLvGUYiODDGzZIzSxKc1YRHc+lN+L2r7Mq8cAtT24ByhD9TuFCSqQFEURVEURVEOFVwJvJ5RWcFNWV3FCedWBIfy5IbiqhkRRYVFiIyMQlRUuHGnfVgYj34NR2mpOxujorwSocFc9ALkZGejQWKCKSxdQ+SCU88JlRQZmZkoKy/HlMlT8OOPY/HzzyMREx2Ntm3aGXfOzEiS8AmxISY888VZJYwnPDwC5T4FDUUAKwYUlpQiODwaiQ0amjCcucKNWzmTJicn2yhagirET3kJKovdkjHOamJEZYnkWfwElZs0KU9XVvLK+nKPraTJLwBKpQwbNm7CiOGj8c03I7B40QL06NZFfFeiTRJwx223IG3LJvzfoy/j6+Hj3XqTqBibV+mjKIqiKIqiKIqiKIpLvVCcVP/qWAkeD8MTLJk5Ki7KK8qMnSU4NBTFJWVGoWE3N62sDEJpSQkiIlyFCG0dTqcQ4mKjUZBfYE7WoX+a0pJiBPHIXrlObtwQcTFRuOjCU/DgzWfg/jvOxT8e+CMuvfhko7AoLyuT2CQVic4oMJiwwDyWlZUjIizC5MFbCqo0iktKkV9UZtLlkiAqUErLStEkpZHJA6g4qSxHCMqNH9sYNp4QuaosL+e/7qlAVLxIXXGmCXGCmIrUiwSIlARKC3JwXN+eUoaz8MAdF+KhOy7GnTddiNiYYGRLmGO6NsITD92Aiy+4AKN/HYWFi1aaNKnEIb5oDTrpQVEURVEURVEURVHqieKEWGVBXHQUCnNzjcaEmeMsjFBHXMsrqjLbs0cPlJaWYujQ4ViZUYlFm/Lw00/D0bhBIjp3SDDKjYryMlRQ4SJ0aN8aq1cux6Spq5G6A/jup7lI27oZ4SFBRoHRunkKMtO3YMKoiUiTpNN2AnMWbMKmjflGYQGJq7KsxCgszL0vtyUlQPq2NKQ0bmAUI1TtuHNAJM5WLcSiFN8MHozVO8qwLK0QH3/yFcLDQ9Gnd3fjJ6iyXOIurVL+EP5WXVeUI7i8HCG+spvyi53jK1elU4Gy0mJUlJQiJQLo3LopRg0fgrUbMpGfXYbNm3Zi9twl4IFA8+YuxpBRk5FRCjRqlISICKO6Mbn97Kvv8P7nX5trW8c6g15RFEVRFEVRFEVR6pHixE5waNkkBSEV5Sgp4Fk4rqIiOiQMMWFhRqFAlUHXFgm48bo/Yd3qZXjr9f/grTdfQWlhDq7/05VmDxQqNLhBbGiIG+uxx3QT0xdDBg/C22++gTWrl6NL5/aIcLUg6Ns9GdddfTnWLF+E/z7/Et569WX8+vNwc2wvT9WJiwhDfGQ4gitdRY49WCIjowhhIcHo0LZllbKDag8qbto0iMSt11+NwpwMvPn6a3jjtVewddMGXHrR+YiTSKlkCQsOQkgQN8Z188l/vcqLMLljuSOpOBK4nCY6KkLSZ2gpn4SPlryVFuQa5cv1f7wCTRsn4p03/osXnnkS77/zBjJ2pCFcwrVp1RIzZ0zD3//2TwwZ+hV69e6Jnj07mxOH0rZuwdo1q02cxDdRR/md0H0fFEVRFEVRFEVR6g9BTj3bibKoAnjxv5+ibZuWuO6qM4wSIm1HEUpLitCyRQPjh2IlZ3hsyChAWtpWRIaHo02b1kgIA4rFngXamZGLsNAIJCdEGEUE7VNTM8ymsq3bNDPLbEpKS5Ao7tHigXFu2VmI7du3IzwsHIlJSUhuFCW2QEZ2EQoL8pGS3NjM3qC+hf4/GjQK2bkZuO22PyE2xLUjVGsw31RmbM+rxLZt24xj02YpaBwTikKxZxwlhWXI2LkTTVJSzOk5xMZBiovLkJWdg4aNG7nKGnHcmZVr4k9OikdBcQUys7KQkJiIqMhQRIk7t0tZvSbVbDAaFRWJti2lrOKfdZKenomMzAwkJTVA0yYNq7RV2dl5CAkOQoOEWL8mTdy4z4yiKIqiKIqiKIqiHMnUI8UJs+FK6l9/PxazZs/CP//5NzSKD60S5qkAoA/6pPFNGKmCCgXa+3QMhpIKB45TichQzhWpDvf2oAIlTJyqFAYeuOcrJ3uEiiPjK5V7d58RYNuOAjzz/H9x2eUX4cwT+5i4bH6YB7MNivjzpkorzvCokIwyHp6sQzjDg/F7FRVsFe/Eg6KyCoRLRm18jIfOVM4wbZad6QeWg3VGN0K/hPmgPdNlmtaedmTXmlIURVEURVEURVGUI5Oa9AW/E34tQf8+XdGkUTyydm6rlkEK9Lznr19pQjWA++u6OXC4d4jPPiIkSIy53AXGQTebhm/P1yqoMAlxqHhxxM0xCha7rCYrMx1tW6WgX+9u5t6bT5aEyhVvsgxFQyVFpHg2Cg+xoFKD/gNndxiliThWUvEjl5GSOJUpNh7mnYbXPKeYq5J8QYwdf41iRH6ZFg3vaYx7RQWCJULu21IhBWf5mN9gca3wlVdRFEVRFEVRFEVRjnTq3VIdQgVGfn6h2Ug1MoKLcvaELUKQmV3i7hHhaiLce69ao2ZYDQxnf43SQqIN9mg/KislrmCqFmCOIK4QExUZbpQMwVXTQ+hK489DTfhz7E/bYB2qEAufm401kGrh90Bgc9cUjuWhbV3jVBRFURRFURRFUZTDlXqpOKlOYPYOhDDvLudh3H5lQe3p7KrA8Nrw2uaZdl6f3rLUEL/X2VKDN0VRFEVRFEVRFEXZFzgxINiefCIE3it+6nGteBUQJFAJsTdU10xQZ0SFias04awN2u4+HTcM4zGLYXy/gf59cfhOw3FjJz77Xfz78DrvxtveYmvUa5g3f/4URVEURVEURVGUwx3KtFZJ8vzzz+PNN99UpcluqMc1cyA0CNXj4ewSKktcY9PZswrBXZbjNV5qs98zXoWGNfuLmuKmURRFURRFURRFUY4s7EqLIUOG4NFHH8WqVavMvVIzh8BSnQOHf+YIf6lEoR7ptyg83HD7A8ZUU0PsnQpGURRFURRFURRFUXaFcjAVJ2vXrkXHjh1x5plnYtSoUT5XpSYOs7k4Vv0QaGrGnXESXGV+u4pCVRqKoiiKoiiKoijKoYOdbfLKK68YJcqHH35o7pXaOYwUJzUpTGj2BDvN3ihN9i82F4FGVTOKoiiKoiiKoijK/iQrKws//fQT7rzzTrRq1cpnq9QGZfPDkLooTPaGAxWvoiiKoiiKoiiKohwcBg0ahA0bNuCee+4x90fwDh514jDb44RFsYZzNazZHwTGe2hgc10bh1ZpFEVRFEVRFEVRDk8omtMcjNNtzjrrLGRnZ2P27Nk+G2V3HGYzTqwawGsURVEURVEURVEUpf5QWclDSvzw3t2D88DLsDk5OZg0aRL69+/vs1H2xGG4VIcd7UDsDmIVMQe+I+9PbG3UZg6t0iiKoiiKoiiKohz62FklPAZ4y5YtVfcHQ3GSmZlpZrZ06tTJZ6PsCbd1lDqiagZFURRFURRFURRl/3Dvvffi1ltv9d0dHKg0GTBgAE499VSfjbInDrM9ThRFURRFURRF2R0c/u/NV20rNhyML+KKciTAZyo5ORlNmjTB4sWLjd3BWLJTUlKCtWvXomPHjggLCzN2XrWAPuO7ojNOFEVRFEVRFOUwhcJQeXl5tf0U9lYoOpDCHPPpFdwU5UigrKzM9Pv4+HifjbuE50AqLpheREQEunXrVqU0Ifb5PpBpH8qo4kRRFEVRFEVRDlMKCwvNMoBjjz0Wd911F7744gssX74cO3fu9PmoH6jAphyJVFRUmNkfDRo08NkcePQ52zt0qY6iKIqiKIqiHKZwpsntt9+O0aNHIzw8HHl5eUhPT0doaCh69Ohhpuq3bNkSjRs3RqNGjdC8eXM0bNjQfAGPi4szX6b5VTohIcEXo6Io+4vU1FR06dLFPKNvvvmmz3b/QDGfZn8cbcx4jnSFiypOFEVRFEVRFOUwxCvsFBcXY/v27eYEjwULFiAtLQ0bNmzA+vXrsWbNGmRnZxt/hAoTKk6io6MRGRlpFC5UoPA6JibGXNONX8mjoqKMPRUrVslCt9jYWGPPayppGAfd+Mt7XtOEhIRU5ZP3/KWyh/YU+OoqqvDLvTUMQ2OXJ/GLvlUGEW+9KMrvycKFC3HyySfjySefxIMPPuiz/e3U1Ke5RI/2fK72BcZBjvRnRhUniqIoiqIoinKEQuVCQUGBmYmSlZVllChUsPA6NzfXHFu6Y8cOs7SH9zRFRUXVBDWvgoN7NlBgo7KC7jS0YzpUatDNC92tEoUKFRra8bcuez0wXcZZWlpq0mTeaoJKnYcffhj//ve/98sXeEXZH3Am2Nlnn43XXnsN999/v8927+Bz/OGHH5pn5+KLLzYzyfhcUFmp7DuqOFEURVEURVEUpU5QdKAihHuncBYLBTPe85oKDBq6WXfrh0oN+uO1VaJQycFfe28VK0zDmj0pTgj9UBnCWSp2hgyvaShE8n7s2LH47rvv8I9//AMvvPCCL6RyoKhr2x3pcM+hG264wSg8/vznP/tsfztUeF5++eVVJ/O0bdvW9Hn+7i+O9DZVxYmiKIqiKIqiKIc9559/PpYsWYKVK1eaJUbK/oMiJY2dzUMlmZ09dCRiRew9lf+BBx7A66+/juHDh+PCCy/02f42cnJycOKJJ5q+PW/ePKSkpKBfv344/fTT8eWXX/p87Rnmubb8BravZXdhDjd0npqiKIqiKIqy37EDbZo9Yf3Uxa+i7C08WYhwfxdl/0LhmUL1559/jiFDhlTtV3O4UNd3k91Xh2W35a8tLGdfcaYIqW0fEobdU9qPPvqoUZpwhkmfPn3QrFkzozT58ccfzUyvmqgpXua3trRs+3LW1o033ohZs2ZV2e8JWyeHOqo4URRFURRFOYKpy6C2tsG0HXxb48UKDnUZWFs/dfGrKHsLN6zlni08VUg5MLzxxhsYOHCg7+7QpKb3Hd9NNdl74bvUzsgYP348Jk+ebK4Z1r5nve9KzhSZOXOmua4Nht3de3HZsmVmuQ+PGqeyxGI3Qq4NG29GRgauvvpqfPbZZ1X2u4ObPX///fc44YQTzObSpKZ68dqxTvZUd4cCqjhRFEVRFEU5Annqqafw0Ucf7TL1uiZqG0zbwbc1ilKf4V4ndqNa5cDBvWUOZfgu40arjz32mJnB8e6771bZ7w77Lr3pppuMEuOUU07BK6+8Yuy8ygMbz9y5c82my5xt4n0P019tioZAey7F4cbOXPLjhUoZzjapqa977XiSzzfffIMPPvhgl42ba+Lcc881s07o95dffjF2tjze9Gjnzeue6u5QQBUniqIoiqIoRyDvvPMOhg0bZq65gefatWvNUbU0nD6+evVq80Vx69at5pQVfpnkSSs0PLqWg3UKFzTc5JOG8XDwbDcAtQIArwON3Ui0NgFBUfY33NeExybrKSMHDrvZ76EM32VXXnkl/vOf/5h3H5US3377rc9199xzzz1m9sbbb7+NO+64A3/9618xadIkn2t1li9fbn554hP3gyH2nWkVDXwP86ji5557Dps2bdpFATF16lR06tTJGC9nnHEGbrnllqq9fFasWGGWUPF9bpU0L774opk90r59e3Ts2LEqD3uic+fOZtPZbdu2mXu+95999ln07t0bvXr1wquvvmrsDwdliRfdHFZRFEVRFOUIg8M/Dpa5gSC/qFIooCLEunFgbYUfezqJHWzboSMHxXZgbH+tG3/tNdndAJrxUpC1eyLYk1Bob9O1acXExBhBgNPFueyC17SLi4urMlY4TkxMNGGoyKFg0rBhQyQnJ/tSVY5E+FX98ccfx5gxY9CjRw+frbI/Oeqoo9CuXTv88MMPPpt9g++R3b0/9jf5+flG6cAlMCNGjDCzRq644gqzpwc3Xm3cuLHP564wTPfu3XHfffeZJUtUdLRq1crs+3L99dcbP97y/PGPf8TgwYPNe4kKjOOPP968d/nO47HfL730konHvpv79u2Lr7/+2ig5CI8J55IZzgKhP0tgnVERTj9Ujv/f//0fnn76aWPfokULtG7dGosWLcK9996L559/3tjvCSpfTjrpJFx33XX45z//iVNPPdXUz7XXXmtmdHEGC2ey2Nk2hwuqOFEURVEURTkC6dq1qxnUc1NBKiFuv/12o2Ag9qsxj5Sl4sF7jCxnidDde6xsTbNHahtiBg7qGQ8N46Gh0GDTIVZx4g1n3W1eeM988ssnvxZzHwveMwyVMCxn8+bNjcBBwe5Pf/pTlRIlMD/K4QsFuZdfftnsP9GlSxefrbI/4ayDNm3a7BfFiX2HHMzn8+677zZLc0aOHIlzzjnH2PEd2bNnzz0eGfzmm2/iX//6l5kFQsXcunXrjIL6rbfeMjNRCN9VfKdx5h6VMjw+mMoLzgbp37+/8UN4RDH3LqFyggoNLulhfqjs/vnnn42f1NRUo2yhooYbxBJbZ/xlOgxHxQZ/+X7nBslUbBAey02lDJ8JzkC89dZbjf2e4ObKZ511llGQsRwzZszA0KFDcdFFFxl3xvvII4+Y+qBC5nBBFSeKoiiKoihHIFQi8Ovo/fffjw4dOqBly5Y+l/oPhQ8qdajQoaKEv1wyRDv+cno9BQIqUmhHoYHKlGnTppmvwpxmzq+j/Ppa22kWyuEHv+D/97//xYQJE4ziUNn/UDFJxYldBrgnKIpaxQgVoXx+KfBz9tjBhgo17k3CPvLQQw/5bF24nwffmXxX1gZnf1AJPHv2bHPPGR48AptLfqhU4HuLsHxUNFBRwXcwZ6b89NNPVbOgqOB7+OGHzYavVGhYXnvtNTOTg+8w9t+NGzfi6KOPxm233YZnnnnG+LGiva1Tzp4ZN26cWRZERQzLwQ1p7XuPaVHZQzsqh7ztURtUnFx++eWYPn26KQtny1x88cU+VxfO0uEpP0uXLjVK68MB3eNEURRFURTlCIODYyobOHg+7bTTjNLEDuoPBThY5yyZpKQkc/Qmv9h269bNfLHlV9w//OEPZn0/vx5zjwF+AeUyjQULFpi1/pdddhn+/e9/m+nr3K/lcCWwTY/076Vc1kU4G0k5MLCP1fVd4hXSuS8Gn10uR+GsiksuuQS//vqrcSOBM9os+7NPcy+RlJQUM/uOsBy2LOedd55RmtSWHpW169evN+8hC99LfOdceOGFPhv33UU464MKBSpw2R/thrrcHyVQaWLzwPqJjIysOgqYS2347ubsGIu3Tqm8oNJk4sSJZoYV9ySx6VuGDx9uTuCxM7D2pDQhVEpzLyzCpUZepYmtH75zqUSi++GCKk4URVEURVGOQDjAtYNc/nLJy2/Fhq8P1CUvXLbDTRT5BZjCBoUKO4X+cIRCEvc34EwLCjt1EYoOZ5o0aWIUhtw/gtSn/nskwv5IhQiVB5zhwaUkV111lVlaQgUnFZsUwAmfXbZXYJvZPl3XtqxNmcj9S7iJ6z/+8Q+jlKU946bxplvbM8TnjHGfffbZPhs3z9xzyYaxv9wjhO+eCy64wChjqDjhvkxMg++jY445pprSxIajUoZKJbsMis83Z63wdB4qfawdZ4RQacLTb/ieO/nkk40b929p0KCByRfhzJXNmzebU4B+y8w75pV7nHCvFG6iSwLrh7NzqDDijBb7vBHrj3ivSeB9fUMVJ4qiKIqiKEcgXgGA1/aL52/BG8fvzW/NC/cL4N4DgwYNwuTJk322hx/c+JfCIDeAPNKhwGdPhVIODBR+A5UTtUGlCWd/8RmkoTKB+3lwdhiXlnDWBffKsPt3UClQ23Nemz2Xplx99dXmVBoqDhgH8xco6HMDV+57xGUvNbGn9wsVPexbu1vKY+PgkkEuH+RMDc7KoNIiPT3d3HPpIZfxEOaR+bVQscQ9UzgTh2Uhd955p1E68Xj5Sy+91GzWylNvqDQZPXp01T4thEtxTjzxxKp88NQ07sPC2TS/BbuZLZc6EubTWz+2bqnU2bJlC9577z1zT+jPW/dU6FI5RiWON476iCpOFEVRFEVRjkC8A3JS3wet+xM7cL/mmmvMaTtci3+4Yr8k84t4fcO2gxdrV5PbvsIlCcQKnUdSn6+PcLkcT6753//+ZzZs9sL3E2ddcKkdlSnz58/3ufjhHkbcgJR7bRD2GW+/ocKQe4xw75LHHnsM77//vrG3wrtV8FCJQeGeShM724TQnzV7wm5IzaU0e4IzRng6D5clcbkO06OCg3XBE5+4BKc2uLcIN2XliToW7snC44+5FIdKlRtvvNEoSs8880zjbsvJpYveGXZUTllljMVbf3XF1g/D0th7zr5hemwHznqzWHcqbeiHirHAv0f1EVWcKIqiKIqiKEcUduDOPVI4LZ7T9A9X7Ewir6BVX7DtwK/to0aNMl/babc3wltd4DIFkpGRYX6PdA5EPdc1Tp4I8/rrr5tTV+xpLt6wVtinUP3EE09UtV0gzz77rNns12L7FJUEFNYptHM/Ds7m4L4iVJrRj/VHODODSg+7zMYr/NcVuxSutnxauPktlSPcr4TLeHgKkT2GmLNjuOcU8ebBmxfu/UJ/3NuJ2DpjeTlzjifc8Ghizi4hjJfh6Y9KYhoLT//p1atXNUXNnspt0wv8JQwbGJ6z+jgThu3A348++ggDBw407cZyc18YludQOOVKFSeKoiiKoihHGHsaHB9JcNo5T6ewgtrhBgU6niLEPQ/qE1bgoiDLL+T84m5PMqlJANsfcKlOYmLiEa04oYKAe1rwFBdbx17hd1+pa7txlgmfub/85S8+G39Y5sfOQKByk/t3cKPVQKgU5AyPwP05WDYeKcxZLFwGwjYfMGCAmZGRk5Nj/DAtmwbzQkWD3Qtkb2Y/cMZJdHS02VyWeOvUe01lDpeKcQaM5dNPPzUzwrini2V39cgNYe3R8V5/4eHhVfYWloV+AuOjkpIKI3uST12x8dSUv8B+ZO+5DIt7nXA5DpVk119/vTnJh888ZxLZ5USB4esbqjhRFEVRFEU5Aqlp4HskQmGDX7+538LhCI9jpjBnv1DXBygg2f5HAYqbW/IkJC6X4PKBAwW/8FPoPJIVJxSs+ZWfJ8fwGFyyP98FdVE6cIbRJ598gj/+8Y9mE1HiFZptfrj8hMt0OEujJqh44abWNk2G4/KPP//5z2ZGB5fxWKhg4aaodmNUmx6VGNyHgzPPCJf/UKlAQ/slS5aYWRyM11KTgM8Tm5hP7nNCvHVqr+fMmWM2VGXdc6YJYRmoHKJy52DCcvLdZ8u9t9RUTgvvWVd87niMMme4cHYfT/nhc84ThLhhsyUwfH1DFSeKoiiKoihKvR+0HkgofNUkDB0OUJij4UyDvWF39WJn6QT62VNd2r7Gr/8ff/wxbrjhBrMRKLFC8oFoD85M4N4SR/rmsFwuwdOFuHxlf9dFXRQnbHOe/GKX6Fi8bc5ZIFzGQ8XazJkzfba7EripNffU4CwmCuXeZSl2GVjge45CPWdkcUPW7t27mxkYRx11lNl/hHuEsI64FxJPkeEGs4RxBPZPHoXO/kXlSE3QnqfMMF4uPbL8Xu9dpst8cHPeA4m3rvjs9enTx8zs8S7NORDP+oFAFSeKoiiKoihHKLUNWA+Vgez+4HAvK0/toIJjb8rJMFawW716tfn6f8stt2DWrFnGjkIy47Z+uGcFZ4/UVRjkXg8Mz+NoKciT2pQx+wPOtuDsG5bFpnckws1Ahw8fjgULFpjNV/cntv12Bzcw5UwTO9uBbe3tazwRhkqTs846yyga7Ga+gVih3Ib78MMPTdycaRK4tIdH4nr9Et6zT3BzVSo12DdYN5wNwtNnuNSH9cPlPjx55plnnsGwYcNM2MA+zv07eIwylSwvvvii2bOHe3d88cUXpn8fffTRZmYJlxFRoWOfycB4Dhac4XLfffehUaNGJh8Hkj2V8feqg99KkFTUIfXXgi9/mt+zoynKwUD7t3Ikwnc7p9HaXem9f6L0mVCU/QefLQoIFEqGDBnis/Vjn70j4bnjSRpffvml2aOgLidiHGrwqz6XGvAIVHuqzO7gkiW7nIFQaKXAyKUVXPbDKf4Utnh8KwVbC7/YX3nllXj55ZfrLIzfcccd5rhk1j1PFLnooovM7AIK1BQs6zJ74bfC41qp4OFRrNzfZn9zKD073GeDsyi414U9gWVf4V4hXA71888/+2yqwxNsqJjgaTc8dph425rLOKiA4KwEKum4JwiX7NAuEM6i4nuMG41S8cF+yeN5ub9JIGz3r776yij9qCBhO9GwnerSVlyCw1kl9mhjLzYeLn3ifj08xYdL42jHZUmc1cL3DPPAWRfeMEcSh3qZ673ixFYwHyhqCvli5VQr5cjAtv+h/qAR5p9KP8KpfHt69A718tZ3tH7rJ5wuz68/nTp18tkoinIg4LiqQ4cO6NevnxFcA7F/o46EdyWFMyqPeOpM4MaKhwMU5LivAI9s5VGrdYF7O3DDXC6b4d4E3OeBe5A88sgjaNq0qdlok8KndwkFlzRwFgOFWJ6EUhd4ign3l+EMEH6Jp/DLzSIpWB+osR/3UeGyIO6xcCBO8jjUnp1u3boZBQTrvS6KtT1BxQmVBpz5URMrVqxA165dzewQ7kXiZevWrUaZy3EyZy5xRtLDDz9sZm7YPUG8cFzNtG6++WbTj3isOPci4cyOwP7DeKg8ZFycbeEdh9fUVoHtyBNrOOuEy5yoPCHeOHhtlT/sW3yfcEzDvU9Yx/bZO1AKQeXAc8jMOKEGjxpvdr7AtWzK4Ulg1wx8qR0iXbcq3+y31Fbzj0FcXNwe888Xq6IcafC54JdOfgWsaSCjKMq+wWeMzxZnFVBxwunjNSlOjiQ4jZ5fuSl48W/04QbLt2jRIsyePbvOM2qscoFQYOVShtNPP93cEypHuPeE3ZiTwiQ3vOTYhenxuFFi+1tN0I2KOwrrPE6WYySeusH9JThO2t8Cps0LlWT/3959wOlT1Pm+7/8953Xunj27azoGBEFURFBQzAhiQJFgBgwoCuYsJlx1FQSMoIgBFVEUV1ERMaAgmAERMJFBDIAirmlX13D2nntf5/KunR827fPMzDN55v/9vF410091d3V1dXWFb/2qmgh0xhlndNttt93U3vUXYoJ0OPzww7v99tuv+U333GaCcGKaDAuiUXzmM59p01pOOeWU676mAmuesHrx6WAin7zBQuPII49swhqhbhSm9LBIIZaceOKJbVFY8S/qPoh+hD3vek2VmSR/EXl8HUaflGVLUdeaTXrNJ13D8rMq5C6fUaN4K1g1qPsvQ1idKDSmczMdM9P+leTGocCezkFej4tb3xyme3dCCHOj3i/o8HJrUSiYiX46QHnjM7X96SlrCc8Zk5Sr9dWZJz7xiU1w6YsmIHRYSFPH06diTQeqT8PO1mqnFq0l4EHHVwfetIbF/MKRTrN4sm5YDIb5a6Vzn/vcp32qmjhWVv3zqYN9qaUW+EXV7ZUuNQDuuIIwwvrIf2uIEE1ApJDHvJ/jOPjgg9s0GFPHiCYQ/3KFNVUuuuii7mc/+1n7PZNo0n+OBBcL2rJa6YsmGF4nrF1WhXBieo6XS6aUiZM5Vz/9QnSUG5JnHsLaYNT73WdcGRBCWBiqPmWab3HMv/3bv22/12dYNC+kZcNKQ5k6qbU24UJesZZDCUolZpx33nltCoUvcnz6059uHdfb3OY2LQ2t32B7NviiDoGmPkcqnjVwJL6L1fbbcMMN22CsqUiLgXgTIUzPYH3Bgqac35wvt/QdEarcKD9hTeoqrIoDyyFfNCFK+M/flCtfNPrhD3/YBAVWOPPFmiO+1GPgu+g/S9cFSxd84hOfaNNziSS+PLPzzjs3f7BC2XbbbcdaSskzBDdrpZjmVX596rf7l5fl39lQcWb9QkCUVtaEGTK83nQsVp4OS8OqqCUU1DLlWq7UQpgrCuG4uNXgKr9ORx07SUMkhDA7+u+fRT5hrv/6Rj8ddNR18nSmZyqfVisWc9WGnkQ8kS4GLfsdVp1OFgnWTNH5tjCshTst2ulTrQQQYkj/E7DTIV7yYS2WCc9gsdr79XxNBWUVM9sO9FxwDU6+GuU22mijv/pdbuhn22Krk7oKxxogno2pLuLk+Uhza9VsvPHG7bmaMuOTvL4AM18IZ9YekRf6VPqbxuOTwaZ6WWzV1C6LAhNNSlQpPCfCyThGvbPj3mNrp1ijxAKxs21jsDSxSK30NPWINdSQcdcLa49VscaJRX5EU2ZNY3r9RKG02p+9Bkt/jZMynZ2Ome45hXVYTczmHXaMdyRrnISwuDCJ1+G1PoC5/+sbyhplDMsb5v13vvOdu/e+971Te9cWOqVG8y0QO1tRgjWJT7Bqg+t8F/KMNSJYAnBG8V/+8pe3dStYLrzvfe9r+cnUCVQ6j8LinzrOrFYe8YhHNLGm2nt1zmLVA5tvvnkTFHz9ZKGZ7p5XMtaose6IfDKcjjIJ8oA88ZWvfOWvFnSttGFpZB0c/4lwhJtRViUWZCX4LNSC8cQy7nGPe9z1pgqNwjtw0EEHNasdU9NmKwiGtcuaMeHwEo5yfUb5Ff1zxh2D/r7h9qhzR/mFyVHQrnY0CNzHWriXEBaLlJchLA1G+0HIX58xXcnXL5j7r2W0QbhJKSsV1gPWwdCR1eHVma11UIgrjjOVwRSYa665pvnPRE39GQ4kLUU9IN+bohX+guk88oj3YT4QpQwU+tzzkHq2RIgXvvCFTZggmhFNRrWPWTItlGgifAIpgY9o0r9ef5sgaPFgcSMmfeMb34hoEhprfu6LF9TL4L9CfZyZYr0wMynx416y/vZMiEvfhTAJ8lo5+XWxTFrnSj9+o9yQSeM/KsxJXFgdpGwMYXExRUInabpFF9cyVcZYxJI1xkJ8hnUlM9e2Qq1vsv/++3ef+tSnuo997GNtagVK+PBVJp1lljvWzPFVlNlQC5HWAsVLWe5bA8Q78Pvf/37KZ+FYLfXXsE1kOgpB1dS1+WAqDmHKZ4cnYbHTbRi+35UGtc/aPaygfHnpHe94R7NCI6rW4GdYv1kzwkllZv9VDqb1+HQxV6oi9VMhOerFrPPMpZupcnGMClZjoxTSvuszfNEWu1AIK49hnujTzzd9R+CTh/2v3/Jy/+sHtWq9PCXPGrFxbJ/6XeFWfhw6/t4TYdbv/r5yEC8Nqf4x3Fxwnnuq8JaCfpzjltaFEFYOFt7X8TUtbjYM3+G18k7rOLM6KYFgLaJ9UG2FSdEB9plmXzqx/oWR+EK7pHj4wx/ewv+7v/u77pJLLpnynb7dW1836bdnlgrCiWe/WF/WWQ0M07vehfk+B+upENJYbqx0qhzTRzR9x7o9FtW98MIL2zRGOEbeXsr8GVYma0Y4gZe9Opfmr5mD+b3vfa81DH796193r371q9uc3n5BXxBA/uVf/qU79NBDWwUxapX5slj50Y9+1My2zjzzzFbgup4XalQjQmdUWPbpGPZ/lxvFuJczL+3qY9wzG/fsofK64oorWt7VmDMi9v3vf7/lY/lII0iFxAzSSuI+FdjPV336gkQ1aoTZP44/01qjbpBPvSfiXvF3jut6P8Rvvo1M53t3KPq+i18NJ3HB8F5G3VsIIYTJqHJUmW/E35SL2aAuUJ9oRzlvrbRHarrJrW996/Z/rVLPb7bU8/VlFB1hI/G+LIKyNLE2xfOf//y2FoQFY2Gx4YsvvnhW0z0sIIrlyEu+/CIf//KXv5zyWT/p54mPfOQjrf23xRZbTPnMDe25ffbZp1merHS0OS+44IJmLSWP6wd+8YtfbOIJlJdrpawL82fNCCcytY4eUcMK3z6fduSRR3avec1rmojixfDtbuZn1WErqxEvOEHE/E1Ci4KUWRaVvUb84RyVxVvf+ta2GrSK4tnPfnYTUewjvqgwbnzjG7dr6MgK8/zzz2/X4e/TZ0zXXFvY4q0j67cwXEs4/DhhuK/yc5yw04Fc3Uz3/DxneeZ1r3tda3jIhxqqFlojBsoTeNvb3tZ961vfavscZ/6lfFv5SP6SX/ipCOUb/6nohA/h1nGuaZVxYfI379e74N0QnvfHcRbRO/zww9t1xUP+rXvx27WEVf6uKTzvhTwM/uLBT972fhKJ7C/BRhiuV9Q1QgghLAw6jDpNyu3ZYi2L7bffvg0crRUIJ9KgvwDqWqRfX08CcYOlab8jqX72X7399re/vTvwwAPbPlhI0zkGI2dC/hOvah8sJTr12kIsr9YXRj3/GqzStiMYPPOZz5zX9L26hn7SC17wgra9Uqm46tuZfqZ9/NKXvrT52cfJnyEUa0Y4UagbPSFqKAR84kph7rdFhYghOnb2OZZ1ihWSFRIKziq0dfJgn45jiSz1cqkMTPd5znOe05RZq4vXqtDC+fKXv9waFK7DSuBDH/pQ6wATV4zmW2naKuW+Xe4YHcVzzjmn7Rcn8WC66HvqVrWm2hvhF74OpmOd149TWFtoSFjp3TMninjWGqusPajihAV5QqOEKWRN6ZHvfMrNtnPkU/n45JNPbiNG8pbv85un/MEPfrC7+uqr23GFd8TcVs4cZt+q/8IXvtB+l1gnbn7XCIX/9hE6iJZMeeVZgou8TWjx/fvPf/7zLb7i4D3kLy5ERIKiODvHPYqzc7wPffEkhBDC/KmOAAtG5TeBfbbUCD2z9qWg6ppiMdo9Okusbmp0fC22rbQb50rVw/Us5J8SYfppVdt77bVXm/6l3p8J+cnXW6q9IIyl6qh65jrMZWm7PjAqbbXZfNHmxS9+cfesZz2rO+SQQ6b2zI3KG5hPvlsKxFW+3mabbdpXgFhWFfYtVV4Mq4c1IZxUxj7xxBObWGFOmm91U0xvd7vbtW+U65R5kXUyjc4TVUwReM973tOsU3REdUh11hT29Sktn0hToNfLr5D1mxjDzO+BD3xgC5docuqpp7YFshQ6xxxzTOvs6gCydNGhJJYQY3R+fQtc48P3wS1AZKqCKRcqjhNOOKHdg1F90zHe9a53dQcffHBblOuVr3xlu08V2UovkMLckAc222yzlm/lHaIH0cG351mCaOwS0IgrRnbkWavcy7NMZuVfgoi89vGPf7x797vf3UYSCHL8NBIJfD4DKC/LSyoO+ck7QICUbx0nL7qe8Cq/+S/Pcxrd3q3TTjutWWC9//3vb1ZexBoWJaYXERgJNW94wxtafiY+EhP9Nl+aSEh4qbxOiBRv74/wi1RiIYSwcBA/dBRqwGg29OuBpcB1jj322FY3YDHqABaPOtHz+fzqSqfq7blQ50n7fhjC7D+P2tbufu1rX3tdJ3TUdbVp1PPa2No4T33qU1u7YCnreIM2+gk1VWt9QntLm/GRj3xksyTS5jv66KNbexFzzSvFUj7H+bJUZVlYG6yJ3KLjp/OoIPaZKR1KHUTKKYsQo+fQSdQBZGVibQidSaslG5knXugA2jbP1ZQFn6vibx0T+7xcnFFxhYzOog6hVaiNVPhklukUlEvfs1dpPP7xj2/z5ph+WVDLt/R32223NpqvU8kixRxR8SaMmKKhs2hkft999+0e9KAHNUWe4POiF72oFXKmVDAtFKewepht4UzEkE99Fu6yyy5rVhny3B577NEqI/mG6GZhM/nCdDANPpZM8pf8bkqaPMVU0rxjU3xYo/isGrGPKeauu+7a8ruwIWyiDUFQHvVZQYKd/C0PFv0KVT61er7K9lGPelTLm49+9KNbBUz0IWASAc11NarBKsa7oaEqvuLjuvKydVoILs5nMUY4Ep/hiGMIIYT5UVMpdJr6AvVMVHm8lNaABpXe9KY3Tf0azXw6eqx7Wd2oz7CaOn2zxT1Jo+nubVwaVttlknRRt2t7DOtvYsl2223X2hXEEpanBk5Mk9B+Xkq0OzbYYIP2HsxXKFhJuJe6H30jFvLbbrttG3yz8KmBOeIWfwNXRC6L+XoemCmfhLA+syaEEy+5gp1gYgRFZ2vTTTdthbZRBCKHwrtG1nXgdCp1PB3ns1NG46ne1Od73OMe3U1vetNWuBuBp0ZXxSEMjtUHSxXHP+1pT2sWJ5Rz0wxsK5Bdy7QEgo35syplnVT7KN0KKmG7NiFEp5d5I6uW29zmNq1TLCydaPElzijs+IlrWF0MGxCjqMYNd4c73KHlNdYkhBCVHTGOAMIKRIO3prOwPLFP/pDnWF6xWPrKV77SRAr5zrHyYplma6xU5dpHHOQ9brimjnuwLRz7vR/MXF1TpWydFRU0Mcd6PuJAYBQH15ffiYMEFc77J87ys/Osys9ChaBY91ZUuoQQQpgfylfiiTJ7EpajQ6Uect0S8MfVW30c4/5mwtRTVsj+rw/MdgBnFLOtf/vHuZ5nY4Blhx12aF8tkd467RbiNJBjkMhaGMSUpazjtWO0nVjwrqUv6/TfBf0HbS39DgNl3iHpbH1GbTOWxayEa30f6b8c73gIq4VVJZyMK/B15hSABBAj2kcccUTzu+c979lGrRUEztVQUCDo8JmmoDNHtDCCryDhr2PHWkXHVKfT1Ij+IklEDRixZ21iRJ6FiY6eSmDvvfduHcAqeFxTp1dFQdBxvm1CiQYLgYbAwwJFxcHaxTUd5x7E23Y1cjhhuyf7U8CtfCZtCBALPHPrlxAtCHIEBqIascT0LsKEvFIWGdVAlE9UkCxVWE05/qEPfWhreDqGgKLiNB2HyCh8eUg+q3BsO867UFNrCvvcD6HDOyIM8SLUGDny31o84qYhSlwkArIiIRa6lt/iZ6SD6GIdl7oGKyuLzH31q19t66sQZJLHQwhhYdHO0FmcZGHY+VBtp2KSelHHz/nqo3GoQ7TZ1DmsF9SfZW08HepC9RlLy2LSOns1oO5GWZiOYlRdKy0mqYMr7fppaCo8SweDQKyHDF5qn7BqtdaaQRztlmIp018fgcWRdvlaov/MDO4a1OU8A++JL8ewyB+S9lYI07OqhJPp0Bm8y13u0qbnsAYxNYCyXSp3VRo6lipVligve9nL2vQXo+am0SgwdCSN6FtLxLokFrmixJbowenYEjyM8PNX6arYWZSYh2uKQlUC1kJRIDOFU0ETVXQaVe7ixbplv/32a+qvdU50Xms9Ca7iRBiyzdkWh7qnFHQrm3o+9bxmQh7QuJHHNtpoo9awIIAIR2NQfuPPxNRx/QaHYzQ6WF/Jk/KU9XRcWz4iArJcsQaPaTF1rnPkO3nYO8LaxfEWVzatzXGOqeN9sco0NO+Z94f494EPfKCZehrF8N6wktpqq63adDdWJBrq4meKkHs0jcgaKvw0nAgp1j2xTgpxxSiUxvBSNqJCCGF9gOWqMlc9sxgI38CSsl+7SX3CAtEUTe2yqhdng7pEvUTMR53rGtaVe+xjH9vd5z73aVaP6g/14ite8YpW18yEMNWP6qpikritFqruHgpY01FtkUnqYGlXDgZ5HvOYx7RndfbZZ7fpOPJCYdqxNm359c9dCmodFnFY6yx12oawFll3bYG44nslFokUTR20cWq5/QoEI9Q6mhoEKlqjCDqZhBWqt0JSp5IVCIFExUC8sJBlWYK4jmkG/JzvGJWNa9gmgDimTFz511oPRuFZBxBUrDvB39dGhGtqkPMIJ+LDUkUcWbgwoXMtli+sYHQYKeEwKiQc4ankhUXM6XeYw8qH2OXZESgsxjcqL1elJk85ztxbTt4gLsgX8iYBRP6Tf+q3BrD8UdYo8pSFiAkscL48JO/Lg/zlN/HQcPTe2Ja3XNt7JzxCC4uVii8hxbHiorFhjR5xMYrEgsS1nWO/872LteiefEwU8Q5VHDSYXJ9FindIGhGKiJveo6rspUm952FtI094/iGExUH7R9uHtaz13GYL034iA9GbJeE41AWsflmJGNCyDhYR5YlPfGIb7a5Pfs4Gg1jW0DLtWpjibhFx1pfaS9p91uUinDzgAQ9obbxipjrDfRhMc42ddtppynftYTDD9HAfKWBdPRtM4fCxA+KGOlxaYpI62DSYe93rXm0QxkAlSkgDiyeWpvb3qWthMet8awsaZPVxh1EWGCGE0GfNCCfVsfKfoKBQ1lHVsdSBhM4qwUKHTsdQR1J4OmeOc375Od9v0w2G19T5I6AIyzUcp+OnMvK/rFJc13Guy5/i7thawd4IfI3yQzw4v+t8+C3MirdGAquDuq7/YeUjbxItPMPZCCfyjmfPyQvyqHzpt3NrTnb/t21ihikz3gO/5Sn5Sf7p56F6FyoPOdZ2WYZ43yof9kepvCPCgPMcD9fjL57CFifvBH/XEXfh2FfxLH/niCeBRhzci32Vt+v9DusH8kSEkxAWDuVn1S/w9TXTOC2WP4lgQIxnDThKOOmX18U73/nO9rU3VoreadaJLCGt6TZbCCbbb7996/y7BitGg2Alluy4446zFgOGVPxM2WbZslaRdkQvQob6dzb46sonPvGJlkfUy3PBdF7Pm/BF0OrnEYM8BmpYf/sqXzEqHy0WphbL/6zCCXwhhDAda2aqTh+dLp05hXKJDyA26LipAHTqCBmO00FTQHM6nDpw/Ev40Hms/Zz9Onr8wc9xznWe69tW+FdH1LXqWL85CEc8xa06sc53bcdCR9NxdR3n1j7XqEomrEzq+Uza8HC8PCFvyK+euf+evzxYeUB+kT8c73/leefKd46VH+X9Ei/kucpTFT/hyGvC4W+73hm/ywmLvzA4cZIvHU8Y6ud954pDvRP+C0OcKs+Lr/+ocB3Xz9fJ4yGEsHCwZFRG99dwm4QSz8HqwwBX1RH98tpX1XTaWZmYimkxzklEE+jwiysrE19f89la0z5MjSb+9EUTdZHrz7bOqLZZWWaGv9S3LDE+/OEPt7SfC8JhUWJabokmlUfAmlo7ejnTnqWugay1tsZJCGFxWBXCSRWy01EF/WwrS9Sxw4oe/WuqiIf0r8f1K4Nx1DHDY6tSKr/+PgzPcT1xGnd8WHr6z6D/XLh+o2Muz2qYxzi/Ub9rG7Wvzuv/LgGmjp2OOm/UscKp/ajrVNj1m3BS2/0Gbd+v/PvXmU38QgghzI4qlwvTL1htTCqcKK/VaUT4+oSphfn7Uy2q/K7rmaJD3DDF5hGPeETz68dlOkzx8WlbdYn/1kl5yUtecp1FWoXjPydurj9dHdK/to8EsH5cqkVylwvpMVsBpPIKQUEe8cznwjAfDJ+JgRZ+/Tw47tjFwjRjFjiXXnrplE8IIYxnVQgn/UpuKXC9mVwdV/S3+wyP6bshQ/9xx0ClUm54Xlh6Rj23ei6j9s1E//mWK4a/i1F+oxgVHw1H03fK9UcT50LFcRJX54UQQlhcWJwo602PnBQiCDFk55137k455ZS2SDgxY0i/PLfGGyyQD+L7TJgm4muDj3zkI9vi/8SNocDhGuqxSeqO/rHiIW6EoLVM3fMkbZBitoLLpPiinutYF3C5MJXYO2AtthBCmIlVIZxUhTZX1XsmVCSzrUxmS4W5UGGPCs//fpoMj1nJbiUxU7z6++fjyrKCW2iqkeP/dK6P39YxsWaJbQu2WpzVf0KKUZjFiOt0LPX1wspkmFdDCPOn/14RTiyAP6mlhfqCJQLhxML7Opy+lKPzOyy/67cpmNaSgM/rQ7vO/nKjOProo5uIf+yxx7aFzC3wP4q6r0nLDVNGLd7v64cpcxaPcWnLysNUXfmwz1I/C/lYHq1pWyGEMI5VIZxQuxdTIJgrCvd+Ab8QYc6WpbzWWqee47jKur9/3DEz4Tz5uFyF0w+3/Pr0z5mvK+o6Rg01FMxhNk+cubXRQF+0ec5zntPWUtGgGQqX08V3pdCPY9zKc+a1j3L2jSrXUtaFsLAQyNUBs10otCCqqxMIJb6SUuuLeD+9v33q9+GHH97qGotvvu1tb+uOOOKI5j88fohpPfvvv38Tari61kLhS3Bf//rX15uvqUjvmdJ8KfGFPiz3QuAsjohovgIYQgjTsSq+qmMBKaMjTOqmM+9cjgphkuSbKX6jGh7jWAWPLQzwbGtRV3m5/wz5LeUz1fglhLzlLW9po3pMVX3GsT5X7IsJPjNsn0XdNChGxW+U32zz8Cj65w7DnjR9ljpNw+zwXEY9m/KTL+VPo8AhhMXB1BfC+Ne+9rUpn9nhKym+cENssDjrOLzL3mlrqWy88cZtLRTH77333m2R1y9/+cvdAx/4wKmj/5o6v3j2s5/dPmFvapDyYSFQz+2xxx7dN77xje6+973vlO/axGegCUXWiFmo9Jsvnr/2xne/+90pn+WBRdPLX/7y9qWpu971rlO+IYTw16wa4YS5phEHC4QNqQq2X8kuNguRbNVRKIbxH7dvFTyyMMDzY/VRX7wZToNZqrxb12Ru/b3vfa81Fs0h95UCVDxOO+20Nn+dBYpPEZrS0/+a02yYTT5dzPvuW9n00TEft2+t4949l6XKb+OY7tmIH+sT4uL6+pxCWGxue9vbdptvvnn3hS98Ycpn9vh8q0Es4sco+mWMz7weeOCBbUqPa/qC2g477NC+ivPxj3+8e8xjHtOOm4mnPe1p7ZPBZ5xxxsRWMuMg5Bx88MFNOFnrQq0vG1nMV7qXcLKcdYH8sMUWW3QHHXRQ94pXvGLK9y8sZdxMHyPofeYzn5lWzAshhBUtnFTB+Ytf/KKtur3QZpohLDUsTnQG57v46kJACPnKV77S3e9+9xv5ZQX7HvzgB3ePfvSju+OPP37KN4QQwmrGANTtbne77h73uMecyvYnPOEJzVKFBQgRvqg2W/3XOd5ss826F7/4xc26sfDpV9NDzznnnO6kk07qdtttt6k943n605/ePkvsukTVhcBnka+++uomJqyEOnkx2XXXXdsgJPHpqquuauKJL+YsFv2uxSgB5LGPfWz31a9+tQ3gbLjhhlO+sxdMZnvcOPrns8IxSGQx4j333LP5hRDCKFbFcJ6OppFI1P+1gsK7X8GEv6TJbNxqoeJqtK2spmp0vfbVdt9vMdHgffjDHz72c5RGXj7wgQ90n/zkJ9vif8VSxK3PMF3KzZdRYcYtjRvHcP9Mx4cQ5oZ21Vwtum52s5t1f/zjH7vf/e53Uz7/+a5WR7T+11omRA9UnXfTm960O/3005sAY5R/NgjTuQvZBjz//PPbVNS1LppAutWCvIccckj7CpIFUQ2gmC7jWVqHphw/++aK51VuiEVhWQ4ddthh1xNNMOr4UQyPm7Se6J/Pmh3aZyGEMB2rYqqOObKmCnDrIx5RFfL97bB6qOemQaKRxnpKQ4Zf/9n2WajnrDHA9cOra/Gz3c9XpkloUNeo3pOe9KS2gKx5yNtss03z6x8/H2YTTsV1yCTXX6j4hhDCaqXKQeK9qSksTkyXmRRf1Hne857XRA/TbqqMFnZdw1oot771rZtliePRFz1KtOmfOx2sVs4888w2jXQun1AewurFOi+msBx66KFTvmuXXXbZpd3zt7/97e6lL31pswBiDcTyZFg/1jPRBvDJapToRTTTFneeNXKIYCxXbnKTmzTnNyHCPueOe66skVg9zZeKa90DV34z5anCGoruwSLG++2335RvCCH8NatCOFHYK4DXV+EkrB2M6hj1kZdLOEG/si9mW+n3qcYDNDLNH/7mN7/Z/Prh1Xb971+bn9EmDeN3vOMdbWTRl3b+9//+3008qRXw+9eaLwtx7yGEEMZTZTZLAutLWBDcwpiTYsqH9VF0wA844IDmN6wP3v/+97d1SU4++eS2XhaqnK//ffGkzh9X9v/jP/5jm9rxpS996ToLgflgyo/1VVhVEnfWOoQT095NjZHWFkI99dRTr0v3wm9tE+La//pf/6u1BQy81Od6+dnHT5vAQJDznQf77fPbABGhhYDGstU2ZxF6n5fWDtK2J4SVs37NuDywWLg317bW2ytf+cop3xBC+GtWjXCiAO7PpV0JVNLNtpCfJKmrIlrqCmSlMF1ajUuT4TkrMe00Qggn8nIJJ6Oe83zjrkH0kIc8pDVSrFFSQk1dq8LvX6effhoSpvFUg5Jg4is7Pg9pIbUQQgirE22qrbfeupXx733ve6d8J+NRj3pUE+UtEHunO92p+aljSgx53/ve1/30pz9t4j1G1enDem5UXVj46onpPdbeWojFYU0jYl1g3RQDA2sd4hXLCnX5uDSehD//+c+tnWBgpab2+P/rX/+65a/f/va3103/cYz9rs8Syf+CCEa08J+IwpLFNvFlk002aQKf/doytS4L4eVGN7pRywfaU/OdanXRRRe1PPzWt761fWI7hBDGsSqEEwWxAnIm4YT6baV3t6QCL6eS4KpS7v8fMvSv35wGQX9fbTsGfvfDR/8ccRniuL5//3q2y0EajDKrDKuHUcLJKMp/Ls/ae3DPe96zNVyYNc/1awEWsuVqatGb3vSmNur3hje8of1fCwzTdzbp7ZhxOLf2958h5hPupMx0rRDC+ouOawnh031SeDp8ocU6WOqZN7/5ze3rbP1yR6dZB7jaQPMtk6yHYZrJcccd18Kab5jPfe5z2xTUa665Zr2wZi7h5Dvf+c514tZSoi0hT5T1CuFF2rOCIaYQW3784x93P/vZz9p+XyAsixbtelYuBBgWLfy1haFdfOMb37gJKdpVnPvjTDXi+GnD2C7RzZRk+Yeww+rIMayZbnOb2yxIfg0hrE3WjMWJglRBq3CtW6r/VclyfSGjCsb6jVGFpQK2BBlUGP3z4Lfzh2FUB3l4nfKr88ofw2PFQYdbOvgfVid94aT/jIcM80bli3FUWI4zemItEkKH0RONFe9H7S/654xCvq28ayTI8VbCZ3rt840+Zdyn4rpYzCX8mc4Z7h93/NC/0oUbdU7fzzZXjdVRx4P/KOoas2VU2CGEUFg3jrjuqzKmY84VX6Sx0KhOsQXEWQVUWbXQ5ZBOv042S5lh2OPK1OnYY489Wtvy61//+pTP2oYFquc+yuJkLum32BBYtJdq8VoCHaHlN7/5Tdu2n+hhPxGmBnpsa69XGDXlqC/G2F91OGsWIiLxj9XJYuXfEMLaYM0IJ6Viux0ig8Ky5l4qIHVa+p3HfkVhW2fW/ypwUccIR3hV0DrWb1QYqP2F84u+YNOHX12HKxUcznFf9dtx0qAU87D6mK1wUgzzk9/yuQaDRqrGpIqfK5hPm7tuJHHvvfduDYv+tfr5qU+FPwr+4vyTn/yku9e97tXd4Q53aCbTRuoqnDrXp/34Mae1Hgp/9231/BIPZotGkMb5RhttNOd8r6ElPhark14WsBPWMN6YLg00voS1wQYbjLwP6c7/3ve+95TPf1JhujZqjZhCmO7TPYYQwmJTwompOm9/+9unfFcWVT7DdpW5o8rtSTGY4DP86kkLgq4P+Myu+tsnoIf113T13kKw2OH3qbY5tMlBUOFfliuwLU6sULQJiqWMawhh9TFZL2aJqQpytigknVNCCdFBZ0enhINCso6peZE6sqVC26/QdEwVoMwDax8/Ba+OF5GjwqoC2n/+9otDFcCOG2IfQahMDiHOVHH/K86o80eFE1YX9Qxnqpztl9d8DviCCy647vgf/ehH7UsI8snxxx/fnX322c2/IJTIP/J35cs+5SfvCXMYn34e824QR+RlYs1tb3vb7qijjmqNL3PO4bw6lyUKQcWIkHgTBS677LIW374oOVs0cN2j6XpzhZBjITz3zcxb47GoeBfD332uuuqqZtot3YecddZZ3ac+9anrvbNFhfmFL3xh5Ojmueeem3VjQghLhjJpVN2wkuiXxcPt/u9JqLpNvXTeeeetV2K19qu26XI893pelf7D/wuJNks5bWvOlDFTeQzmWCOFM5DjU9R90QRzzVshhPWDFSucTFqgOp6rQk9nz6rpxxxzTHfSSSe1jomC1Kivz6CZR8nss0aRL7/88u6KK65opoA6qzqKOpY6qaecckoTVszDNAezjutbgxQKaR1E+11PZQWdWJUWqlCHkR/XMxXC9ZhR6pyJ84c+9KF2To0OLEYlE5af6Z4ra4tPfOITrdNdyDvyXeX3ykuFjrh8yCpkVENJXpSnLNxnf1ly2RaWfY6xbaTmyiuvvO59IBr4EoGvKbBoOeGEE9q5hWlCO+20U7fbbru1z0duuumm7RzxHL4r0yHO7k9DxwJ+GjrTMV1j0D7xd33v5iQCTj9c5wljlOWLRriFdI1gjqPiMES6c3Ml5UIIYS6s9LJj0npjJiosdR/WJ+GknrV6aLmo9B/+X0xSP4YQFpIVK5woUCctVPvH6+TojPDTCdSR9N8Itu/Xv+pVr2odQub7hJP3vOc93T/90z81QaWsRYRhxXUdS9+mN93ASvE+V3bppZe2MKszVE4h/f3vf78tNqUzVSPlzhUeIYVYw4rFJ/2OPfbYNlqtg1wCiniKg//C1Hmr8KfrIIbViec6pPx8OtBcbEIgEQ+Vr7mhaCJvvfvd726fHrzzne/chI86Fv7Ls6xAfE2ByFIiYAkc8l0twvaDH/ygWZiwaiE0yqNERO/PXe961+7xj3/8dY1QcWV1Ya6wPO09KMsX70XF1TtkYb7Pfe5z7bdj+D3jGc9oViqo90o8xNF9WIyQiMm0mpjiekXdn3f69a9/fVu8tvbXPUMchg0p7yYR6PnPf35bHI6YKf5wnmsqF7zvsODuS17ykva5TSgjiFvvfOc7m3AEv5/+9Kd3Bx98cJtqCM+t4iHNlCO+OMGaSBpB+ST9nvOc57TPSUNZw1LG4ojCJ/y6Posf++reQwhhNpSF7XwE26Vkocs49Z86tb4GtD6g7ubWtzZk6scQwkKyYoWTPgq+SQo/HSOdvu2226517CyKZQS8OkzEDE5nRcOB03kyxYGfzo1OEjHDcXe5y13aufx1bHSs6tx+JUQUEY5ReNMVjjzyyNaB0jE0VcG2juBHP/rR7vzzz29WBBbaZFlCQNGB1dl93OMe11ar33333VsDx7Ur7tXxCmufsnCSdy1eVh3pyg/eiX7egOkk8q11SIhvBAfUMfIogeRjH/tYy38+v0cAMMWEaGGdDgKJBQN9wcAUl69+9avd2972tpZvHefdIiaaQuMd8H5BI1Q8H/SgBzUz2C9+8YvtXSmBESy/rM3CYsW9eU8+8pGPtDhZyNb6H/3pLqYHOUaDj0BB4LjPfe7T3m3xlUao8sG75ksR5u67FtGibyHiuIpLnUMYYsr77Gc/uwmkLMqIGYU0Mb1HQ9sXCbzjT33qU9u9icNmm23W1jW5//3v375g5F0Whq9M+O39ti6NdJD+yhnlwQ477NA+6ckKTjniXf/MZz7T/IlIhCUiiWtLI4vxsvrxHNzjIx/5yFaerEYq3/bzbghhaTCVkohsqsL6iPJdHXWrW91qymftU9bPw8GWEEIIs2fN9sJ1RHQyfKZMh8e2TqTOywEHHNBGb81t1IDQsdJp85lVHSgdN+ezGNGR8y15HR+dxOc973ltMTULROpAavjrgNnnt07h0572tCZ+CO+Zz3xmu6ZOjjBYtghP52+vvfZqx+2///5thFx8dBDFV5xYw4T1F427Wj1ePjONy3YJEdXx74t3rKdQJsj29fOod0Aef+ELX9hExUMPPbTlPVNhiBHWLmEtRQgw5cZXdB796Ec3gcU74j0SB3nZZ/tOPPHEZgmy4447NjGR8Hf3u9+923jjjVtDTVxd138iBjHgs5/9bPfBD36wrYfCksM7w6LFe0cckPfrnpzrmu5B+EQkohArHAviEnlQx3u3CB3C9/4SOcrKRBr4P8T7Zh0U/8Xf8a5ZuI5rE3e23HLL9i77T7xgVeM+iUbu23GEjic84QndVltt1dJPWCxMKh7ipVzyeUjHsA4SBusRYhZRi7DieRPBWKFJX/ctzZUzrHBY/BCqCC7FqPtbaQzjuBriHMJaQr3C4oRQvD6iblUn9RdVX+uot0yzDSGEMHfWrHCik6RDoaNY/3Wubn3rW7fO0R3veMe2/oKOjs6Wjo81ISx+yU+Hy+g9f2KGxr3RCR0mx+nAuIYwdWw4VEeVZYrO0Q1ucIN2nJFn56iwXaP8HavyrnUmIL6cDqpj+lQHMaxORnUSh371zHWOdaZ9VpjFgQ63/FN5xXmOJS4UBBAddKKEfKtx6Dj/5dGytqhrWkOEYCBfeS+Id6w8iCiO5YSngd2/rrzJGkvn35QfViGmnmiMV96tY4Utnjr8BBWiwpOe9KQ2JYVwSAwxZYUlhWl0zql4ouIqHhZ4A+FGmJVWjhcfYqV33FQX/6VBHScc8SvqXTJtaJ999mnXNiWGiFHxRwmp0tm7WiN3juHnGkQVcWJNIi5VHsBv5/svHs6TFoXzlTn8lRGm+DzlKU9p6UpIEqZnVJiKpMyyhgwrHOcWlR6rDc+m70IIiwcRWzlfa0etT+8cKz2CtE/2r08YAHnZy1429ev6rNZ6I4QQlpo1K5wU1SCozpMOS3WCqvPiv06ZDgiBxYi3DglxQ+dLR1Cnxz7/OZ0p4RlN1uF74xvfeN21ShQ544wz2lxaWDuBEKOTaAqBkXadIWGbgmEtCZ26qsBcYxzpWKxu6hkXw9+Q/0zdslCwvGWKCssQ1hQEAnlRPtAI7HfyiYHW/zj66KPb2hyslnTiTz311CYq6NzL6zr/8t7pp5/e8iJYO8iDGpSm77DmKCFFXjZtpN4fVBxMK2E1JXzXLNGAmCA88SMkEmnEx2gnKxNxcIy8z5+g4lrur3A+4cB1XE/8C/59IaTOvd3tbtfeQVNp7HdeTWnx3tb74x0TN2kt3Uw5co54sRohUrGmMVVJOPxZxniXNbxNY2LxIT3dn7QhLhFXjj322DYFx3oo7pHQYVqRY4io0pKViw6M6XrO50/gch8s2kqU4dwrxE+8HvzgBzeLE+GsNkbl9xDC0qF81JEefjoda719oV5T9ym71zema1eGEEKYmf9y4LVMba9YdDyIGjoV46gO0qhOad+vtvudJ/gtDNfQudHh2WKLLVpnxm84V6fRsc7zW6dJx8sUCab09ukg6mRZ04S/TplFHVmvMMu3toPjdPDclw6Yjpj9hBjXGBVn/4XbH2EOqwt5WYfYc+zTf8YgZshHLJwKc7IJAzrmHAsoedZ/FiGVb3SorbOjQ85ywdQcU0p00k0NcW3ihsazdUiIBEYeiXymkGlMm7JDBGBd5Xhrb1jM2PnOdV3X4rwz97vf/ZqASKBgvUEIsS3vEyLds/dJnndN74b4bLvtts2SxoK2flvzwzmF90Qj1zoiECcihHslevAvSxjWKN5L01yID/Z5f8WXSGk+v3OEL00gXqxzCFTuyfQlVjHKElNm3J975tyTY4gXphtJmzJ99lylIcFD+runSo+99967PUuijTCkg2dmv7LA8azQOGFYO4bQ5T1XJkgD9+a+hcEqhsjleBY/05WLK5XKO+NcCGHxMCD0sIc97DpLNu+c6YSE2KVe90T5VizFu2+wwNpUFgPv1zUhhBDCTKy7ttJa8cMLOosEhb65+hAdHZ2lfsVrW0eqRsjhdvmPq6wJIUa1dSZ1vHS6+iP6qPP9F7ZOmOsYNS5/nUUjxdDZ8ts+266hM2g/f79dQ1gV1378UOHqJDonrE4IaTq61XGv51x5Cv6XoDdb5BthVP7ydRuLC7NSMQ2Gn/3epQqfICIuOuPeH+9Y5c0SEOVRIo38yV/4qGt5J+u99JUXggLRYCE685Xnh7jXfvrUOzNTmvXDq7QeFf44htctpJF0gzQaJWz2rz3umFH+/fP61+kzLl4rlbqn/r2FEJaGUe+demn77bdvC3tbGHwS5vMeO7f+VxiLXSZYr8vi5CyA16fPEYcQQpg/q6e1PQM6HDoP1YHkVMY6d/7z71fS4Mc5pv7rLBImfElHx1KH0vHlMPxvVHm45oJwKkzH+S0sTgcJ5W+0Wqeo4sHZ538dB2Gvpg5SmIzKP1w989lS58kf8tKuu+7aGoem+VjDgxAoTOII5C15T9617TziiN/yaE1pEaa8LW/WNWCfc/odeZYarFbq88SzofL4KOpa6KdH/x2oOHK2ZxtenTOkrtO/Xm2Pe/dYrRTKof65Rf9ao8QPCGd4bv+8/nX6jIvXSqXuaVT6hxAWF++dcqbKGlMIWa5pl+y7777Nz/Z0ZWmfSd7jum7/+sP/fYZ+/d+jjh9H/1j1nnJ6aPUZQgghzMSasTiBzh4RYtKORDUQ+h2Xalz0WciGvrBd1/8KV7wrLv17sJ8TPxYwq62jFP7C0OJkMbEuhgVYDz744DYFxJd0TN0xrYdAUozK88O83v8t/1We7Dc+fY3HF6UuvvjiNo1lPvTfCwzjVb/RP24+VJhEonHixkLSv4chC3VPIYQwiipjv/71r7dPqZtGaCqleoOwoAzkp67ydUELV5uKaJqPNaq0x0whLMvF1YJFtdVVpoYOLfxCCCGE6VhTwgncTjUIirrFYWeE/3KKEESSskgZxrcfr9p2zPAewupiKYWTwoKmhxxySFu41ddy3vnOd7a1MxYan9o2b5zViUb2QlLv8CgW+p0Yvo+LxVLeUwghjIJF4WGHHdbWVdImIZgQFAxE+bogC0Sw1ODn+Cq7THVhaei/NbhKUDGl2Bpa6rq+ODGqXBv6+S18rixDbBs4Era2IKtgdajfk4rcFk/3uXqLlGcQKoQQwiSsGuGkKszFwBQFlipXXnllG4XQONBI0IhQWVfFXtYgkmxUA6D8/SeIlGl9JbF9TGC33nrr1nEdFUZY2/SFEybSz3ve87of/ehH11lxQH4ZZXk0HZXHisqH8qBRQaOIFkEVrsao+ewWLHWMhnB/SloxKn9WuMIp0Y/z9Z0rrrjiukWQ/e5TxxfOqfDt825bS8WI5z3vec/2ZZ9aEHa56MdxJiY5tnDOOCYNq0+FO58wQghrn9mUW9osLBS1jdRZvhbmi2/aZdaVs+1LNRa4t6i5Y00XRYkwVZ/B9UaVUbaH9aD6y/nVnrLtPzFF2FwJKeo59drmm2/eFs5mHaOOG2LdL4MHRKFJRZcQQgjrN2tKOJlNI0Cl76sYvmZhBXmfG7WY5aWXXto6lyrgqpyhwla5qtD7lX8f+ziNBY0M26bUEF+cr0OoY6pBIY6cjqF5xb7WoYNoxEZnOhX52oZwIn/JExqhT3/605vYIL/0G5SVT2p7HDPld8iPrukd0uA00sYKpTBS6Os9FjeWf7mZrlnXlV+F6R31PmmwaqzK696hYfzEBcKvfbZd0/tFtNQQ9+4QUJiQ3/GOd2xfP9AwJjrV+2Xb+1pf1QkhhLD0KL8NQCm3lf0EFb+J8vZpBynvh3Vav57pt6/Kn59tTvvMABcBRx2hvjLY5XpVb7mm/a5rSqovHZpadOc737nVGdpohx56aGt/HXvsse0ada1hXRVCCCEMWTXCyWy/JqPS5FTUKlWCyPnnn986dTqtBAxOmCpRazHooBnpNr3AXF6dzBrRcKwOoM5adfpQFXrhGH4qb8cRecpqRYVdjQr3YDV3n0z1BRL7jc5Xp1bl7V5rbjF/61OI4wMf+MDWeQyrl+WYqjMKYs3b3/727qijjmr5701velP7bK64zYW99tqrfSLXpx6JgXPBu6MxfPnllzdh5zvf+U77VLeRQWbf3h3x8474793x3nlH/PbOo95T+9F/bwthVEO53mPv/t3udrfWyNbYJmYKt/+e1zn9c7lR/rPB8XV+nVP/K6zpqHP7161tabNYVnpLhWfYv78QQhiF+oPli4XQTRfV/lMfsehkDVP4HPxJJ5204NNJQwghrH1WjXCiU6QToCE97CyA6GBEQgfutNNOa34ECM55RBCdr0022aS7+93v3gQTo9jMOQkbSz1qTZBRsavgv/vd7zbLF18j4V+WKoWOIysZAsoznvGMbp999mmd3Xp0q+ARXg/xrWc4KaOe/Uqm/2zEmUWFjjxxzj75khvVye/f40Ldr2u6RgkkX/rSl7ojjjii+9rXvtamkO2xxx7d/e53v+vmp3svhvEgOLJa4YwsfvrTn+6OPPLIJr7sv//+7V7kX9fq339RfvW/wvdfulQaGDXU4PVeeFd+/vOft2s7huWYfd4VljveffSvV+H2r9O/ViG+HMFV49u2axMplR2uKQzniFvFbxh+PUOMikdhX3//OJw3vMYo6piKF8GkBNxXvOIVrcxQrlQYs7n2clH34v9QXBTv2h9CCDOh7FZHqENYq6jXCOKzGYQLIYQQhqy7trH/V63oldY4JZzAyLKKEMPGv86CfUQTlaQOjykI5WrEeohzdCqgs4G6/9l0MPpp1e+YlL//futI6iDXMYQerjo7hf3ioYPtP0sVnUICy1ve8pa2Av7RRx/dPfnJT77OomVI//qLQd3DfJhrHBf73haDSi/x9sw8874lQz8969765/QZ/u6fOx2j0sw7Ih7y0AknnNDmfZ9xxhlNLPG+CLtEAzjOtnwMDVHCibD/6Z/+qXvVq17Vjqk8OYyb3+XnnP5v9OPofXYd/6XX8D0p6nri6R0bd1xR16j/dX3/PRsCjfKGVZjpfCV01TH9/7BPHPtxL+oei/72OJzDuZcST/vXHYZR163z7K//yg4i2E477dSOkU51/EpHPAkndT/Faol/CGFlM64sDSGEEMYxUjjBSqpEdGQ0ogkN6zOEFB07K9dbIyOsPkzVkY+X2sJpNshfzJqJJ/6bO86yQwOzxBCdefnPujwESRZct7/97UcuwhfCfJDvqnMzrj5KZyeEMGScKBKxJIQQwnwYK5wUK6Fi0dlkbTLOvFJlOIznsIIs1kpFOeqew8pnkjVOKg+vlucsvrPJl0t9P3lXVif951bvwpA81xBCCCGEsBRMb9d+LeMarEvJTHEY1XjmN8qtROaSxiv1XsL0mNpRz9v/SdxSMJvr9OPEEqWc38P3bZRbauZzzdmkx0piGN/VFv8h4r/a7yGEEEIIIax+ZhROwuKzHJ3J5aY6RKPcWqeedwkJM7k6dinSqX+dUQz9+/Hk1hoLeU/zfW6zPbeO838tPpMQQgghhBCWmggnc6Q/yt53YWaSTpNRnd8SJ8qNYqHSdlz45T9TPNYqc03f/nlzDWPStF7fnk0IIYQQQgiLxYxrnGC5G+A+E1qfFZ4OX9aor2vonBA2hnH3m6v91Ymp4+r/dJ2bOr9/bvkVFQ7K39oW+QzefzJT+q5VfDrX12IWY42TOl7+t9Dr8Ksss6V/fH9qUSE+9dWb+qxyn/k8P2Gt5ecfZs9s8m3ySgghhBBCWApWxeKwsxVOfE7U50Nn6jD272munb46rn/+0K9+E2igk0k4IaDM9NnUtc50naKZnsF05y4Hs80zmEQ4GeK+x12r8pj9PkNbn7PGdOdNh3P651WeLX/34QtB9TleoiU/jLpmPbfyHx5T+8F/uN89zva9GXX9lcRKj99KoJ8fRpH0CyGEEEIIS8W0vZDV1DDVqdJh1HkTb04nS0eOUEG0KP9ycEzfDY8Z54RbHUb/6xoov36YjteR9YnX6uSur8zUIbJ/Ore+UPdKEJS35SNcddVV3ac//enu3/7t39r/iy666Lp85pz+e8C/8ukkTn71v/KwvF2iCGFS+PKzvFzPxX5xev3rX98deeSR3bvf/e7uxz/+cffDH/6w+9SnPnVdnDB8jsPfcOwvfvGL7j3veU8TT8VjtjhXOh1//PFNRBrHqOv26e+f7thx+8b5i5/PrIuf5/utb32r+8AHPjBxONPFaRIWKpyFpPLKKKbbF0IIIYQQwkLzVz0RDdJyqwmdMh068a7OXnX+4L/OgU4k/3KTUh0M4duuMHUg//SnP7U4CLc6q/3OXsVhfaf/TNaCWyyErdP/spe9rHWwi9///vdNLMGll17aXXPNNW27kAflM+f38ylmE1/HyL+Qr+Vn/8XFe1bh1TXK4dRTT+0233zzbq+99upufvObN8sa8bvwwguvd+06p4SX+s1VXMGi5X/+z//Z/EfRPxb/8i//0hxY91x88cUtjKLiXVS45TduP/rb0x2HH/3oR91//Md/NH/3iOE5ED/peoMb3KC72c1u9lfhFLPxlxc8H9Q1+wyvXQyPHXfccjDqvselRQghhBBCCIvFfznggAMOtKExulIbpASJvkXHKHQYjNyWYOH4s846q43kXn311W3UW4dOp/O3v/1tC7M6DNVJHMK/xJXqTAiX1YgOog7djW9849YxfOtb39p98Ytf7O54xzu2Dsw3vvGN7qc//Wkbcd9iiy2u69CIm+k6fUFlfWR97fzIo559vzM/Hd/97ne7H/zgB8264+53v3vLjywV5K273OUuTUDZcMMNu1vf+tYtTeVTHfYSJMrZx1W+s135u87zX94Ux1NOOaXl7Zve9KbdlVde2b3lLW/pvvzlL7fr3OIWt2jijGu4j3pHfvOb33Sf+MQn2rS6bbbZpttyyy2bcMLyg8XI9ttv347zflxwwQVt3//4H/+jXdcx3/ve99o7TkQoxOd2t7tdm6bHkkacL7/88u6KK65oQkP/3SVAvOMd7+jOPPPM7h73uEf77d0k4BAyhF3X8z5+//vfb3EuwaKfTtLQvp/97GftPYc4/+QnP2m/heU4uB/xkVbio7w55JBD2rWlgTSSVhX2z3/+81YeucYvf/nL7ra3vW230UYbtf/SVRn17//+7+18z8CzP/vss1u5U3GBfEB48UyVaW9605vauZtssklLL/u/853vtPPEzbXFQzqKL3/PwPn2ee7ipIxbSVQ6F8PfIYQQQgghLDb/5cBrWekN0dkIJzocOgRwPzpcOhs6OjpEv/vd77r73Oc+rcPyrne9q/vqV7/abbDBBq3DorNTHZvqaOpguZ4Olv/8dBAdq0NCkNEp0ik7/fTTu5vc5Cbdk570pPbbvssuu6wdrwOkE6fzUx2zv/3bv03jfz1lUuHEVJxdd9215U/TYAgX8h8hgDih8y/P8Yc85hqOL/p5jb9jSrzzu94redu26xAd73CHO7SwTzvttPZ/n332aaJIH+9lCSdEwhNOOKFZfBBXiC8bb7xxe38JI9tuu2075gtf+EITYy655JImitgmuJx//vntPXVfhbA++MEPdltvvXUTIw877LAmNnjniAV3vvOdp478z2NdX9rc5ja36f7hH/6hO+mkk9r1zzjjjCZCeRfF2dSh8847r1nCEBvEo0QlggIhVNoqV254wxu245UZRKxzzjmnCaTS8P3vf3/z974TVoikxJYTTzyxWbzc9a53bWUDPAdCh2lMwnFt4szDHvaw7txzz23hEk/e+c53NkHMtat8sY+wJK0JI8Sh4447rqWZNFVOfeUrX2lpQ1DzHI855phWBhJP+EsraXjQQQc1gYQAJd6Vho4ncBGYVxrSrlwIIYQQQghLzZoxe6hGtY6gDofOD6HksY99bLfzzjt3D33oQ9s+I6w6UDoeBAwCieN1JqHz6LcO1/ve977uRS96UfeZz3ymdcJ0ct7+9rd3Bx98cBs11vnVqTn55JNbZ/MPf/hDO/cBD3hA97SnPa175CMf2Tq9OrKoRr9jQhhH5UUdYnlG5/ue97xn68ij8npRx4+if5xtHW/59utf/3p37LHHdq95zWu6j33sYy3fs3BgKbH//vu3zj8/74lOOxFAp9q5ZaGAfl6+053u1D384Q/vnv/853f3ve992/G1NotziIj//M//3N4lYoH1Sz772c+2uDjm0EMP7Z7ylKdc977AO+s8/71fBAHx0/knkHAFEXT33XfvnvzkJ3f3v//92zlEEe8/awyWLMQN90RM2GmnnZqQQvxgNVIoO8SN0LPHHnu0+EmPN7zhDW39FnH42te+1sQN/gcccED3ute9rgkkBBbPSzzEc7PNNmvPx/2Jyyc/+clWJjh+t912a4Kqfaxj3LdtZcsOO+zQPeIRj2iWbYTYRz3qUS19iSXEF/4PfOAD23294AUvaNs77rhjuybBi/hCdDn88MOb2CSdxNc15IHnPOc5TQhjHeMe+BOt+kJUCCGEEEII4T9Zk/NFdFR06HRkTGNgpq4ToeOlc/DKV76yCSA6gTplOojQ0dLB44zMQgfD6LARYaPWRnOf+cxndptuumnrbDjOyLmOlA6WjiLT/1vd6lbNzN51xaU6tzWqHcI4dJ5BlPv85z/fLBRYXegs6+QS/Co/Vd5F+dX59b/85W9+Ov6EA3n/CU94QhM4/P7Sl77UOueED1YNxBIiA0sH+ZxFA0sF549C+N4Jx3gPvF/i5/q2CQesQlh6EFD4syR59KMf3cQKAoTOPUuOPq4nbNNPCCLQ+Wf9UlZmheuXn3NYT3j/IQ78iCKETtYp3mmCg/e1cAwLFO94/SZ2FMQUcSFwECwqvsQt91FxkL6o5+DZEW8IunDvruFYaVHHsdYpKxViLcuaj370o03Auve9792sRzwf286TPu7NdVnXQLrXdaSVuAnLcTVlyj3c8pa3bAId4UhcWAiFEEIIIYQQrs+a7sUzXzei679OhQ4QU3odB50WnSqdFfuNgjPJh44Mk/kHP/jBbV0JHTSj8TowOivM9B2vc/KsZz2rdap0Np/61Kc2P6PHNYI87NiJQwjjKJGD+GCaGasllhTbbbdd6/yaeiHPlhjnf9/qozrfqLD4ycM67fx0ruVfnf573eteTVzUqZdXH/SgB7WpMgQ+1ljPfe5zmxWJ/5xpLuJWDIVAcfE+uaZrVRz4ERh0zF/60pc2y403vvGN3d3udrcmNLJ0cY9HH310EzX61P0Jy/sMfmWt0cd16preNe9jHeM35768w4QalivEUXHrnyf8enel27e//e22DUJSvf+EnoI1C1FLWAQU+/sQUa1NY30RmCZjm4Dh2iWgEEIqXcVVGXTggQc2ixzTAW9/+9s3AYoAVPfmmRCGHQ/PSTwLVnYVZ+VSIf1NozLlZ6uttpryDSGEEEIIIfRZL8wfdEZ0MDgdK6OupuxUJ0xHxWi+KTc6FjotOk86IjpDOiVEFccbZean0+NYYeuIEkqqg1fXqk5Nn4pLCKOovKGjrgP+jGc8owkXppXsu+++bcqFzrfOtmPl0+lwjPzOosR6KfJ/WYywANHxF54pJfK3NYBMCeInf8vT8jfnWt6Nfv7tb9svf/vPv3+scFhgESzf9ra3tSkk1tQgklhU+c1vfnMTb2oB3D7CKSf+cB2/h1jbhLjE+qve5Xov636IBe7RdBnTdMq6rOIqbGlR5xF0pAehx7QXa5OwzDHNh4BhupNPLxNBTEECscsUKOuIFKxI3D/RyL2bAsiqRxwrvVzTtriCWMZCxzQh666wAGIlQgAWd1NyPvShD7U0IwbZZj1imo91asRXWlvPibUc0cT9Fc5hwSLt5YEQQgghhBDCX9MWh53aXrHotM20OKwOVY3YojpBfcpPx646nDocwrV2gA7HQx7ykNaBsYCl9Qh0JHfZZZe25oBOkg6P+Bg519EQL9YoNSWoOlt1rf7/iptO76hOX1j76Lh69sSM6ZCvdM5ZCRS2WRuwXGAd4DfLKAvDCk/+ks8IfSVecPaZZmOtDFNdiIamqFiQ1fQPU1Wsy0PEsEYHSwZ5W6caRAP53XXLyoNzDWF7h1zbfZnywom/6SS+AuO9EJ7r+i8cx4q3KTH1XpuiQiCqqUgVd1NIatobYYRVBWGBeOCcvtBCVHB95wlbGtbxwhEnIihLG1YhrkUAEbfCdd0vJ56OYWnmeAIEKyAiCGGLv3feNQhcNS1IPKWBeHN1P4QT5xJaPQtrl7hG3Zt94i0cx7PGsWYKxKPyhHVMHCM/2S8dLFYtXGkgXziGlZyy6XGPe1wLw33y7y/yKw9Im3reIYQQQgghhOuz7toG/V+GH1co1mDQ0Cc4jKNM1WvElpsEycDpAAnHaK6OiI6RawuXMGOdAB0XHRAj9jpoOmU6lDpABT8dy37y2naM86cTgcLahYWBjn6/oz5EvunnpcpDM+XpEgOt41P5sc713/nynbxp3RSdZetgEBJq+o087bwb3ehGLc/X1BfnCZ+rePB3HwSE8p/0vRtFxXW21PHTnTdTmNOFIQ364gwch1HH2td/v/lJ8+muP6SeZT8fFDPdyzj659U2C6bjjz++LRRLfJlr2CGEEEIIIaxl1ozZg45KdW40/Cdt/BNAOJ1FnYcacTcCrFPJCZ8JvpFfx1WHEa7vPA78a7uPMHSiQhjHsLM82/xcx8mLfdGEn7zKQUdevmaVwDqB1QKhxfEEE2t6yO+OEw7LCfm58nrl8/4757iKYx03V2Zzr33q+OnOG7evn0b9/31Gva+O4yotCukwFFn8Hnf9cQiHQ6X9MK6T0I8jhEE0MY2H9Q3RpPxDCCGEEEII12fNWJzoWJhCozOno+e2Ju0E1PE6jKgOk7CEX7/7I8jDfdUJrY4o6jjHEF0INGH9ZDYWJ/NFXjOtRB6UV+W/yoO1n/BXebjo59eif55teR3uwftYYsxqxb3177dwnyVcjDtmFP1jJzlvPvSfD/rX7d/HEGWVfEJECyGEEEIIIYxnzQgnMEpuFNVaJDoL/U7LpLdJgCkBBTqZZY0yqoNiv/hZX6HWOegjbo4Vbj9eYf1iqYQTebW2+53n2q78XUJIn8qf/td+5whrmOfHdcrRP3alslBxXA332mcY374YHEIIIawkVlsdG0JYm6wp4QR77rlnW9R1ubDwoik+1o8goHD3u9/92sKPC8Gox5XKZPUwV+Gknvtq7pyHlUueVQghhJVM6qkQwnLSyqBr/6wp4cQnOH0C1BSCKmD7he1sC91+stR2/1x+nBF5/3WGfdHi3HPP7U455ZTuyiuvbAvNsjSxLoqvlfgSynxxrVQcq5ehcFJ5axL6+XoUyR9hJirvJK+EEEJYamZqy2r3W7ice/GLX3zdp/5DCGG5aOXWtX8m77ktMQpQX6JZTWuD/PKXv+x+/etfd1dddVX7DKzPjt75znee2hvWV3zy1zQZoloIIYQQwlplJoGkj8HHs88+u/vsZz/bffrTn25rcGn777LLLt3b3/72qaNCCGF5WFXCiYJXZ7PWbhjSL5zN1UdZg9Qt9o+ZDbM5to6psFkT1NoPru83WJ74nPF0YQ739eOO2cRnEsaF17/mYjHdvczm+s4ftb5Gf80O+/vXGRVu//jFvm/54t///d9bnFhPLXc6z8R8zl2rTPfMFjq9hteq8KdbV2Y29PP8kCq3FpNx8Xfdur53xSLf873XEEJYDYwr7xcS11juel0b6Le//W37mIOBxUsuuaS74oor2v+rr766xc8U9913371ZmRx88MFtIfMPf/jDUyGEEMLy0MrQa/8sfu9tnhBOfCHEF2nGLWg5G5a60qhr+d9P5vIvv/6+pY7jamc1pZW4Es8847Ke6j97rLT7SV5cPoZ5o1iuZ1KixmKXUcJnleWz2CGEEMaz2OXxbCBs/OEPf2iiyO9+97vmWNcSSHyswT7b/v/sZz9rfv4TTzbZZJPuf/7P/9n+77jjjt2DHvSg660J+JSnPKX7yU9+0qbA54uUIYTlpJW31/4Z3TpfQZj2wlxvtX/6NASNCVZI1uwJIUzPSugUhBDCWsDAYznW2wYkyxrab9umy/hvmkwJIvyIIfwIIKah/+Y3v2mDmtZtU06zEiynzPa/wr/RjW7Ubbjhht3GG2/c2vI+omDtv1vc4hZNNLHm2zhRZJ999mkWKaeeemqzQgwhhOVi1Qgn1GkdzVHrQoyL/kpubFec0yFY/9DIMJoe4STMxKiyLWVGCCGsLaqs959VX1/M4Ee4KLGDlYbfHGHDcf5rW/z+979v/sQOokW5OoYTHr86lxgiTH7+++0Y04k5A5ZEDe1vVt9EDttEjLIMvNnNbtbd+MY37m54wxu239zNb37zJpLUsbPFtbmaprn33nu3KT2Ek1ichBCWE2XTqhFOFMD9r+q0yK/STkQleTpB6x8aNxoRo0TAsHpZqvJoNZd7IYSlZa2UF8t1H65LxCjRgqhBXKjfJV7UcWWl4T9XggXLDMc4z28WG4QK5xA5WHPYFnZdh3DgnLLg8Lv+V9z6uJbzHMNpM7PmKCFDm8OATYkf/muL+M/VgE5Zf3D2c6xka5v/Qj0L91Bh1f0Mw37605/eff3rX2+LxrJcCSGE5aKVWdf+WdHCieiZqqPg70/VaZFfoMJ7LVKPNWm0slhrwslafw+rATsb5psW9c4i720IYSFYH9tKyu0SNVhhlCjRn3bivzU51MlEDtvEB/5EDSKHc0o4sa8sNIRbx9rWNpXGJUwQF+rrebWt3ve/BAlxrKkt/PpOGMQKzrZwHNcPk7/r8vPfwCJ///ktN/36DHPNg69//eu7I488svvud7/bLFtCCGG5aPXptX9WnXDSj3K/MOZfpo38dXhUTotxi3XdCnvS3+A3SdwqjOmo0YnhdcPKYKmEE/l+urw/Sb4r5KXKTzX6JZy+/1pC+ilPUPeK/rtlu58GnPNqf//3cKSwziu8u8Nj+vtDCMKATx4AAGpsSURBVGEcBIK3vvWt3Qc/+MFWfpx44ondXe9616m9kzMsnxYbQoTylmhR1hw1fYQ4QdgoSw73qi4lgNhv235riLHm8F+5qzwlJtiu36h7cx2iCH9iAysNU0581aXanDe5yU3a/xIn1N0lTpQFBsGEH+e4uo79nN/98n0lU3UPxj3/2eSNCme+eeiwww7r3vzmN3ff//73u1ve8pbNb6nzZgghoJU91/75Sym5AhG9WhxWpeR3RVnB2S88VbRGB+xXSc731pw/rnDuxwHjfoPf8PcoxvkX/TD6231U5tJJZR5WHvMRToZ5bBxlAuz/bN6Dfv4c5lXU9fj395dJb//cPuP8h8zmuNkew823cSoMjXiNdY3qunZRcZC2trlx8au4OHZI/5xqWFdDvPznQj++cw0jhLB6+PGPf9w9+tGP7s4777xu22237bbYYovuVa961fW+TrLYECvUO6w6tNm0x9R3xAkLilpM1D7/7VMmOl5Za5sY4ljlrv9+O54gwk+5qHwkaGjj3Pa2t71OvCgrDP+Vo4QMx5Xr7+dKCLHNckMYa41xdVJh/2qoH971rnd1z3ve87rLLrusu/3tb9/8VkvcQwhri1b2XPvn+r2CFYbojRNO+h0kfkYfdBYVqOOO61P7qwD2e1gY1zGofcPwUX7D7WJUuPzKvx/eMKyizin6x6G2NQJ0aufbgQwLz2ILJ44xIqdBOqqzPqTCGobtt+3KY/W78JsZsUapfDY8fxT9MIbhrSSkXY121n3Vu1Tx7osmff8+o/zq3D7SkZ/jbSvrKuwKw3+dCfv7DK9R8Sr62+MYFc+h36hjQgjLD3Fh++23b4LEhz/84e7+97//1J654V0ndFiXQznIgsNvAgdxhJj8wx/+sLXLHOtTsY5R9mijKaOUl/4rMwgZ6iTbxAsWIupAlhzaKWXlQexwDEFDvWJ9DvWk/SV4CKumwjinrrWSmansnE3ZuhBhTMJChzcfPvShD7Uv61x88cVNEMRKil8IYf2hlT3X/vnPnsEKRgWtgu0LJwrNfsGps2OEgl8dowJW0arooSNU8DdaYSRjGF5VxNWZKfrXq2uUn/BU4uLBvyr0fuerjq3z+uFheNwo6hjhu2Z1lDRq7ONcVyPE/aOuF5YfjVuNPs9nrlgs+YILLmgji9/85jeb+eod73jHtk9+KAFxXINSXqi8Mi5f1DGVv/rH1T6U1Ukh/x9++OFtRX0N27vf/e7t+B/84AfdDjvsMGMjdxgn7/4pp5zSRlPHpVnFBcNzjVJJJ+/LOIbn1xx2/hWfClc4/Prp2z9utlS6ekcrPP+90+5T2eSYusb73ve+dj+veMUrRqZhdWTufe97Xy++P//5z9so9IMe9KAWZtGPb/86sA/9+xnlF0JYfq6++ur2adfdd9+9++QnPznl+9cQPWqKizrkF7/4RXfNNde09pGvlig/iLPaEoQQ4TpWmV4QO5RPrucLKsov5f8GG2zQnM/LEjgIH9prjid8wO/aXmtUeVrlZJ9R/tOVo+pvVjbSt+q8/vnzLYMrrgsJ8Y645v+3v/3t7tJLL23Pe7fddpvXdDEcd9xx3V577dXWONlmm22uS4uFvocQQpiJVn5e++evS/oVhgq9RhzGFfp94QQ6ICp/jQIdA+fpzNmv86NxoKBXOfU7FNWJ4DQKdKB0apw3rqB2LeFpaFi8SphGVaDysE+4EIa4VEeFf4Vb/0c9kv61XU8HnJMmwteQEaZzhVnpFVYWkwon8u8b3vCG1vjQCIFGyUknndQ99alP7Y466qi278EPfnDb53iNl8qz4xi+R5Xn+HF+9/3qf/nBNuGkBDp85jOfae/rfe973+5rX/ta9/CHP7z72c9+1n3lK1/p/vEf/3HqqL9Q+b+uMcQ7LEz3rlE+RBz653pfpYH0JRh89rOfbWJDCSfDeyrqvUeZhtfvPp6fsKzuX++0MCtcacHfO1npJZy6z8I2fx0Y5Q+RSbz5i3uF4xgimfsgftzznvf8q7jjS1/6UpsD/tKXvnTK5z/50Y9+1P3zP/9z859NedBPhxDC6uCVr3xl95a3vKW1N5TJhIuNN964WY0QSJRDBGH1gvKGxYd6SFlS5ZgyRxlBALnVrW7VpsJY60N7hvVHLVLKlRVIv+20UBCBdZL91456yEMe0uqTPuPK8ZVA1QfiNl38pLt7vPLKK7vzzz+/O+OMM9p/7VLnq2ee9KQntSlXBKgKs+j/Hu4rxvkPERftBuIasUy+8V/6a1urEwk6lYccR3RTzxHnHW9bnhBv+YIfIU5bZaeddpq60uSo+x7xiEd0Z555Znef+9yn+c32vkIIYSFpZc+1f/7SE1qhKICr0h5VYPJTaCvcYb/jKdUqJpWOAvyZz3xmO1alr7A//vjju4c+9KFNUFHQEyTKpFRl4dytt9667SPK6FBolPhf66k41n6dFqaECngVjpFu6Ozd7na3aw0VcUA/yTVeOBVXxb/m3Oq8uYb9ztWxquN1RD//+c93d77znVvnaL/99msNH8c4tho4GJVmYXmYVDjR+SeOaLgeeOCB7Tlefvnl3Re/+MVu77337o455pg2CnO/+92vHa9RU42bmZ65eGhoyy/yN+Rv74EGkzwpb8uDtuVJyGOQr6qTD8dZyM0787CHPaz5QYPHKNRzn/vc1qiSzx2rcS98eJ/EQZ4VprA5catjKh+XsKFR3/cXL+nh3p/xjGe0dDr11FO7xz/+8S0MYdc9gMAjDcQDw/BLQHDv0km8PvKRj7T/D3zgA9s+9+5eHC8s7yJRZcMNN2wNUeWSY0pAdZzwHOs8osZd7nKXZpnjN/ppiqOPPro1Wl/wghdclxbi7jkp20CkIrA8//nPb79dG0aSCU8vfvGL2zWls/sTvnJQmnkm/CGeIYTVhzpBOWAARx3gva4yVJlCEFGmsVBUdio7CLaEEWWB7cUSTatsHW4Xyv5vfetb3ac//enuox/9aCvv1HnKMXXSN77xje4Od7jD1NGrE+U7y0vl9FlnndXaiBdddFFri0p3061KrPK87Fff7LHHHq2tOhekNfrpLW8Qa773ve+1a5hqpV0izYk2xHxt2D7yh3yiDuHEz3PRVlWX2JavPDPrkLCAJdgZ8HA/hLB+3TsbKp+ow4lnPknMajWEEJaLVi5d+2fNCSdQSBMzFN51ns6l484999w2ak81f9rTntY6I74Rr5Jm1q+CUIGbIqCgVglstdVWrQHyne98p1XyRvlVEhAfFdyWW27ZPeABD2gCyv77799dccUV3Tvf+c7rOkXDpFYBGY0XpmsKU8fF+RZ722yzzbpb3/rWbdRdo8Y+FZxKVhxUwJtsskm7R/emIyUuOmYqtuoE+b1YDaIwGZMKJ8cee2xrTGlc3elOd2pzfAl68mYJJzre5rXXc5avZxJOxEH+JGhoQBvRc+4555zT8iRBjvWUxhTxT94ywuQ91IhyLU6DqTr5Gkdl2fCiF72oNcB22WWX1jgzmvaSl7yk5WXTTuRf+XvPPfds74X70JAjSBhlq/e8LE6e+MQntv1f/epXWzx/+tOfdo985CObFUYhXVxXw5CQ6H38wAc+0IRL75h3hbDgvTj55JO7L3/5y+0804BqJAvKAQIHxMH9eR+NBoqDtDaC5v0jbG2++eZNLHIN5QDR5G53u1tr6GtMaiw710KNnqFyRBz4aTi7D2UQEWOYpgQS96RxS/gVVyNwOkjiJv1YG/WFE2LJO97xjiZ+abRKr9e85jXtWfrihnhKGxZA0pGf+MhH/fREPYcQQkC/TBhu9//P1OZQzppqqjzT3lLWE50J3Y973ONa+alto04wBbGmIY26pt99/z51zCj6x093XDEq/FGo37Qx1X3qfPenvFcGK5OV9+qCe93rXq3uUIcQHEoYL974xjc2ayLnOn6uqFu0GT7xiU90p512Wqvf1NPQFlBnSW+WRraJbNqk1Y50DKddLQ38V0fNlB4G+NTn6nxtgUmo9oy6TbtafV0DFiGEsFys2d40oUKFRKnebrvtmpqvIFYIK4CNDOtgqQB0TlTQOkEqFxU6P+q743QYwU9HQwfF6I5RWpWKDqiwdMz812HZdNNNmxCiYnFd1H+dUJWQsD/2sY91F154YbsGf2EzbTRqJHwVqd/ipqJjkk9YIagY1Sfq6MRVI0XlP64Bgdk0DsLy0X8+BAx5UP7V0dXIhGerIVTPuZ61/zM9X8do8BAZdPLlJXmfP9HE6A6RRt7zDn3qU59qDT9CDLHBPnnSdYbX8r7tuuuuzTKCAMniw7tU+Z1oaeTMcS9/+cvbNYSpMWd0VOOKEODeCud4F/hpWBNDLRR3wAEHNLGToFEQJjS4CUqmCImfkTWCACHB/RAqvGMaouLJ4swIp/cadU/+SxP36honnHBCSyuilHvxXIgZRF3vJLFC/DjbxBJp5v6d7/3WYPXOe4+VE+JPFPO77rmeZUG8ck+Pecxj2n0pnzR63/3udzcLJCOX7kmcpKHrf+5zn+ue8IQndL5GoCEsDcWF5Ypy6fWvf30L23HuUwNfOTkUTTCMTwhh/aZfJgy3OW2RUaKJckuZp94hAhvsUf6qE4jR73//+1s7xyeVTUkk0BMTnv70pzfxXH2IYZlUv4f+RcVr3H5UeT+TK7TlWGUo51nDEP7/6Z/+qXvKU57S7bzzzq1NRtAmGqirtDNtv+lNb2pCkTJb3acMJ4Zrx5VoIi5VD6nHCBjam3OFxaf2r7Bc13+f9yXmqPOV/+6BZeurX/3qdg9EDnW4AUMDD+qRmoZe69f002McnqN66fTTT5/ymT0Vfg0iqEtDCGG5WTXCSVXEsyms4TidkXIqJSPgKjoj36973evaHGAdGBW0TpcOHRHEtYxwq9RVJEwlVWQ6OCpyHTGdJqMHOoU6YYQMo8s6g9YbYcnyspe9rFU4NXotDpwRZ+EZhddxMqqgUhW+Dpm4E11MwRA3I/OupwPpfFYH4igc/6vSrwrX9mzTKaws+s+NBYf8ysqCmSrRj1WVRhiGz7jywCjkU3mn3iMCiUbNoYce2u27775tPwFFB9vojndBPpanS1wx+iS/OqboX08DqSzDbFdnHra9a8yxiYXWHdGgk7+9T6zACCcltNS9lejiOu5bo8/UOo0pYfetzNybY90nCAZG8ogF9hnN9H4SNlhdsMJgbSGcSlNoFLtevWPeU3FUZuy4444tDPdp5NB76lhClGl6hB9rzyhjjNrZp2xh0SI8oo046gQIxz7lU12vf++F+LFus185ICx+no0GN7EE7kF6uH8WQ9A5YVFEuHFtac/ShOWM50jkrQZyCGH1ogwp+tuTUgM8k6AMU3YTkgnF6gz1FpGEOKANpYzRgVffEEvUJf471pRFYq/O+RBlmHJY536+KFv7rhiWuYW00G4kLhAvCPCsGdWdyl9WINqJLIzVX6ZPK4O13QgRhKJLLrmk1bdve9vbmkVmiUIY9Zz6cWMF4hoE87lgsIWAYwDB1HV12Xvf+94mRrGEdg9VX/YRr6oHZ8O447QZ1FU1MDEJlQbqS3Vfv64PIYTlYtUIJ3NFga4Ark6QSkJhbuRYp4GoYQRABWM0mIml4/yvbecIR+VFOVcREk0U6BoKRuGJGTpqrqVTxfpEpaSC5KcS0lHRWarfjqfgw29xMeprXqj4GFUWT50z+3V2dP50ooQlTuLHQbgcZlvhhaVFPpwtLJHkD/myGg/ygA6yDjb41bOe7pnLI/KXfMTJh/2pZvKijr9Glkbescce247XaNMQluflaYJF5TcM70fYJayIo9/CF5ZrGL0y6iZvs/RgeqsBR8DwrljgkGVIn7ov4dR23Yd06cOv4uR4725fFLGPM6Lmmlx9BQjO6Z8P5YSpcvA+Ck+DWLmhQU2IES/3xzpHJwLKAaKqzgERTONRI1K4Rv6IOkQN98uv7mmI8DwvGOkjYhXEqIonpLlGsvIJyifb4kZAOuigg1on5eMf/3gb6RRvz6aYLg+FEFYu/XKgvz0pyj/lm3JC20aHm9UEwcA6VU9+8pObBRwRm3WFOkM5TlT2n/BO3LVtAIqlm04vqxLlD0s3dRtLQ3VOrdOEfvlT28RhA1M1tbLPJPc5XdmmDDYwxYJP3eQeCSKEHtNYicvuVTqIh7Ydywz3YzoowURZr+w1ICCMxz72sdcJLIU49N102O9ZGHxjvamsn4l+mOLhGUg/6S0+6g/041CuUAdJV9ce7psNw+O1YdRDk4ZTaK+bvmpQMYQQlps1IZxM12CoCkDHoTo/OksaA1WYaxwYXdex0OlzrE6cTohK0Ug/Px0f/o6rTqEKiemidSCqw+hYFXG/I6Tg12BgiaIjowIz4mKNiQ9/+MOtsyouLAyYgOpgih9n5NrxjnVt99TvvA4RTr+SGqZJWHr6z6O/PQ4NV8/YtAp5hhWDBT417AgZ9fzlucp3nvPwWcun8o7O+8EHH9zO85tFCSGQiXGNZrGakF811ExL01mXR/lpuLGOsF3XKYGhcF864eJj2zvgehwR0vsnPA1m74LGpmuYB/2hD32oNbBYdPXvwbmOqXD6nfyhwAJWIN4TQo/jvYv1DooXEURDkuWL0VAm4ETKSsM+7kFclA0ayzoSGsfED+KFuLAGIya5DkHFiKkpOYQJIovjdBqIJO5PmrE2UeaYflXX6VO/67+4VaPZmktGXoWvDJOunqW4uJ5ySHmh3CLySlvH6Hi4D+sEeN7iqJyRtiXKhBACiMKmBrK0s26SekIdUoNH6iFtH2KD8kXZphwyfZhVhWk32jTKGWEpa1lksDQ0LbAvJvTpl/1V/inTlWum8cyVKsuVxaY+swbxtTr1KkHHoIC2FaGBJaJ6QVlP1CH+HHHEEe3e1Vsnnnhiux9TIVnvsaYhkJQl5CjqXsRh6GZC3aF87wvm46jwWP+Y0qouVweUOCUeFRf044VKp+H2fFDnq5tmI/yMQnuFeDJcsDaEEJaDVbM4rIJTAToOnSgVeKnkQ1QAOjY6nUxIa96m6Trlp+JTyKtECR1GD3TuHFPmjkQUHUCdL4toqUSNyhs113mqa/tfHUvbrm10mrm8+a/C52/NBlYERv81OpitmragM6OBobMqHA0U04g0TmqUvV/B9fG7v8BkH/sWojIMk1HprvLX8fZ8pkOe1MmXt+pcHWiNVeeyQpBXCW2skKoh6ljvgbzoHM71TE3hb70U+UnekOfNcfYuWJtE2EQElhJ+64ATHeRJ75dGdAkMwq18Jj/67Vh5XCONI17osMvL3mHvjHO+8IUvtOOIDkYt3YMGtnBMdamv3MC5ru9cU1KkCXFFPLxL4l5mzxBPc8eVF8ylCRSOF7bruDcCgrhpUEoHjWWN5kpnjU7XkU7STpg6ARqv3j9p7Th+GoPC954qPzTMmXRrKLJqIaS4X7/F1T27H/ey++67/1V6+i/u4lu/lTviw6oNygyjidJemeH5a1TLW45xnik57sMitaxuaiqONWvkLXH2jIVLyCkRJ4SwdlAWcNUWmS2EWWsrKTsI3SwvhKNsqkEj/+2flCrXJoE1C/Gepd5Mded0qF9qkVnltfpJO1D7S32n/DQ9RrnNTRdP91FMej+TQOSxfor6Rh08G+r5EbgsrDuO4bOYy7NB/7xhGMQyQpT6lgA2KdoOBgy0FVirhhDCcrLmhBMF9qjCv36r6IXjGJ1LnSFrMvDTidEg0PmBTlUt3KhT4rd4GMl2HMsRHSIdl2qY8O83Umzzcz1hCYM5P7+aRsDftYVV17Sfn46N6/F3rI5a3Z//w3ut7ep8DRkeH5aGSveZhJNxz2em5ya/1H75Ut6p3/a5noaufcKSLwkO8pRjmcHWyI7z5DPvhm15D0PLhAq33ifUNUfFd6Z7KOpexh07Lpzpwu/vE/6wI2E/x9+9e+8cX34lzigzpBf4eTcdo/yxz7OtY73PwjAaC+c5jshBwOH4VVwqXtLUc+n7Dbf79P379zkJ48IOIawulDHEWQM7Ov9zKRMM5hCUlWG+6mJ9p+VE55vFCktBAnT/nia5v2OPPbZZjbA6YZFhSs2kuF4x6rq1fy7l8CgMBJg6RAz3TGZTVksrg23Wl+lTaaXe0RZQz/QHHubKdPd82GGHtbXUrNNGlJoU7Q5TpYhGrMVDCGE5WRMtZYV2jX6oVEZRBbvOjQqDq46hThJxhp9R2ir8NUCMDPNXUfV/E2lc0yiFfcKvSqlPxYe/sJ1fx+l0uXaFyc81CCsaLPZB+OJK0Olfo7br9zgcF1YH454lf8+xnuXwmcojw7xQ2/bJe9WRl2+dL88N87f3gJ/8VtiWF4fXrLCK4bX7ceX6+8uvsF3vSt3LOMbtK/9h2OifI/w+jrW/79/3sy3tiF4acYRUDU/vKT9OGom/NDTNiavj6rd0dr7ODIHK/v41ilFxHZZrji/XP9+5tV37+5Rf37/CHl4jhLD6YKFm6ghxAP3yZLbopLIOsL6aqSwYVZ4sFSwOlJcWah0yyf1ZhN+UIxYmhIUho8rHIa5XbhTT7ZsLnoM6hqUg+vXUKJTjFn8diiaoeLEA8Rl87diFZph26jv131yvZWCmBhxDCGG5WRPCCYz0Ek505KpiqQpQRcKVpQhX/vxq1HjU7wqnzlHx+A9+Okr933XsKFfHFfW7/7+/XXFxbvnZLv++X4Xv3qVFv+Ku7Qo7rGzGPSfPsZ5l//kO0ciQBwrhVZi13fdDbQvXduWpckX/XKNVZRkxKj79uA73D/3qd8VjPgzDnonhscqRstaqe0XFr1y9cyVEVVrVcf2041e/lRlcvdvEFf8d4/0dxgdDf9vl6nfR9+v7o/z6/qP8QgirE4NDRAZurigL1CGmMJqmXMy2jKgycqEQD8KzaYoYxmPSa5nmOkqEWYmYGu5ZzlY4mE5YqXSyXp/pntWOHMdCPEeWlRharE6Ce5rNGi8hhLDYrAnhpCrRmmpgqov/TBDr90yOeXzfOa/+V1gVnoqg9ju3tpnkO8Z2HVv7yq+2+65/TrlRv/vHchW++NRxdT2dvyHSabYNn7DyGdeo0emvfNnPf/zkFe9J5ftylZ/q2HIVRt/1j11IVkr+JAZZZ8U9VpkydO693rlKP+dw/JQFtd1Pr3pHPQfn+c/VPv81Ekc918VKm0r3lZD2IYSFoUTbYi4d4LlaCSx0eWLakfLUOm+jmPRa1q0jRhCwUWlT8V7IuM8X9YU6aSGtQ2pgYJhHhixEWlTaziTSTIf4spoNIYTlZlUIJwruoYo+qhHgGAWsEXfO9qhO4ijnuKETRn+7ftfx9buupXKr69e1++eOc44duuEx/bBqu5z48HcewUQ6rKSKP0zGbJ/duEYNv35erDwjn1RHvTru/f2zdXV8/50c1yifS2N9pSD9xpUNXKVdpUf//3Df0Am3RBiOn2dmlHdY1o1D2pZbSpbrma7mvBTCUjN8XyZtExBx++dMev5Cory0tglLiYXA9BEWDKaQrFTq+amrLVpruuhqRD0KllBzxfOfj8VKCCEsFKticVhrLqjEqwCuKPcrcn6zrdgnOXYUTOuNVCjIVWaUdGH6L66+5KFSvuaaa65bzBWuyQ07RjPFxfmmAxgd4HSwaqTAoln3ve99x37eL6ws+ovDzjcfTkflOSzWNdZ3Zkrj/v4h/ePruEme01zOCSGsfXx5xtfYjjzyyO7Zz372lO/kWA/ksssua59iJ+4uB8q5KuOe+9zntq/A+Rw7oXk+vOc97+n233//9tUZX2xbifTv/VGPelRrd/oqmvbffPEp5ec973lt3RTToOZLvz7qxxs+4+wLcz5L7Stwc8H9+9oki6P+NOQQQlhqVqVwsphYHPOqq65q4gfhQ0eX4m/k2Gc7fcZUZaNBoSKTfEw+/edYfDApVbmV5UdVJpXU/UplHI7tV0LCsl2CSe0z71VF8oAHPKB7whOe0O28887N5D+sTOQneWS5GqJhean3OcwO6YWkWQjTU2XLQgknL3vZy9qn1H22noXGcuNTtD7L64tBPus+Hyycu8cee3Rf+9rXpv1c73LSrysOOOCA7gMf+EATD+bySd8hSymcnH766W1xX59IfsxjHjPlOxnO81UebXP9gOE1QghhqVgVwglxgGBAICBS+F/WF33GFaRVyPrvyzQ+veq/cIkjF154YatAdGpZkLAW8RURgglLD9YlzAxZddz+9rdvX9LR+SVQcCWQ4KY3vWlbeMwnipkX9r864vqV3I7vx7fiWFR8/Xe//fmh0oC/64q77+OrVIg6m2yySTPr9M373XffvR3n3H543HKzEuKwFAzTXD7y3FT+ZalU1DF9v/kwTONhuHN9Bv14Dt/BPnMJvx825ze3UGmylPTvv+6ltmdD//whqzE9Zot7847c5CY3mTYNQgh/ocqYEk58avdZz3rW1N7J2W+//Zq1CeHEV2iWm2OOOaZ7ylOe0u5vyy23nPKdG6wfHvGIR3Snnnrq9Ra/XUn064y698svv7xNWZovSymcaKNut9123Rvf+MY5C3msn0477bTuJz/5SRvEDCGE5WLVWJzUmg39QrlfWKN/K/2CG871ydDddtutCQ0a5sw9y5KFOEHJv/vd797d61736rbeeuvrplNUB5cQYoXzSUwlK07+9+M0jN98sXAYZd8I0ec+97k2F/ijH/1o9/jHP7511nVwXXO6NFoq+nFYX5DWnC8DEN3kJekwKn8sdPrM9Jzner3FzD8Vp8W8xkqmnx+KUX5rEffJaSCvr88/hEmp8kFHdauttpq3cPKa17ymO/bYY7szzjij22ijjaZ8lw/tGoNBX/ziF5uF7XwQlsEllifahCsdAg9rYm3X+jz0fFhK4YSViDg/8YlP7A466KAp38l4znOe08QuohmL7hBCWC7Wq6k6LDVOPPHE9hk6HVcrtbMeufnNb94sRHRoJ6FfWRTDSmM5sK6KkSIVogZUWDlkqk4IIYTFooST+U7VOeqoo7oXvOAF3be//e1mwbLcbRvrkRA7TNlhfdFn0ridfPLJ3a677toGmqy/sRLabdNx9tlnN8sYa7PstddeU75zZzGFkyEGiwgn22yzTZtuNBde+tKXdscdd1z7HDWr7hBCWC5WxVd1UAVyFdBD+I/aV37+syrZc889W4Wx7777djvttFNrYJi/O0o0qTBHhYthJVHHsu7o/19o6jp914copIHh3mrf8JiwPCxWnghrn+SbEMJsma8QUBa+9cne5RYWTN0TJ1Oph0wat1pMv+5tJdIv78WXtfP5558/5bOykP6cOA/rKYOepnpdffXVUz6TU9NzrCsYQgjLyYoVToaFb62lMK6CrIJ7SPn191Xh3ndzpR9uxWF4zfleY0j/OuWG1PVq36hjwvKykHkirH3yDocQhgzrEZa1mO/XVww0mea7UjqrhAOdcOvSzRdTtcEaAiuxbO3HiZUFq+j5iA+jWOg2iDgP01Jai/+PfvSjKZ/JMa3eeoEV37SdQgjLxaqxOFlIqnDvu1HMtH8Uo85ZjkJ+kjiHpaMabCGEEMJCYy031Cd759r+KOGlrDKWu7NqbQsL9vu64XwhCuHPf/5z+7/SYXHBktjisKsR0+GJVKYqzwX3ru20UqyfQgjrL2uyF6eCZ6Fi5GWcqykTdeykblSYfeeYKtzr/3I3PMLKIJV+CCGEhWBYnwyFk7lS0yNWSmfVdGpizs9//vMpn79Qbbnp2lj9fXVvlVarAdNdfvWrXy3I9KKlbov6fLSPLIx6drOBcOJ8FlAhhLCcrCnhhEmpEQSfGqZs1+eFKd1DZ5/PEnN17CRuVJh9Z4FWhXxVUBod6TCHpaDfKLI9/B0WjqRnCGElUWLAfBfTr/NXUmf1H/7hH9oXBLWvxjGbMpn1AhFmtVicYPPNN2//tS8XiqVqk26xxRbNyufiiy+e8pmMWm9nNQldIYS1yZoRTlh5qAT/9Kc/NUWeOg2F7dANrUdGueGxw/Nqf9/194uPQt7/CCZhoZDPRlFWTujnt6FgZ3tcGOsT/TSYzTYqffv003Zcuo46L4QQFgNtIJRVxVypsm0llV8WiDVA9oc//GHK56/pl8njcMxsjltJ+LKRQcEf/vCH7fe4+mYlwuJE+9wXmuYC6yn3G4uTEMJys2aEkypUCSYqRCMKZdppUama0+o42/21JhzPj6sK1XFchTM8zj4Nigq7ruUY2CcujgthIZCX5C+jbT/5yU9aA9KCa8x35cE6BhopJ510UnfRRRd1l156aRulsrCczw9WHp2Efj62vVj5erHCHSINfvOb37Q547alzU9/+tPrpY1tafz5z3++NVarzOjH0Xv+gx/8oDXk69zaf9VVV7VPn/fLmrmwmOkdQljdDMsGwomO5nyn6mjbYCWVPbe61a1aecrqpE+Vvf3yeyYcO8nxy83tb3/7Vtd/5zvfmfKZO/VMl+rZbrjhhm1x28suu2zKZzLkZYNDNSAaQgjLxZoSTqBSrQpRB1MFqxNp2zFlfqoAtu04gkcJHUZpHOd3hencOp6/Aty2hoVGiv06TqYFOacqY8dyIcyHD3/4w62xVPlKB//Tn/50y28nnnhid9555zX/yvs4/vjjm2giPxJQzC0+44wzmv8opsunlaeJCx/5yEeaCFPXWQi8T/XlhoUMdyaIJh/96Efb9te+9rXu9NNPb9t9PvvZz/6V2NTfJtZ+8pOf7K688sopn7/sL+FqLrjmJz7xiWa1tpRpEkJY3WjvaJ+Uxclcy48SfFdSZ5XFifiwvBgyyX06drWVq7e+9a3bcyXGYzXFX1zvcY97tIGeuazRUm11eTuEEJaTNSOclNChcPVfo0Fn6JhjjmmdSx0gha/O3wUXXNA6TUaEddqs0u77+NwVV1zRzieKCEPH53Of+1y7hs/gfe9732sj+M4Vns7occcd1/3zP/9zu45V38tCpRBeCHOBZckpp5xyXR6EBi1X1k9lbVJoXJhL/IhHPKKZ9+63335tjjFxQp4dxXSNsNrnfcINb3jD9n9S+u9Bf/uEE05oQlCfcULObN6l2b5v0rD/rg7TgBiqTNhtt9262972tlO+18e1qoMxZDpT+XFpUWywwQatrDrttNPa7+meTwhh/WVYNhjIMRg0XfkzGyrc2ZanSwGrBeWysnF9w5dpdt555zaIwlpytfHgBz+4u/DCC+c0mFB5OVN1QgjLzZoRTlTyVdHX/7/7u79rn7Dz36rcCl8F72tf+9ruhS98YfeLX/yiVcRGdd/ylrd0+++/f/fLX/6ymQXqUOkQEUludrObte/QU8rf9a53dc95znOaci5Mi5W5xo1vfOO2XY2M6tz24xXCpBBA9tprr9ZoIqIU0+UvIsRnPvOZ7o1vfGMTCAmFGtPydJlfE/7+8R//sXvMYx7TpqIIQ95/1ate1T384Q//K8sUDVULKT/0oQ9tv7/yla90e++9d7fPPvtcFy+dfALiy1/+8u5JT3pSs3IxjaVEH9f4xje+0Z166qnXxdl79O53v7t705ve1B155JFNRPn1r3/d7s354sGi5lOf+lR3xBFHtPiWlQjse8YzntHeyWqQDdMD7u01r3lNt+eee3bHHnts8yM41bH+V9oUX/ziF7uPfexjLZ3KNPwVr3hFC0OclRtVTpQA86Uvfal7whOe0L3yla9slkHV4DvrrLNaehGxpL3rnXzyyd1RRx3VHXjgga0sIpC5P+cqgx7wgAe0RjJxN4QQZoOynkA+36k6VTauJOFEW8zAgPJyfYRwop7QLp0Po+rIxebud797u+53v/vdKZ/ZQwhkabQQXxQKIYT5sCaFExW9ynXbbbftHvvYx3a77rprt8suu7RCl/v7v//79mk3HZ6aTkP8uMUtbtEKZ05BrVOnw3S3u92tCS4ccYS5qGtpoNz73vduHU2j+0amhV9zMbkQ5opRJVM25Ct5WecblW/Rb9SW3/3ud7/W6daBZ7nw5S9/uX0JSieek6eJJcIlIhoFOvvss9tUFdd561vf2t3hDne4XtisrZzrPeG/0UYbtXMf9KAHNbEE3/zmN1tH/3nPe153//vfv01VESfhm1aEc845pwmZxW1uc5tup5126h73uMc1gYhYwsoDzmMlwzT5ve99b7fNNtt0r3vd65oQRLgh5rD4etGLXtQ9+clPbuIKgWQU3lXXIFII//vf//71OhajOgf3ute9Wrmx7777tvsgRG211VZNwCGaiJPzCC4szc4888zuq1/9aruGtL3kkkvaszKNR9oedNBBzZ8VnGf7rW99q+17wQte0OIj7dyf56Yc0UkQNsEqhBBmg+nD2i99a7q5UJaMVa+sBKyVoX01ly/L9MWC/vZqQr1qwE492mdU/bXSuOMd79ja0oSfSdH2qPZ7CCEsJ2tGOFFxlCtquo3RFw0JwodOm1HjQw45pI3i68DoDLJA0eG55S1v2QQPHTYdLBU1QcWcWhXWs571rO7Nb35z6/BpoOgYuYZGimsM4xDCXNHZJt7pXBMJCBMasfJZn8pv1RiUhzfZZJPudre7Xcvv8mbtIxbIyzrppq9ZP8VK994N4bCYYApNIOg3Lu2v65Y/8eULX/hCs6zQmCXSEEEs4EcgsJCdKS5EFmIH8ca7dM973rOd716EZe6298kUoB133LHdL2HCfyKQ947/Djvs0BbI23777ZugxJrF6JV7+PjHP37d1wZGQSglxLBW8V8DrH9/o95ZVmbu5T73uU8TjpQFxB3p+/SnP70JsLUwr/RxfSKqLwhst912LQ2UD6xsPMsPfehDLc09S2WH8KUXIVbYRhHPPffc1sCsMsv/zOsOIcwWZWe1e+ZDlY8rqT1j4ErbrF92zxVl82rDgMbWW2/d1hrrsxDpsdhoe6jnTfkdtUbNdMjL6lh5O4QQlpM1JZwM4aeSVeD6r6OmE6fztfHGG7dOkQaGkd1NN920deB0OHU0TRfQIdSB1ClUMencONexzlfxVrhcWZispIZGWL2YpqPTzKJD51uHW+danu3nsdquxpPGBadzXvv6x7A40VknFppW8773va919PfYY49mvfGBD3ygTZvp07eeYilBBNh9993b9DaCSYmI1RjVMHIt4uVd7nKXZjXBqsJaK4QG70sdKz41HcU7xnqDtQZB02/vWN86xLWEy5+oYSqNaTjveMc72rtc1D0L2z7nOJYYMxuLMGnvWsoPQkt/Xnl/MWj34X8dV7hnz8F1H/WoR3UvfelLu1e/+tUtLgSkfiNQOfP+97+/xfVpT3tauxbRy7khhDBblL0LKZysJJTZ1R5b36j6zOCdgY+5WG4UlX5LnY73ve99W902adyr/o9wEkJYbtaMcDKOfgWhg6NzA+JIbeuA2ecYBbNto9JGN1il2A+dIvtqhBnO0Qms7f5/VGUXwiRYu0PD13QQ645YA4RIYWqKqSslNMiTQwGg8nnlPZ1/eZQfqw/Cn21Chs6/6THEC4KIjjrLDuH2827/fREWawnHEjgsqGyb0FDCgfiV6GCBWtex3gormCFGoayTYt46sYQoaf0P/wumyRZm5ljImEYjLO8pMYlFiGk6Og2mHbmfeg/FV8OLwEMQdbz33P2ZwgPxHjbKnO8euC233LItIk3EYgXEwkXnhAAlTUFUtd9aK1//+tfbYtGeDQsbIqzzpFHFzXalKesaaUnA8tylh/iJV3WA/O4/kxBCGKJMUx4ToNca6geWeH0Re6FYLWWr9cPufOc7t3ZBtU2rDbrSedjDHtbqYpaXk1DTxqq+DCGE5WLNCCfVSRpX+fU7HaNGm2ufBoeGh6kAOktGffthDs+tcPvHoC+ehDApOvGmfPTxOT+ih7xlW169613v2qyk0BfwrFVC3LBtHR6NFeGZBqLD/+IXv7gt0nrwwQc38cE0GVNf/LZ4qcZZPw+zzmLxQiRg5UFQtMYJUcGCphqzGnP19RmiY8VROCWoaPiiH7ZRKIJHLSJbwk4JJ0QbJsqEHdYwpsN4Nwku1hZiqWGxVoKEd/eyyy5r1y3EzVoswrfYrPOJLqbvmPbj3ZUum2+++dQZ/wnBwr3qgAjDFD8CBxFLeliQVoPOfSonWLKwLLHQtClE1nphxeZawjn66KNbPKVZnSccaAB/4hOf6A4//PAWPwvpEaQ8l74VTT/dQghhiLJU+UJAnw/DNs1KwGL+BOr1cd0nZX89E9aL6hjremG+z3qp0FZRFxvAmAT52f0P2+0hhLDUrLu2IF55teO1iFZ1EhSyOiY6fOPQ0dJgmO52+mGOQ4fLSLARDZ3D6oDNJpkq7LqOjqx496cZhPUbwgMhoAS5Ufmx71/bo44dd/5CMYyHKT0aME95ylOa3ziIAO6xj+kprERqfZNi1D2w1vAu1xd8LLjKSuTZz352+z0dRB8WNMQiCB+TptNSpu0olEHSjIDywAc+cMo3hBD+mn55YgFRHWkLaM8Ha2qxgPNlMYvsrwRYMlrs36DAcJ2PSTAIQIxntWH6Jha7zF8IKo4GVoj+1v+yZsik8TYV97nPfW4bIDRQsZRYAJ0lrYEQ64XNJt31AUyztzYh0SiEEJaLFStTT1oRDEdY+ucPw1JQc0X/N0Vbp89ot//8R5lB9s/HqGuIz6T3EdY28kPlnXF5o+9f26OOHXf+XBjmZ1T49tnWeCYC1hdy+vTP9y4WGrrWQ2E1MRRNRqExpcGvUduHMFrXGP7voyHWF03Eey7pNJdzRsVnHDOFb6qOhqKvI2GSsEMI6xf98oR133SDTONYyWVMxU3doqNvOqR1phaSuZT5C8ls0r/i6Pn6tL110EzrnZTlfNbaEKwua7rxbKgBzEnOCSGExWDVTNWZyURPhUJ9V6EoZIkWKlnbHBGEub3/XO1znm37ys9+CGN4Xp1b/8vVdep88WBV4NwQ+ix3A20U08Wp9plm8/jHP75NH9HwGtf46oeloWNKjVFQDM8ZXtf0mD333LOdU1gjxdzoOrfOGRVn72Ixav9ispDXMxXJF3yE6b6X+l5CCKsT7RZl9HxZqWUOaxNfPJzLJ4mXgmEdN66eHDLb9GZtYs0sn/tXv86ljTnbOE1CLZjuS3MsWaxp5ktznPW7DIqYVsvCx+BGTVOdzX1Xu7rWdAkhhOVixU7V6dOfqiO6XN+6BP3bUJkoaKFQHooudS5LEuc5hl/fssRv/vaPsjgZXt8xQz8Io18xVDz7fmH9QcNC3vQp29nSz9t9ljIP1XsyjlH7Zzqnz6hjR71TlRZLee8hhLBaIFL7ktnb3va2KZ/ZMSyDzzrrrLY2k4WwrWO1UrAOlDW6xi02Phss9M2q0dfYLLo+jqpvsNh1DqtK63RxpvRq91oInhBhm2BiAXSLjFsAHb6wY9rNpHE74ogjmsWK8PtraM0EscpC7LWovGcgDM6aXCxJHGO7n3aFNry1wN761re2wYHZQpQhtDz/+c/vDjrooCnfEEJYetZd2zkZ3StbQag0WJP01woZ1dHq+w33VyG+2JXfXGBq6escPr2KlRjHsDBMJ5wMGxrrSz4Y9a7W7/57OzxutTLdffTvF8PfIYQwDiPyO+20U5viRxSYD9aW2nHHHdsCpI985COnfJcfXy6zeDmrE9aIc8Ei3xYyt96GT/IvFgbxCB7WrNL5J4hYP89C7Nq1LDNMtyE8mGLl+dVAHSGFX9UB9cU5n6637hVRazildbYcc8wx3VOf+tTuS1/60sg1tFxXW0Ua+/Sxr9lZjNaivO6FlYv7EkcQQ8SNMGKKKSyqzvKprECtGciS1OK+k+JahBOf6n/DG94w5RtCCEvPqhBOKPAqFOICputMjLP8WImonL7xjW+0NR2sFq/BY6QI6TCtTTRGNCRYUI1jfX327rt/z8PfIYQQxqNNsfPOO3e77bbbvBfR9BUyXy3z6dhaqHuxqDqvT5X9w/pQfAg5FhH3Rba5QLzwFTjnH3fccVO+M1PTUVhVEBHK+U0U0U71ZTfHcKxFONYZBIeClYfBE4OBBBFfodMm0Ma9xS1u0b7Gpk1ozS5ihGkqhAjbvgbXZy71pMXXtTVZhrC4cV8sSVhna2//4he/aNNrQAgRX3H19TlxIjoRQfi7Z8eIP6GoP112oYhwEkJYKawK4UTFoHJSeahsZqokVGAqBJWZystvDQoVmMpLWNR+lYMCmdii8K8pPRV+/zozVU72lSPc9AWc4VQh+6zloAL89re/3VauN3LA4uTJT37ygsxPDisTedKz1wAKIYQQFgr1C1HBQt6mcUzCsI3j8+k+ue7rOr7utZiUODKkH5+K35lnntkEj89//vPdLrvsMrV3ckxXedGLXtS+EvfgBz+4CRgsKAgfLEB8ecd/ogehRTuyFinXpiMScNqWflebj3CgLaejzwrDmiwlgNTn5f0uiwzXnctg30xt0plgSeLLbQQy+cbUGXElgPhsMDGEuEIgIeZMN9iz2Ehjabnvvvt2b3zjG6d8Qwhh6VnxwklVDv4r3FVctX5JYR9UWCqhV7ziFd2b3/zm5kfJJ7aomHRYHaMCE4bKjskhf9ewr8KuylFlWOFXXEZhXx3bP6bORfnzc00jCsw0jS6oFKj5RoqMOhBy+vTD7Ye5GhmXhkvNUqej+zYHWB6TB12/nms/LsPfC81803+p0m1UOsw37iud6dJ2tve+EGEsJ+KvvL7xjW+85p93CAuJjv7uu+/ePfOZz+z23nvvKd/Z4b3rv2+vfOUr27SY+mzsSuG1r31tc/OZqlP4HPGHP/zhtnaItqV2mTYj6wrtM2KBbeliqonfRAQCg7YmEUQ5RQRxnP/VplyMwZF+2T6fsrH/rA0eEoBMh59rmMO8s9B4NsQcebra9iGEsBys+//+v/9vaXpB82BYyFOfxxXSrFJq3qgOKtW81pOwT+WmYiOScPz8Ft6oMKui6ldYoyCaFP341nnDsMWNSSRzyJNPPrmNPJiqQ0Dph4Vx97raWe77mumZToe4z/b8/rE6hLaH5/pd6dHfV+eOS6tx/tMxl3MmYXhv82Ex4rqQ8VtoxG26543Ffn6LScXd/+Fz4Fdln/dEQz6EMHtYRzzucY9ri2j6OtkkDMud7bffvrWNTCVeTlh61KKjBxxwQFtzxbokr3/966eOmBzlTFl5sCbRXhR+TZ8hjCh/CCELbWkxqtzD0H8c/We02IyL01LGAQYSDTD60tyhhx465RtCCEvPqhBOJmFY+a8GTBlSSZfAE0IIIYQwCVdddVVbONX6JvNZl8RioIQT60m84AUvmPJdeFgS1LoghAuDSdbfYAHik7vux5dbLrroovYlHF/RMZ2ZNQwIINp7q6HNN1thZByLeY8rvd0sj5gyRBSMcBJCWE7W/b//7/+74oWTqnAWumAfVmTD8GdbmcwUzihWekUVFo9hfhmSfBFCCGFSWLA+/elP7w4++OD2ZZ25cu6553Y77LBDs9j1FRQCx3SU9QaIGeq4+m/Ks44vqxG/OVbDHAtisP5FTZ92PWvRmS6z2Wabdfe+9727rbfeunvUox6VAab1EPnHgrTW7olwEkJYTlaFcGLFb5WwuachrAWmE08inIQQQpiUCy+8sH1m9vDDD2+fq50P73rXu7r3ve99TRTh1Euj6q1Rfv06zHatHycc06P9tm0dEFNjrKFisXxCyaabbtoWV7V+iCkzo3DN1JPrDyWcPOYxj+kOO+ywKd8QQlh6VoVwYo6rxavGVaIhhBBCCOszvpTiazpHHXVUt80220z5TsZKFSVKoIlgsv7BAsk0rQgnIYTlZt3//t//e8ULJz4pzITT12ZWIqnQQwghhLCc+HTwfvvt1x177LHdHe5whynfyVhJwkm1rYq0sdZPCCemjO2xxx4RTkIIy8rkH49fJoYV6EIh3HKzZVRlngo9hLXNYpVBk9IvsyZ1IYS1y5///Oc2rdnXYOZKtWVmKi/GlSnlP3RFf3scdUy1rYZtrGEYswkzrF48e1O98pxDCMvNqhFOFotRlfJMTHJsCGFtsFLe+36ZNakLIaxdCCfWg1iIT3nPVF6MK1PKf+jmQzrM6zdEE2vjsDwPIYTlZN3/8//8Pyu+RlqqqToq5/lW8CGEEEIIS83xxx/fHXTQQd1ZZ53V/d3f/d2U78pn2Paa7++w9rjLXe7S3fe+9+3e8Y53TPmEEMLSs95bnPRJxbvy0CAKIYQQwvT4vK+pOkbnVxPDttd8f4e1hzztIxEhhLCcrPdrnMyGdN6XjzSIQgghhJn505/+dN0nf9cX0j5bP/DZ6kzVCSEsN+v+4z/+Y8XXOj5HrMD0nf9JMd/Xityj6Fe4tnXS6z/3X//rf20NkPq9Fqj7W+kM4zlJvOdz7kLRv+ZCXn8uYRmB1JjuM10aTbcvhBDCyuRtb3tb9+EPf7h9lni1Mq6+0Zb76U9/2v3sZz/rrrrqqvaVlXve855tX+qotc+OO+7Ybbrppt3RRx895RNCCEvPmhVOVKQEE6arJZwMK1bHDKljqiK2yBqle7UybFCshgZGP44aSX/zN3/T1reZJN4Wybv66qu7m9/85t3f//3fT/kuLeJuUbMNNthgymdh+N3vftf96le/6m51q1t1//f//X9P+V4f9/6jH/2ou+td7zp2rrt34yc/+Ul3s5vd7K/WDyK2/PjHP255f8MNN5zyDSGEsFI59NBDuxNPPLF9lngt8C//8i/dJZdc0n3pS1/qPvrRj7bfN7rRjVq9Z+27pzzlKe2ew/KgnaB97rl4HjWVpgZrbnzjG3e3ve1tF6QN/aAHPajbZJNNuve///1TPiGEsPSsWeGEWMJs1XklkAw73uVf/xX0Cvw+LE50PFmfrFYuu+yy1uh4+tOf3m200UZTviuXvnDy/Oc/v4kf//RP/9R+zxaNrRe+8IXdG97whu5ud7vblO/sGZdnxlHHo84RZ/6ve93r2u+F4uSTT+7e+973du9+97tHijKXX35596IXvaj74x//2L3zne/s7njHO7Z8LV79+zFq98hHPrJ74xvf2O20005Tvn/hcY97XHe7292uO+SQQ9rvUfdYTJpeIYQQFpaDDz64+8Y3vtGddtppUz4rF3XGqPqCYH/66ad3X/3qV9t9/PrXv25tgB122KG75S1v2eoz1iaf/OQnW/127LHHdo997GOnzg4LhedDCCGIGIjyHAgkV155ZRuU+eUvf9naGNoRBmB+//vfT535F/7bf/tv3a677tq9613vau33+bQTHvzgB7fBog984ANTPiGEsPSs2cVhFdAlmiikuZr7ywrAtv+wzaqh/AvnCGMopkC4Q7dSYeL6i1/8olV+S8lc06Rfqaq4xX8mCGVMlFXocO25VM7FqHOnux/Hlytsyz995pImw3Pqd+XfIUceeWR305vetDvppJNaAxPy9fCe+BEE+3l+SP8aw3vsvxd9/xBCCEuPunK1fE2nX1/okKu/99hjj+4e97hH94xnPKOJJ495zGOaBY2pR//8z//cvfnNb+6e/OQnd9tuu233lre8pdtss826j33sY1OhrDxmqu8JDz/84Q+7b33rW92nPvWplga+GvOmN73pOuee3SOhYrFgwfr973+/WfZ88IMf7F7ykpd097vf/Vr7Ycstt+zufe97d7vssku37777NnHOMeeee26zWvWlm5e97GXt+XhWJ5xwQvsvzq95zWu6z3zmM21wBvNpJ0jLtDFCCMvNumsLvsl7cksMixOdtEk+R/wf//EfzeKkKi4dWB1ElY9tlhc629VppJob1TA1R2e9Cmj7+VHOIR7TdTSHhbvfXJ0j7LJeUenYHlqzOP4Pf/hDM2+0T2PIf+H2LQdGXat+O1+cnVfXtl+6EIlsw/HSoe5P/KTbP/zDP7Tf4+733//931u69Pc5lzgznBrDzz30w6x7QD9NoDEhbDz3uc/tbnOb27SKvB+X4b2rxFnUaGSwnmB1sf/++3cHHnhgd+c73/mv7lE43PD+TINx7YqP/RwBoa5XaYfy8yylax/XFrbGg/sftWDf8Jk5hhkyf875/W184Qtf6I455pjrBJL+Pbg36SCtjzjiiOZXGDm6wQ1ucN31zBffa6+9ute+9rXdAx/4wOaHym9PfepT25ziV7/61VN7/vNZokxvXU/chvcmvbnV9EnMEEJYzbzqVa9qFgE6sSsdU1m/8pWvNAvKM844o3Xe1dU64g95yEO6e93rXte1Gcahjvre977Xff3rX1+2KbmFerDfJhkFa5pzzjmnuWuuuaa1O6WDdqnzoc50L9qp1e74+c9/3m299dbdhz70oW6LLba47ti63kzXVk9rV0lj1zOQZjrvhRde2P3mN79p1z/vvPNaOOp2gpQpvNoLft/iFrdoU3ZZuN7hDndo7Y7Ztscf8IAHdH/7t3/bff7zn5/ymRum6tz61rfOGichhGXlLz3GNUx1Pk855ZRWQasgFOIqDBWSbdNZmIV+7WtfaxVQnYN+hcRf51FHXVhnnXVWGxlxnspnWHmpnKj0Rx11VJuPW3OPTbUwmvK0pz2thVNoADziEY9oZqlEAOaPGkPHHXdc269jTxDQ4HCtF7/4xd3hhx/epmboTBOZTK0wOvCsZz2r++1vf9tEBMf94Ac/aFNXjGg4l2Pm6lyI6/Oe97zu0Y9+dHfQQQe1a/fTAcw1TYGxUJd4qIxhRMj1VJIEAxU1Pv7xj7fO+W677dZGHUqssojdy1/+8tbwcTxU4tKDSSaxwbHmyBIlxNOoh+kv/PvprDFgFMQz3G+//VpaER80ulig8LvPfe7TRkHwuc99rvmZG/2oRz2qCQXygri4trQ79dRT27HSc5999mkNFnjGT3rSk9ozFwcNH6Nj7tv9SZMyJSUmiCuTYsLEM5/5zBZeH2G4P8/Y/T3xiU9s+evMM89s8fG8HfPlL3+5PUMiCWFH40oeMJ3GyJD8B40KDVH3Kt4Qx913373F8QlPeELLu/AcUM+XGCavaqDIr+LRtzgxt1je5GqUz3N/wQte0KYNGS3UGOTnOtJSngwhhLD41MDIctNvM/RRP7Cs0MYwDVTdqf2j3lPHmWZ02GGHtbqjL5qMC2/zzTdvAw4sVpabfpsE6nUihbaC9gzBg/WGNo761f0SIrR/tJO0P1l9fPe7320WKOrpElm0R7TlpJc2gGvV9aRN/9raQdpA0lEbghBloNDA4J3udKc2dUbbQJuDdYg2ykMf+tDuIx/5SEv/888/vz0TFqv8tCm0KZ/97Ge3qb2Ek3GiibiUK0yv0tZwnfmgTTnOyjaEEJaK/6tf0K1Uh1H+M7myFqDca0yoqHWkVUp+q7So6Ap1foQFhbzCuaZYOH8Yro76pz/96VY56UByto30DI9VeaqMVHx3uctd2gj+e97znta51AFnTaECu/jii5sI84//+I9tpXjChc62hpD1OogawhMfFaOOvt9GW973vve1EQpzf9/61rd2F110Ufu/9957NwGBAKIydLx7/uxnP9vi5TcRQUWo4UHIED8ddGH47RjUtZlkurY5q8QQaSfuOs7MZ1/xile0Tjxrh7oeoYVjKaES5k+wYhZqoTcWIsQsViUW/3rpS1/aOu+eg848EcH82D333LP7xCc+0Uw/hVHOqIg5zp6lhoXK3fOTThodOvQaDMQajSyCgDDcozAd9/jHP76JNOLtWAIIE1pxuOCCC9ozdy3n+13WTKwxrrjiiu6Vr3xlG4UhwtXokcaMfOW/hoq4WGum4l2OmOX5EMqIT/Km5yF+ZdEhzS+99NJ2X56pffKyRhjT5gMOOKBZkDz84Q9vI1P3v//9myAmXf3XcPLMPF/5gphCgBE26xL/WZ7IZ567NJXviD/2GcHU0CP+EPzcr3zt+RNppCERSprJz9ttt13Lg/KkBpMwRr1LcXFxcXEL47QXWFaO2rdYrtpYfb9qM3DqEcIBKxLrjRlIUWc+5znPaWKB/a9//evb4Ma4uPfD6zvtNYMLLChG7V8oN7y/UU67guihHidEuB8WNEQg7R7TXgz8aHuoJ9XXxx9/fGsbPOxhD+u23377JgRZx4O1B6fdQ6QgfqjjtVG1+Vyv6lO4tvbNNtts065pAERbRltF21BctC2st6a9anAIhBoDLeps9be2hLZC1fsL4Raq/ne+9saofXFxcXFL5da0xYkbhM6vQldlTTBQKeiY2tYBVukqkB2j01vnoMIo/FaxUeAtTKay8p8Ycve7372F0Ue4OsIqKgudqsRYT+i4EgRUhKwHdMwp/KwCVGIqXqaq9jmfg3jx03mG/ypFnV6Ciw4vAYeViYrYsSp9ppIaJUSFEgN0vmG0h9ihk+88Ig/LCGlDuCgIAOKhA6/TrnPuN2sM5wvPf9fTgIMV1Zmnuk8NBYIJpAuBgtWHRgELHGaYGhYqcH6OMb2Ev/QjqLhHC5H18TwdT6SRfuLEioToII468cQAaeW5SwciicYcywgCgNEcacjqQ4dfA8bIGG54wxteL/2lsfTUYCNomIfsebEo0vCoPKAxQwgyumYfoanWYOljZE040lr8C/7uDf7X1Kia9iSdWP6Y6y1feX7SgEAnjxJQPCt5VhwJW/ItsY71iLCIJtJFftQg0+iT1oQo5rp1LwS2Gj1jNaVRB+nBsWjRIHYOKyR5m5AlPcRdOMN3KYQQwsKhzq46Y6nQLlCX6CCrA9Qx2lqmkurIc+oV9Yd60KCTzrp6SxulpuXOBRYc6mHWqgvBuDrK/aHfvjMgpc7V9iOOGDhRf6trzz777Fb3syZh6ey3QR9tOwNirD9q6nCf6epIA3/S2uAGbBfS06CbeEp/bUnWJCxZ/OdYJhsUI9LUYrryyxBxmEtdXeeIVz9u/Pu/54IwtHvqOYQQwnKx3pRCKnUdRlNgCA1GPlQaxBOVtw4zKwcdTgX0UAApVAAaBSxCTPdggeE/Nd9ow7BgF45O7cYbb9x+6yDr0BtlML1E48FUDo0LAk51SPuIezWGakSm8Hurrbaa+tU1qw3hsioxpUJjpSwLXFeFTYiw36r1OrqEAdfW0TWqUaajGgSEF9Q1NYYcw8rFqAYLDOKGBkydq9KWLkxNpQuhgWWEeyyIK/0Gk0aIeAwhWGiAFASDEjH6ON9zq3gKn4igMQP37l4IBfZJ5zI3lQ88o/4n82ybllQVdu1z7cobwvFc+s+s//xt979i5Nh+3CscjRkjbgQrFiPEK8e6dqW/3453H/IDSx3WIyBUcHVt+0u4co70r/wDeVG+sa/Si1Am7EovEFTs5wiKxBHTkTQM5R1ilXzjnan8LX+xLDK6yNJHIw7iNnw3QgghLAzKae2Z/hSXpULdYVom60NWDaalaIuoNw2GfPvb327ihraJuqHqzKpf5kq1DQj/C8F0HXz3pn5Wp/lvUIBQwqLTPViPzRQclqUGiAy8mPpsKq12QF8oGXfP012/pk5XW6SfduJCnJHO0t2ACj9tvVHtJZY62gUGVoaIw3yFjj4VRwMsc0V8tLfGpVsIISwV60VPRqWu4CUwEE1YchiNV3HoKDJtZMmgY09ccex0BbSKS4VNLDDNxX/WE6wQ0D/Xtk6sQh868zqZTFTFS2Wi8tKxFQ8iiikROqqsI3T0hVtWEUYvmLZWxVbiD/y3z4iGqSZGJlSm4tDvTJvjaqTEPFaVOtyD8B2v4WUuMsuOuo7/wtepN7XGlBxmnpw0ZU1iuomOPmFJR96xwtCIIrDYLrHAvVd6QfrrjOt0s75x7wQLx/VHRYTLf4hGiX0aDnC/rlXp7hzx52e7XwmLP/GH1QurD9NtpKO0kUc8b/cm/YzssOiRHqxWiCtGcjwv57G6KUFI3OvZwHX7cdcIEh+CG6sYDU4NLs/FYmyeAYFL2K7LgkU+4eQZ4pZ0ZT3iWcu7cFxdRz4VJ1PJpAkzYc9ew8o9OJa/hh2hhAWVtGA55NmKo+NYU7E68Vzcn8ahkT5xcb57BWGQCGNu9c4779zO4RdCCGHxUKcpi/sd9MXEFF31ko66+oo1BdSbBla0JVge8jdlcxTqFm6uGPDQBlNnzYXp2nkwpUadyTLWfWibqae1B2pdENOuDXoYVDCNVT2s3TYdc7lnz3d4nvhrQ6i/tWlni3QjqGg/LDbaENpO1fabC+6z2sshhLCcrIo1ThbCQadYo0KlocKoTrQOtxF8x+mk87dd//uOn9ES65P4egnTU/+NNpjeUZVDHa+iU0GpPPy2zTJDZ5KAYU0Oooh42SZ6sNZg8moKjIrCqIUpHtam0Ak2ilDXqCk4tnV8VfLMRmtOMZFDhauDW3EgFAhXI4vVAD8daZYzrAmcL461rkff6aQbbTE9yVQRYpR4sS6wjonGRK0lIhzzevn7fJ3KnXWCcMTVc6hw3bspIhonLBvcu3jrpLtHx3hWGiR1v30nTdyv6Tbi6Bjnuc86t86TDjUK4zdrI9clWjFjtXguS5maB2xdDw0j/4k/rCo0BJxLoCJmuW8WNkQxYoR9rtG32hCXYdzlDwsCm0Jl8TXPy5QeQoVt1igaZO5bg0z+kibymkYdqx9pa80Rwpsw5eW6jntzL+ZYSx/PTUOXUOUehEXcIQJ5Bu7TgrPEGxZIFU+jhp6daUfyrQZkTSVyj/WuiJPFYqWjxiWBTdgVTlxcXFzcwjtlcAnYo/ZP6qpM54jkynZCuEVH1U2sd9UH2hymvJoKygpSPep3WcL2w1xop14kyhBxRu2fyUE7gzP4wCrGVCJtFvWlAQ31IYFE/a2usy6JdoB00AYyMKQeHxX+QjrPQzzhN1x3LteudklZ1S6m097Sxqg201yce9eGq/ZcXFxc3HK5dX/605+ml9xXAEYTVBiTfI6YKKDz7ibnSkugayul6iDOBYW96TLi3jehJdZoiOjgWgfE/8KaIEQe5pQ1dYL1AYsQFgQqIQ0GU1tYeuj86pxCA4fIIr4qdMfpyBNBdPhVmGA5IE1do4+4OtZ1dZzdfyE9VLQaGMIRfh8WJtKcQFLxZl7qeNdW6alECSasKIRXJrt1HdYqnrd42ccCw7Wkn+P9dr+jpvWomFliOM/1HWu7RlbE3T1JD2lJ5KiGB9y367u24/pIU3mKQONclig1jUZ44iytNRg1uIgTdY/uHcPfhbzAikUcNTbLFFcj2NowhBCuBD7xsC0O4kUgImoU1RiysFzdGz/5isVTHSt8aSQtK2/KF5wGsPvyzOq903gh3DnPs5eunq+wyxRZ3hQmf9fnH0IIYXFR/1nsnFhOSF8I1COEAoI7y0j1j/pY/epapjmz+FB/mH6s/mStuJSoaw0eqPur3TEJRJe3v/3t3Re/+MVW96m31O3WFDH4Y8qLgQzTsx2nHu+315YK1j0GNwwqGWCbDs8N/fZbH+1JlkGEH4NlC8G4axpcMjhkCvdcng+0Pazdpm3F+ieEEJaLddd2wNa0cIIq0CfFeSqBuQondX4x/N1nun3zYbZxmNR/JqY7b7ZhqiwJLSBA9Z/BMIyFuN4Q52G6cE0vkjcJNOY3+7qMdWBY2hSTxHU+zBTf6ein9ZBx+2Z7H4t1vyGEEP4TYjprTZ19YsJCwfqC9aCpICwrCQq1vlYfFprKehaYSwkxg2UjK8n+OmozUfWSwYdjjz22pZ/BABa50nDY5mPFa006g1U1ULWUSFfWzeJqbZWql0fV+6P8+hhgq2nrH/jAB6Z858e4a1oTxlf5WLPWWmiT4l5Z2bJs9hXCEEJYLtbsGidVeFflOCnOqfPmcj6G500XzlyvMQr3PK4SG3ed8q/zMEna9c/DTOcNjx9Fv7POmmMco+LJT2WL2d7DEOdxw7jWb/tYVpgaZHqS0RSNkL5oMlvGpcc4/1FUfDHJeRgnmmDcvrrWTMz2uBBCCHNDmT8cYFgIfNbWAIH/BIW+aFL1jLqWAMEKcqlhicBKhGXrpIg3i19TfE290ck3FbXSsF+PVnui/i814mSQhmURql7u1/vFKL8+rIZYjhJQFhuDSqxS+19onJQaCC2L6RBCWC5WhXAyUyUwCsdXZ1tlo7JTCY5z9vePqcpxug77clNxHTKX9Cr6500ShmMrPjO5ucRveHz/96iw+E0nBoxC3EYx3bWtTfKxj32sfVHAF4VqRKUf1nTnF6P8inHxmo5x4c0lrBBCCCub6lSbXrnUWLONm8QqeKGw9pfOuamok6COnK6NUG2VYrkEk6LEHOLYfNGude/9hfcXC1Y8pv0aZCombYfI1+5bWCGEsJysCuFkLqgYrP2goKVSc+al+s9vnBvut97EcjREZoNKfboO91JT8ZnJrVQWMm4LFdZCp9lKTv8QQghzQ2d0qYWTqk9Mc9HmmusaFvOBtQnriW9+85tTPgvDSqsrPVfPmECGuQyC1DmelfZtXwyaS3izoaY1zUekkb8IJ9bHCyGE5WRNf1VHRUP4sJgop9C16Ff97jv+5cqP8NI32YyLi4uLi4uLW2lOh9r/6mAvltPZHlrxWjDVoBPrjzqm9i22c10WJxanJxyNOmYhXLGU99Z3xA7XNm3F77nEA7XN4mQp7qUWw7fw7qj9s3FEF/HVPh+1Py4uLm6p3KqZqjPptIuizp3r+SGEEEIIKxmdYF81q6+yLRbVpupbZJx11llNwNhyyy3b76W21rDOiTUwLNy6ViGcaLSzrsF809j5CzHtZybq63q+uDRXrI8if/W/TBlCCMvBeq0mqISG/2sbtkuRDyGEEEJYiWirsLjQwV4sxrWFzjvvvPZ/rl9NmS+++EMEsAjpWoUlEdHEc14ITF0ntBWLJXYRTSyY78tMc4VwYmpRhJMQwnKzKqbqqCjKDHUhHYb/MeqYuLi4uLi4uLiV6HSqiQdlmbAYrt8e6rfJ/u3f/q1ZoVjjpPwW29WgFue6poL8/Oc/v94xC+mKUfuWwhFO3LPn7Hf//ufiTEOvsBbTifcGG2zQnXPOOS1/jjpmJseaqKbdj9ofFxcXt1Qu81emgQJfLoQQQghhJVKd4KWaltxvF9lmEVCIx1JQ17nVrW7V/l911VXt/1rEc+2n63yfs09Hl/i12LAIsk7JBRdc0H5P2qb22WTnLPY0tBBCmIlVY3EyX3U9Li4uLi4uLm6tOu0kHcxR++brhm0wlL9OscX3+/sX29X1uVve8pZtSshll112vWMW2vWvudTOGh+erbT2ey5t4v451h4R3lytQIZuurR54AMf2CxPzjjjjJH7Z3Ismog8Ptgwan9cXFzcUrlVYXHCpNB8zBBCCCGEcH10inUuF2uNk6GVgOtBx5u1S309BcNjFwMN2LoOa5db3/rW3c9+9rP2ey3iuRIfrPexEBAhPDduIfA8hv9r27MRd9N15oJpWPKbqTohhLCcrArhpOY2rm9UpRNCCCGEMA6iCSFhqQaZaqqIz+MST250oxu130tFX5whKtziFreY15dbVjoliElrzEWc6p9jQJIYYUHhPiV4DN1MCLvcENYyrIKuueaa68KaTZhFLfpL7AkhhOVkVQgnKuilmrcbQgghhLCa0AnWTjKyv1iM6uz6Mgt3k5vcZMpnaak43fWud22fI77yyivb77WGwUPuj3/845TP5Ayfn99lOVSU8NEXQkaJITMxPO8BD3hA98tf/rL713/91/Z7kjCtcSJvx+IkhLDcrIo1TtZXh1H+cXFxcXFxcXF9V4zatxCuwtbZLj/rT+gQL+UXdfqu4nS7292uWWVcfPHFf3XMQrhi1L6lcO6N5Ub/a0bzcYQLYsSo8BbjPjfbbLM2leq3v/3tyP3TOcJJrfEyan9cXFzcUrlVb8bhJkIIIYQQ1lfKcmAprHP71gK/+tWv2hoUpmIUy9Eu23DDDdt0IcLJWsRz/Yd/+Ifuz3/+85TP/Jn0Oc10vPVSfvOb33SXXnppd9FFF3UXXnhhex62Tz755CauuYdJMR1suSyaQgihz7p/+7d/W7HKg0J6aM43yi+EEEIIYX3lkksu6R7+8Id3H/zgB7vttttuynfx0SF+/OMf351wwgndjjvuOOW7dFSbUKfdfd/sZjfrPve5z03tXThe/epXd+94xzu6yy+/vAkAS421SHbbbbdmufGud71rTgJZv/28//77d6eddlr35S9/ubvxjW/c/CbBuiPnnXde973vfa9Nj2JN8sMf/rBNlxrCWsYnow866KDuYQ972JTv7BDnXXfdta1hc8wxx0z5hhDC8rCiLU4ikIQQQgghTI8pFzrTS2Fx0sdUHczFkmAhqHaizjmLE9OG1iLukyMk1AKx80E+YckxW1zz29/+dnfggQd2O+ywQ3O7775797rXva4JKMSkPfbYowlMhxxySBNmXv7yl3f77bdfd9hhh3Uf+chHuoc85CFToc0eghErG59PDiGE5WbVTNVRWSBiSgghhBDCX6g20lJjaoavndRUiuWKB+52t7u1hWpNHVpr+FoSqwvp/fvf/37Kd+5ssMEG3dVXX9395Cc/mfL5C9Lw3HPP7Y499tju+c9/frfLLru0NWR23nnnZlnkizwPfehDu+OOO677/ve/351yyindUUcd1R1wwAHdS17yku55z3te98pXvrJ7xSte0YSWffbZp9tyyy3n9MUnlkSmoVnjJIQQlptVIZwsZ0UcQgghhLC+U22xfpvMNI2NN964u8ENbtB+L+fg1t3vfve2+OhVV1015bO22GijjdqUmLkKJ/1n84hHPKJZcTzxiU/s3vOe9zQR5G1ve1u39957d9tvv32zJnnBC17QnX766d1tbnOb7qlPfWr3vve9r4kkpve8+c1vboLKJptssqifCa681hdd0icIISwX6/71X/81JVAIIYQQwirl/PPP7/bcc8+2xsm222475buw6LDWdJHqhFvfxFdPTjrppCWfJjTEVJIHP/jBTQh47GMfO+W7MLCcOOKII7of/OAHy7LGCV772te25/upT32q22abbaZ85w4R5A1veENbr8RnrAkgf/d3f9fEkAc96EHdVltt1Rb9ncsaKAsFIczaLtZGYcWCfv4LIYSlZNVM1QkhhBBCCH+NzuRij8SP6qxeccUVbUHW5RZNUB38a665pv1fSFZCR10aW1OGmLAQmHrzta99rfvkJz/ZffSjH21WJ1/60pe697///U0Qu9Od7rSsoglM1bF+z9/8zd9M+YQQwvIR4SSEEEIIYQ2wWOLJuHBZK6yUTq0FasVloYSFlUZNV1nI9T4IQltssUUTSTbddNMp37+wFILcdFgc1iK2LGFCCGG5+b+qUIyLi4uLi4uLi1t9zgKao/wXw8H/f/3Xf21fW7FWxvCY5XD//b//927DDTfs/vjHP47cPx9XFiej9i2FMw3JNB2fnN56661HHjMXN1O+Kfp+S5nX5C/u7//+70fuj4uLi1tKF4uTEEIIIYRVTDXqFotRU1V+8YtftC+s+OLKUjPqfgkmP/7xj69nAVPHLXTaLHR4ff793/+9++lPf9p997vf7T772c92z3nOc9raLTj44IOvExEWgrlMQVqKaUt1f0QTVif9Z7oU1w8hhFGs+81vfrN4pX8IIYQQQlhUfBb2MY95TLNKuM997jPlu7hYH+OZz3xm9/a3v717yEMe0vyqwztJ59Y5dXx/e7boWPt8rs/hWrz1c5/7XHeXu9yl7esLDPPpcB9yyCHd4Ycf3l122WXNwmY6ZnsP1ivxBSCfBCZCnXfeed2ll17aPjkMU1R++ctftk8977rrrt0LX/jC66bTzCWdVhN1f9LbvR922GHdox71qKm9IYSwPMTiJIQQQghhDdEXDBaLX/3qV23djVvd6lZTPv8pTkzaoe8fP925poj84Q9/6H7+859355xzTnf88cd3b3rTm1qH2ud1TR2ywGlfNKn4TBqnIaPSk98o/1HXEm9fPrIQ66GHHto9+9nPblYkO+64Y/esZz2rfQqYaCI973a3u3WPe9zjmljz6U9/uvvWt77V9o9ag2R9wOKwfZYib4cQwihicRJCCCGEsIoZWpyUaLCYHH300d1b3vKW7utf/3r7ss6kVAdYx/j3v/99+/3nP/+5u+iii5o1xv/6X/+ru+SSS7qLL774ui/lECAIKKxMTGkhNLjfvfbaq31C9wY3uEE7zjH1pZ+FSIs3v/nNTaSx1shMVh8sRVi+eCann356sygRf/co/qY33ete92rxvsMd7tA++cuqhABl30wsxbNdbuoeiUm77LJLE46IY/19IYSw1EQ4CSGEEEJYxZRwcswxx3TbbbfdknQuWUNYf+MNb3hD97CHPaz73e9+1z4fS/AggBA/CAa+ciM+1iCpr6QQPf70pz+1fcQEx/ld61n81//6X7v/8T/+R7fBBhu0/+6F341udKPu5je/ebflllu2r8H42srf/u3fdre4xS2mYvUXKg0WIi18pvdJT3pS+2zv/e9//ynfv2A9EpYhl19+eXsWF154YROTOPcjrttss00TSixia7rPqDhPh/vA+iAa1DMjnPhs8hFHHBHhJISw7Kz79a9/HeEkhBBCCGGVouP++Mc/vlmcbLvttkvSubRw5zOe8Yzui1/8YrfRRhs1yxGWHvgv/+W/NHGk1utgSUFAIIL4nC5BwTYR4aY3vWm38cYbN+uRG97wht1mm23WxBBCCXGhrEimo0QF1H2P8psrxJ373e9+bSHcI488st3rN7/5zWZtc8UVV7T1SViU3OY2t2lfvmGV4us3frvPcQzjWL9HxXe6fWsV1kbSnXVThJMQwnIT4SSEEEIIYYXS7yiO6zSaQvKEJzyhO/bYY9s0kKXqXLIuOe2005pwQOggmNS0EyKI6TKEEl+CYR1CHOH6U1JqSs1scF9c/xy/Me5+FyotvvOd73R77713W7BVeAQdnz/eZJNNurve9a5tbZWtttqq3f90zBTfPv24T3LeWuGkk07q9tlnn+6EE05oAgoW6nmGEMKkrPvVr34V4SSEEEIIYYViSguxgBgxCsIJixNTSe5xj3tM+S4uK6kDu1RxseYKCxsWMyxjbn/728/KIibMDtO4TNsiwJn2RAy84x3v2NJ8EoEthBAWgwgnIYQQQggrmAMPPLBZabzqVa+a8rk+FlTdY4892pdm7nSnO035hrCysIaNtW5Ma7IGjilQV155ZffTn/60rZHz4x//uPvhD3/Y1sjx1SZTuI466qj2paEQQlhuIpyEEEIIIaxgHvvYx3Zf+cpXuje+8Y1tXRCLsFpMVQfU1Bedz8997nNtAVPrcOh4ssAYWmGwzKgpH/Uf1iapNUq4Oq5/DPrh9cMZdR304zAMCywLTPGB/Vx9Ece+/vn8xLGuZ/8oKv6o8CqMflz699pn+HscwnLsqOPtq/2jsE/8++vCwPF+u9eK61wQTqVV/a77rd91bdvyk+vxs239Gv/rGNtc/3j76hp1Xu3jhOd407n8huMqXSpcAgo/i/7e9ra3bQvomtZl6pOFeOVvOGc+aRJCCPMlwkkIIYQQwgrmnHPO6d7xjne09TV0uHUiCQ6sUPwmoPgCifU2dDqhk4rZdsKHQkV/u+j/Fody0zFqP7+K3zCM2p7N9Yf090ub/nVGTfXQeYd98xErKhxUvEpkKGyXqzg5pn9u/R4V18WgrtO/pu1+vCFdpksf+7gSsxzH+U3oqzVtLBBsDRhfS+IsDGzak0WAbY+i4iZOc30+IYSwEKy7thL+65onhBBCCCEsO/0Oo6kN9bsWWsXpp5/ePfvZz+7e9a53tUU0Hden3+Gs7WFHuLZH+cHxk6LTOwr3MOyco+83av8kTBff/n2hfs/lHof0BaEh5ecY2+WGjPKbD+5v3D0P/TFMh+H59buOW4h0G1JpMCp+IYSwHEQ4CSGEEEJYxXz+859v03iOO+649mngEEIIISwsCy8RhxBCCCGEBWWcdQL+9V//tY3Mj7PwCItHPZdRrs84/7XEWr63EEJY9y//8i8p5UIIIYQQVimHHHJId/bZZ3cnnHDC9RbTRKY6hBBCCPMnFichhBBCCKuYCy64oLvhDW/4V18giWgSQgghLAwRTkIIIYQQVik+HfvHP/6x23LLLad8YmUSQgghLDTrfvGLX2SqTgghhBDCKuSaa67pnvSkJ3V77bVXt++++075hhBCCGEhicVJCCGEEMIq5aqrrup+97vfdVtsscWUTwghhBAWmggnIYQQQgirlDPPPLNN19lqq62mfEIIIYSw0Ky75pprMlUnhBBCCGGV4fPDe+65Z7fpppt2hx122JRvCCGEEBaaCCchhBBCCKsQlibf/OY3u80337y7xS1uMeUbQgghhIUmwkkIIYQQQgghhBDCGLLGSQghhBBCCCGEEMIY1v385z+PxUkIIYQQQgghhBDCCGJxEkIIIYQQQgghhDCGdVdffXUsTkIIIYQQQgghhBBGEIuTEEIIIYQQQgghhDFEOAkhhBBCCCGEEEIYw7qf/exnmaoTQgghhBBCCCGEMIJYnIQQQgghhBBCCCGMIcJJCCGEEEIIIYQQwhjW/fSnP81UnRBCCCGEEEIIIYQRxOIkhBBCCCGEEEIIYQwRTkIIIYQQQgghhBDGsO6qq67KVJ0QQgghhBBCCCGEEcTiJIQQQgghhBBCCGEMEU5CCCGEEEIIIYQQxhDhJIQQQgghhBBCCGEM66688sqscRJCCCGEEEIIIYQwglichBBCCCGEEEIIIYxh3RVXXBGLkxBCCCGEEEIIIYQRxOIkhBBCCCGEEEIIYQwRTkIIIYQQQgghhBDGsO4nP/lJpuqEEEIIIYQQQgghjCAWJyGEEEIIIYQQQghjiHASQgghhBBCCCGEMIZ1P/7xjzNVJ4QQQgghhBBCCGEEsTgJIYQQQgghhBBCGEOEkxBCCCGEEEIIIYQxrPvRj36UqTohhBBCCCGEEEIII4jFSQghhBBCCCGEEMIYIpyEEEIIIYQQQgghjCHCSQghhBBCCCGEEMIY1v3whz/MGichhBBCCCGEEEIII4jFSQghhBBCCCGEEMIY1l1++eWxOAkhhBBCCCGEEEIYQSxOQgghhBBCCCGEEMbwf/2f//N/Oi6EEEIIIYQQQgghXJ/rLE5KQBm6EEIIIYQQQgghhPWVdZdddtm06si6deumtkIIIYQQQgghhBDWL7LGSQghhBBCCCGEEMIY1l166aWxOAkhhBBCCCGEEEIYwbQWJxFNQgghhBBCCCGEsD6TqTohhBBCCCGEEEIIYxgrnMTaJIQQQgghhBBCCOs76y655JKRa5xEOAkhhBBCCCGEEML6zl9ZnBBMIpqEEEIIIYQQQggh9ISTCCYhhBBCCCGEEEII12fsVJ0QQgghhBBCCCGE9Z18VSeEEEIIIYQQQghhBP/n//yfbt3FF18ci5MQQgghhBBCCCGEAYSTWJyEEEIIIYQQQgghjCHCSQghhBBCCCGEEMIYIpyEEEIIIYQQQgghjGHdRRddlDVOQgghhBBCCCGEEAZkjZMQQgghhBBCCCGEaVh34YUXxuIkhBBCCCGEEEIIYUAsTkIIIYQQQgghhBCmIcJJCCGEEEIIIYQQwhginIQQQgghhBBCCCGMYd0FF1yQNU5CCCGEEEIIIYQQBmSNkxBCCCGEEEIIIYRpiHASQgghhBBCCCGEMIZ1559/fqbqhBBCCCGEEEIIIQzIVJ0QQgghhBBCCCGEaVh33nnnxeIkhBBCCCGEEEIIYUAsTkIIIYQQQgghhBCmIcJJCCGEEEIIIYQQwhginIQQQgghhBBCCCGMYd33v//9rHESQgghhBBCCCGEMCBrnIQQQgghhBBCCCFMw7rvfe97sTgJIYQQQgghhBBCGBCLkxBCCCGEEEIIIYRpiHASQgghhBBCCCGEMIYIJyGEEEIIIYQQQghjWPfd7343a5yEEEIIIYQQQgghDMgaJyGEEEIIIYQQQgjTEOEkhBBCCCGEEEIIYQzrvvOd72SqTgghhBBCCCGEEMKATNUJIYQQQgghhBBCmIYIJyGEEEIIIYQQQghjWPftb387U3VCCCGEEEIIIYQQBmSqTgghhBBCCCGEEMI0RDgJIYQQQgghhBBCGMO6c889N1N1QgghhBBCCCGEEAZkqk4IIYQQQgghhBDCNKw755xzYnESQgghhBBCCCGEMCAWJyGEEEIIIYQQQgjTEOEkhBBCCCGEEEIIYQwRTkIIIYQQQgghhBDGsO7ss8/OGichhBBCCCGEEEIIA7LGSQghhBBCCCGEEMI0RDgJIYQQQgghhBBCGMO6b33rW5mqE0IIIYQQQgghhDAgU3VCCCGEEEIIIYQQpiHCSQghhBBCCCGEEMIY1p111lmZqhNCCCGEEEIIIYQwIFN1QgghhBBCCCGEEKYhwkkIIYQQQgghhBDCGNZ985vfzFSdEEIIIYQQQgghhAGZqhNCCCGEEEIIIYQwDRFOQgghhBBCCCGEEMYQ4SSEEEIIIYQQQghhDOvOPPPMrHESQgghhBBCCCGEMCBrnIQQQgghhBBCCCFMw7ozzjgjFichhBBCCCGEEEIIA2JxEkIIIYQQQgghhDCWrvv/AXq2SiwWCz7mAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "15bfad65",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58ba57",
   "metadata": {},
   "source": [
    "Directory loader is use when we have to load the multiple filed from any folder "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "60aed600",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310439d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 0, 'page_label': 'Cover'}\n",
      "[ 1 ]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 1, 'page_label': 'FM1'}\n",
      "Building Machine Learning \n",
      "Systems with Python \n",
      "Second Edition\n",
      "Get more from your data through creating practical \n",
      "machine learning systems with Python\n",
      "Luis Pedro Coelho\n",
      "Willi Richert\n",
      "BIRMINGHAM - MUMBAI\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 2, 'page_label': 'FM2'}\n",
      "Building Machine Learning Systems with Python \n",
      "Second Edition\n",
      "Copyright  2015 Packt Publishing\n",
      "All rights reserved. No part of this book may be reproduced, stored in a retrieval \n",
      "system, or transmitted in any form or by any means, without the prior written \n",
      "permission of the publisher, except in the case of brief quotations embedded in \n",
      "critical articles or reviews.\n",
      "Every effort has been made in the preparation of this book to ensure the accuracy \n",
      "of the information presented. However, the information contained in this book is \n",
      "sold without warranty, either express or implied. Neither the authors, nor Packt \n",
      "Publishing, and its dealers and distributors will be held liable for any damages \n",
      "caused or alleged to be caused directly or indirectly by this book.\n",
      "Packt Publishing has endeavored to provide trademark information about all of the \n",
      "companies and products mentioned in this book by the appropriate use of capitals. \n",
      "However, Packt Publishing cannot guarantee the accuracy of this information.\n",
      "First published: July 2013\n",
      "Second edition: March 2015\n",
      "Production reference: 1230315\n",
      "Published by Packt Publishing Ltd.\n",
      "Livery Place\n",
      "35 Livery Street\n",
      "Birmingham B3 2PB, UK.\n",
      "ISBN 978-1-78439-277-2\n",
      "www.packtpub.com\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 3, 'page_label': 'FM3'}\n",
      "Credits\n",
      "Authors\n",
      "Luis Pedro Coelho\n",
      "Willi Richert\n",
      "Reviewers\n",
      "Matthieu Brucher\n",
      "Maurice HT Ling\n",
      "Radim ehek\n",
      "Commissioning Editor\n",
      "Kartikey Pandey\n",
      "Acquisition Editors\n",
      "Greg Wild\n",
      "Richard Harvey\n",
      "Kartikey Pandey\n",
      "Content Development Editor\n",
      "Arun Nadar\n",
      "Technical Editor\n",
      "Pankaj Kadam\n",
      "Copy Editors\n",
      "Relin Hedly\n",
      "Sameen Siddiqui\n",
      "Laxmi Subramanian\n",
      "Project Coordinator\n",
      "Nikhil Nair\n",
      "Proofreaders\n",
      "Simran Bhogal\n",
      "Lawrence A. Herman\n",
      "Linda Morris\n",
      "Paul Hindle\n",
      "Indexer\n",
      "Hemangini Bari\n",
      "Graphics\n",
      "Sheetal Aute\n",
      "Abhinash Sahu\n",
      "Production Coordinator\n",
      "Arvindkumar Gupta\n",
      "Cover Work\n",
      "Arvindkumar Gupta\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 4, 'page_label': 'FM4'}\n",
      "About the Authors\n",
      "Luis Pedro Coelho is a computational biologist: someone who uses computers \n",
      "as a tool to understand biological systems. In particular, Luis analyzes DNA \n",
      "from microbial communities to characterize their behavior. Luis has also worked \n",
      "extensively in bioimage informaticsthe application of machine learning techniques \n",
      "for the analysis of images of biological specimens. His main focus is on the processing \n",
      "and integration of large-scale datasets.\n",
      "Luis has a PhD from Carnegie Mellon University, one of the leading universities \n",
      "in the world in the area of machine learning. He is the author of several scientific \n",
      "publications.\n",
      "Luis started developing open source software in 1998 as a way to apply real code to \n",
      "what he was learning in his computer science courses at the Technical University of \n",
      "Lisbon. In 2004, he started developing in Python and has contributed to several open \n",
      "source libraries in this language. He is the lead developer on the popular computer \n",
      "vision package for Python and mahotas, as well as the contributor of several machine \n",
      "learning codes.\n",
      "Luis currently divides his time between Luxembourg and Heidelberg.\n",
      "I thank my wife, Rita, for all her love and support and my daughter, \n",
      "Anna, for being the best thing ever.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 5, 'page_label': 'FM5'}\n",
      "Willi Richert has a PhD in machine learning/robotics, where he used \n",
      "reinforcement learning, hidden Markov models, and Bayesian networks to let \n",
      "heterogeneous robots learn by imitation. Currently, he works for Microsoft in the \n",
      "Core Relevance Team of Bing, where he is involved in a variety of ML areas such  \n",
      "as active learning, statistical machine translation, and growing decision trees.\n",
      "This book would not have been possible without the support of \n",
      "my wife, Natalie, and my sons, Linus and Moritz. I am especially \n",
      "grateful for the many fruitful discussions with my current or \n",
      "previous managers, Andreas Bode, Clemens Marschner, Hongyan \n",
      "Zhou, and Eric Crestan, as well as my colleagues and friends, \n",
      "Tomasz Marciniak, Cristian Eigel, Oliver Niehoerster, and Philipp \n",
      "Adelt. The interesting ideas are most likely from them; the bugs \n",
      "belong to me.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 6, 'page_label': 'FM6'}\n",
      "About the Reviewers\n",
      "Matthieu Brucher holds an engineering degree from the Ecole Suprieure \n",
      "d'Electricit (Information, Signals, Measures), France and has a PhD in unsupervised \n",
      "manifold learning from the Universit de Strasbourg, France. He currently holds \n",
      "an HPC software developer position in an oil company and is working on the next \n",
      "generation reservoir simulation.\n",
      "Maurice HT Ling has been programming in Python since 2003. Having completed \n",
      "his PhD in Bioinformatics and BSc (Hons.) in Molecular and Cell Biology from The \n",
      "University of Melbourne, he is currently a Research Fellow at Nanyang Technological \n",
      "University, Singapore, and an Honorary Fellow at The University of Melbourne, \n",
      "Australia. Maurice is the Chief Editor for Computational and Mathematical Biology, and \n",
      "co-editor for The Python Papers. Recently, Maurice cofounded the first synthetic biology \n",
      "start-up in Singapore, AdvanceSyn Pte. Ltd., as the Director and Chief Technology \n",
      "Officer. His research interests lies in lifebiological life, artificial life, and artificial \n",
      "intelligenceusing computer science and statistics as tools to understand life and \n",
      "its numerous aspects. In his free time, Maurice likes to read, enjoy a cup of coffee, \n",
      "write his personal journal, or philosophize on various aspects of life. His website and \n",
      "LinkedIn profile are http://maurice.vodien.com and http://www.linkedin.com/\n",
      "in/mauriceling, respectively.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 7, 'page_label': 'FM7'}\n",
      "Radim ehek is a tech geek and developer at heart. He founded and led the \n",
      "research department at Seznam.cz, a major search engine company in central Europe. \n",
      "After finishing his PhD, he decided to move on and spread the machine learning \n",
      "love, starting his own privately owned R&D company, RaRe Consulting Ltd. RaRe \n",
      "specializes in made-to-measure data mining solutions, delivering cutting-edge \n",
      "systems for clients ranging from large multinationals to nascent start-ups.\n",
      "Radim is also the author of a number of popular open source projects, including \n",
      "gensim and smart_open.\n",
      "A big fan of experiencing different cultures, Radim has lived around the globe with his \n",
      "wife for the past decade, with his next steps leading to South Korea. No matter where \n",
      "he stays, Radim and his team always try to evangelize data-driven solutions and help \n",
      "companies worldwide make the most of their machine learning opportunities.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 8, 'page_label': 'FM8'}\n",
      "www.PacktPub.com\n",
      "Support files, eBooks, discount offers, and more\n",
      "For support files and downloads related to your book, please visit www.PacktPub.com.\n",
      "Did you know that Packt offers eBook versions of every book published, with PDF \n",
      "and ePub files available? You can upgrade to the eBook version at www.PacktPub.com \n",
      "and as a print book customer, you are entitled to a discount on the eBook copy. Get in \n",
      "touch with us at service@packtpub.com for more details.\n",
      "At www.PacktPub.com, you can also read a collection of free technical articles, sign \n",
      "up for a range of free newsletters and receive exclusive discounts and offers on Packt \n",
      "books and eBooks.\n",
      "TM\n",
      "https://www2.packtpub.com/books/subscription/packtlib\n",
      "Do you need instant solutions to your IT questions? PacktLib is Packt's online digital \n",
      "book library. Here, you can search, access, and read Packt's entire library of books.\n",
      "Why subscribe?\n",
      " Fully searchable across every book published by Packt\n",
      " Copy and paste, print, and bookmark content\n",
      " On demand and accessible via a web browser\n",
      "Free access for Packt account holders\n",
      "If you have an account with Packt at www.PacktPub.com, you can use this to access \n",
      "PacktLib today and view 9 entirely free books. Simply use your login credentials for \n",
      "immediate access.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 9, 'page_label': 'i'}\n",
      "[ i ]\n",
      "Table of Contents\n",
      "Preface vii\n",
      "Chapter 1: Getting Started with Python Machine Learning 1\n",
      "Machine learning and Python  a dream team 2\n",
      "What the book will teach you (and what it will not) 3\n",
      "What to do when you are stuck 4\n",
      "Getting started 5\n",
      "Introduction to NumPy, SciPy, and matplotlib 6\n",
      "Installing Python 6\n",
      "Chewing data efficiently with NumPy and intelligently with SciPy 6\n",
      "Learning NumPy 7\n",
      "Indexing 9\n",
      "Handling nonexisting values 10\n",
      "Comparing the runtime 11\n",
      "Learning SciPy 12\n",
      "Our first (tiny) application of machine learning 13\n",
      "Reading in the data 14\n",
      "Preprocessing and cleaning the data 15\n",
      "Choosing the right model and learning algorithm 17\n",
      "Before building our first model 18\n",
      "Starting with a simple straight line 18\n",
      "Towards some advanced stuff 20\n",
      "Stepping back to go forward  another look at our data 22\n",
      "Training and testing 26\n",
      "Answering our initial question 27\n",
      "Summary 28\n",
      "Chapter 2: Classifying with Real-world Examples 29\n",
      "The Iris dataset 30\n",
      "Visualization is a good first step 30\n",
      "Building our first classification model 32\n",
      "Evaluation  holding out data and cross-validation 36\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 10, 'page_label': 'ii'}\n",
      "Table of Contents\n",
      "[  ii ]\n",
      "Building more complex classifiers 39\n",
      "A more complex dataset and a more complex classifier 41\n",
      "Learning about the Seeds dataset 41\n",
      "Features and feature engineering 42\n",
      "Nearest neighbor classification 43\n",
      "Classifying with scikit-learn 43\n",
      "Looking at the decision boundaries 45\n",
      "Binary and multiclass classification 47\n",
      "Summary 49\n",
      "Chapter 3: Clustering  Finding Related Posts 51\n",
      "Measuring the relatedness of posts 52\n",
      "How not to do it 52\n",
      "How to do it 53\n",
      "Preprocessing  similarity measured as a similar  \n",
      "number of common words 54\n",
      "Converting raw text into a bag of words 54\n",
      "Counting words 55\n",
      "Normalizing word count vectors 58\n",
      "Removing less important words 59\n",
      "Stemming 60\n",
      "Stop words on steroids 63\n",
      "Our achievements and goals 65\n",
      "Clustering 66\n",
      "K-means 66\n",
      "Getting test data to evaluate our ideas on 70\n",
      "Clustering posts 72\n",
      "Solving our initial challenge 73\n",
      "Another look at noise 75\n",
      "Tweaking the parameters 76\n",
      "Summary 77\n",
      "Chapter 4: Topic Modeling 79\n",
      "Latent Dirichlet allocation 80\n",
      "Building a topic model 81\n",
      "Comparing documents by topics 86\n",
      "Modeling the whole of Wikipedia 89\n",
      "Choosing the number of topics 92\n",
      "Summary 94\n",
      "Chapter 5: Classification  Detecting Poor Answers 95\n",
      "Sketching our roadmap 96\n",
      "Learning to classify classy answers 96\n",
      "Tuning the instance 96\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 11, 'page_label': 'iii'}\n",
      "Table of Contents\n",
      "[  iii ]\n",
      "Tuning the classifier 96\n",
      "Fetching the data 97\n",
      "Slimming the data down to chewable chunks 98\n",
      "Preselection and processing of attributes 98\n",
      "Defining what is a good answer 100\n",
      "Creating our first classifier 100\n",
      "Starting with kNN 100\n",
      "Engineering the features 101\n",
      "Training the classifier 103\n",
      "Measuring the classifier's performance 103\n",
      "Designing more features 104\n",
      "Deciding how to improve 107\n",
      "Bias-variance and their tradeoff 108\n",
      "Fixing high bias 108\n",
      "Fixing high variance 109\n",
      "High bias or low bias 109\n",
      "Using logistic regression 112\n",
      "A bit of math with a small example 112\n",
      "Applying logistic regression to our post classification problem 114\n",
      "Looking behind accuracy  precision and recall 116\n",
      "Slimming the classifier 120\n",
      "Ship it! 121\n",
      "Summary 121\n",
      "Chapter 6: Classification II  Sentiment Analysis 123\n",
      "Sketching our roadmap 123\n",
      "Fetching the Twitter data 124\n",
      "Introducing the Nave Bayes classifier 124\n",
      "Getting to know the Bayes' theorem 125\n",
      "Being nave 126\n",
      "Using Nave Bayes to classify 127\n",
      "Accounting for unseen words and other oddities 131\n",
      "Accounting for arithmetic underflows 132\n",
      "Creating our first classifier and tuning it 134\n",
      "Solving an easy problem first 135\n",
      "Using all classes 138\n",
      "Tuning the classifier's parameters 141\n",
      "Cleaning tweets 146\n",
      "Taking the word types into account 148\n",
      "Determining the word types 148\n",
      "Successfully cheating using SentiWordNet 150\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 12, 'page_label': 'iv'}\n",
      "Table of Contents\n",
      "[  iv ]\n",
      "Our first estimator 152\n",
      "Putting everything together 155\n",
      "Summary 156\n",
      "Chapter 7: Regression 157\n",
      "Predicting house prices with regression 157\n",
      "Multidimensional regression 161\n",
      "Cross-validation for regression 162\n",
      "Penalized or regularized regression 163\n",
      "L1 and L2 penalties 164\n",
      "Using Lasso or ElasticNet in scikit-learn 165\n",
      "Visualizing the Lasso path 166\n",
      "P-greater-than-N scenarios 167\n",
      "An example based on text documents 168\n",
      "Setting hyperparameters in a principled way 170\n",
      "Summary 174\n",
      "Chapter 8: Recommendations 175\n",
      "Rating predictions and recommendations 175\n",
      "Splitting into training and testing 177\n",
      "Normalizing the training data 178\n",
      "A neighborhood approach to recommendations 180\n",
      "A regression approach to recommendations 184\n",
      "Combining multiple methods 186\n",
      "Basket analysis 188\n",
      "Obtaining useful predictions 190\n",
      "Analyzing supermarket shopping baskets 190\n",
      "Association rule mining 194\n",
      "More advanced basket analysis 196\n",
      "Summary 197\n",
      "Chapter 9: Classification  Music Genre Classification 199\n",
      "Sketching our roadmap 199\n",
      "Fetching the music data 200\n",
      "Converting into a WAV format 200\n",
      "Looking at music 201\n",
      "Decomposing music into sine wave components 203\n",
      "Using FFT to build our first classifier 205\n",
      "Increasing experimentation agility 205\n",
      "Training the classifier 207\n",
      "Using a confusion matrix to measure accuracy in  \n",
      "multiclass problems 207\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 13, 'page_label': 'v'}\n",
      "Table of Contents\n",
      "[  v ]\n",
      "An alternative way to measure classifier performance  \n",
      "using receiver-operator characteristics 210\n",
      "Improving classification performance with Mel  \n",
      "Frequency Cepstral Coefficients 214\n",
      "Summary 218\n",
      "Chapter 10: Computer Vision 219\n",
      "Introducing image processing 219\n",
      "Loading and displaying images 220\n",
      "Thresholding 222\n",
      "Gaussian blurring 223\n",
      "Putting the center in focus 225\n",
      "Basic image classification 228\n",
      "Computing features from images 229\n",
      "Writing your own features 230\n",
      "Using features to find similar images 232\n",
      "Classifying a harder dataset 234\n",
      "Local feature representations 235\n",
      "Summary 239\n",
      "Chapter 11: Dimensionality Reduction 241\n",
      "Sketching our roadmap 242\n",
      "Selecting features 242\n",
      "Detecting redundant features using filters 242\n",
      "Correlation 243\n",
      "Mutual information 246\n",
      "Asking the model about the features using wrappers 251\n",
      "Other feature selection methods 253\n",
      "Feature extraction 254\n",
      "About principal component analysis 254\n",
      "Sketching PCA 255\n",
      "Applying PCA 255\n",
      "Limitations of PCA and how LDA can help 257\n",
      "Multidimensional scaling 258\n",
      "Summary 262\n",
      "Chapter 12: Bigger Data 263\n",
      "Learning about big data 264\n",
      "Using jug to break up your pipeline into tasks 264\n",
      "An introduction to tasks in jug 265\n",
      "Looking under the hood 268\n",
      "Using jug for data analysis 269\n",
      "Reusing partial results 272\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 14, 'page_label': 'vi'}\n",
      "Table of Contents\n",
      "[  vi ]\n",
      "Using Amazon Web Services 274\n",
      "Creating your first virtual machines 276\n",
      "Installing Python packages on Amazon Linux 282\n",
      "Running jug on our cloud machine 283\n",
      "Automating the generation of clusters with StarCluster 284\n",
      "Summary 288\n",
      "Appendix: Where to Learn More Machine Learning 291\n",
      "Online courses 291\n",
      "Books 291\n",
      "Question and answer sites 292\n",
      "Blogs 292\n",
      "Data sources 293\n",
      "Getting competitive 293\n",
      "All that was left out 293\n",
      "Summary 294\n",
      "Index 295\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 15, 'page_label': 'vii'}\n",
      "[ vii ]\n",
      "Preface\n",
      "One could argue that it is a fortunate coincidence that you are holding this book in \n",
      "your hands (or have it on your eBook reader). After all, there are millions of books \n",
      "printed every year, which are read by millions of readers. And then there is this book \n",
      "read by you. One could also argue that a couple of machine learning algorithms \n",
      "played their role in leading you to this bookor this book to you. And we, the \n",
      "authors, are happy that you want to understand more about the hows and whys.\n",
      "Most of the book will cover the how. How has data to be processed so that machine \n",
      "learning algorithms can make the most out of it? How should one choose the right \n",
      "algorithm for a problem at hand?\n",
      "Occasionally, we will also cover the why. Why is it important to measure correctly? \n",
      "Why does one algorithm outperform another one in a given scenario?\n",
      "We know that there is much more to learn to be an expert in the field. After all, we \n",
      "only covered some hows and just a tiny fraction of the whys. But in the end, we hope \n",
      "that this mixture will help you to get up and running as quickly as possible.\n",
      "What this book covers\n",
      "Chapter 1, Getting Started with Python Machine Learning, introduces the basic idea of \n",
      "machine learning with a very simple example. Despite its simplicity, it will challenge \n",
      "us with the risk of overfitting.\n",
      "Chapter 2, Classifying with Real-world Examples, uses real data to learn about \n",
      "classification, whereby we train a computer to be able to distinguish different  \n",
      "classes of flowers.\n",
      "Chapter 3, Clustering  Finding Related Posts, teaches how powerful the bag of \n",
      "words approach is, when we apply it to finding similar posts without really \n",
      "\"understanding\" them.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 16, 'page_label': 'viii'}\n",
      "Preface\n",
      "[  viii ]\n",
      "Chapter 4, Topic Modeling, moves beyond assigning each post to a single cluster and \n",
      "assigns them to several topics as a real text can deal with multiple topics.\n",
      "Chapter 5, Classification  Detecting Poor Answers, teaches how to use the bias-variance \n",
      "trade-off to debug machine learning models though this chapter is mainly on using a \n",
      "logistic regression to find whether a user's answer to a question is good or bad.\n",
      "Chapter 6, Classification II  Sentiment Analysis, explains how Nave Bayes works, and \n",
      "how to use it to classify tweets to see whether they are positive or negative.\n",
      "Chapter 7, Regression, explains how to use the classical topic, regression, in handling \n",
      "data, which is still relevant today. You will also learn about advanced regression \n",
      "techniques such as the Lasso and ElasticNets.\n",
      "Chapter 8, Recommendations, builds recommendation systems based on costumer \n",
      "product ratings. We will also see how to build recommendations just from shopping \n",
      "data without the need for ratings data (which users do not always provide).\n",
      "Chapter 9, Classification  Music Genre Classification, makes us pretend that someone \n",
      "has scrambled our huge music collection, and our only hope to create order is to let a \n",
      "machine learner classify our songs. It will turn out that it is sometimes better to trust \n",
      "someone else's expertise than creating features ourselves.\n",
      "Chapter 10, Computer Vision, teaches how to apply classification in the specific context \n",
      "of handling images by extracting features from data. We will also see how these \n",
      "methods can be adapted to find similar images in a collection.\n",
      "Chapter 11, Dimensionality Reduction, teaches us what other methods exist that can help \n",
      "us in downsizing data so that it is chewable by our machine learning algorithms.\n",
      "Chapter 12, Bigger Data, explores some approaches to deal with larger data by taking \n",
      "advantage of multiple cores or computing clusters. We also have an introduction to \n",
      "using cloud computing (using Amazon Web Services as our cloud provider).\n",
      "Appendix, Where to Learn More Machine Learning, lists many wonderful resources \n",
      "available to learn more about machine learning.\n",
      "What you need for this book\n",
      "This book assumes you know Python and how to install a library using easy_install or \n",
      "pip. We do not rely on any advanced mathematics such as calculus or matrix algebra.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 17, 'page_label': 'ix'}\n",
      "Preface\n",
      "[  ix ]\n",
      "We are using the following versions throughout the book, but you should be fine \n",
      "with any more recent ones:\n",
      " Python 2.7 (all the code is compatible with version 3.3 and 3.4 as well)\n",
      " NumPy 1.8.1\n",
      " SciPy 0.13\n",
      " scikit-learn 0.14.0\n",
      "Who this book is for\n",
      "This book is for Python programmers who want to learn how to perform machine \n",
      "learning using open source libraries. We will walk through the basic modes of \n",
      "machine learning based on realistic examples.\n",
      "This book is also for machine learners who want to start using Python to build their \n",
      "systems. Python is a flexible language for rapid prototyping, while the underlying \n",
      "algorithms are all written in optimized C or C++. Thus the resulting code is fast and \n",
      "robust enough to be used in production as well.\n",
      "Conventions\n",
      "In this book, you will find a number of styles of text that distinguish between \n",
      "different kinds of information. Here are some examples of these styles, and an \n",
      "explanation of their meaning.\n",
      "Code words in text, database table names, folder names, filenames, file extensions, \n",
      "pathnames, dummy URLs, user input, and Twitter handles are shown as follows: \n",
      "\"We then use poly1d() to create a model function from the model parameters.\"\n",
      "A block of code is set as follows:\n",
      "[aws info]\n",
      "AWS_ACCESS_KEY_ID =  AAKIIT7HHF6IUSN3OCAA\n",
      "AWS_SECRET_ACCESS_KEY = <your secret key>\n",
      "Any command-line input or output is written as follows:\n",
      ">>> import numpy\n",
      ">>> numpy.version.full_version\n",
      "1.8.1\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 18, 'page_label': 'x'}\n",
      "Preface\n",
      "[  x ]\n",
      "New terms and important words are shown in bold. Words that you see on the \n",
      "screen, in menus or dialog boxes for example, appear in the text like this: \"Once  \n",
      "the machine is stopped, the Change instance type option becomes available.\"\n",
      "Warnings or important notes appear in a box like this.\n",
      "Tips and tricks appear like this.\n",
      "Reader feedback\n",
      "Feedback from our readers is always welcome. Let us know what you think about \n",
      "this bookwhat you liked or may have disliked. Reader feedback is important for  \n",
      "us to develop titles that you really get the most out of.\n",
      "To send us general feedback, simply send an e-mail to feedback@packtpub.com, \n",
      "and mention the book title via the subject of your message. If there is a topic that you \n",
      "have expertise in and you are interested in either writing or contributing to a book, \n",
      "see our author guide on www.packtpub.com/authors.\n",
      "Customer support\n",
      "Now that you are the proud owner of a Packt book, we have a number of things to \n",
      "help you to get the most from your purchase.\n",
      "Downloading the example code\n",
      "You can download the example code files from your account at http://www.\n",
      "packtpub.com for all the Packt Publishing books you have purchased. If you \n",
      "purchased this book elsewhere, you can visit http://www.packtpub.com/support \n",
      "and register to have the files e-mailed directly to you.\n",
      "The code for this book is also available on GitHub at https://github.com/\n",
      "luispedro/BuildingMachineLearningSystemsWithPython. This repository is  \n",
      "kept up-to-date so that it will incorporate both errata and any necessary updates  \n",
      "for newer versions of Python or of the packages we use in the book.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 19, 'page_label': 'xi'}\n",
      "Preface\n",
      "[  xi ]\n",
      "Errata\n",
      "Although we have taken every care to ensure the accuracy of our content, mistakes \n",
      "do happen. If you find a mistake in one of our booksmaybe a mistake in the text or \n",
      "the codewe would be grateful if you could report this to us. By doing so, you can \n",
      "save other readers from frustration and help us improve subsequent versions of this \n",
      "book. If you find any errata, please report them by visiting http://www.packtpub.\n",
      "com/submit-errata, selecting your book, clicking on the Errata Submission Form \n",
      "link, and entering the details of your errata. Once your errata are verified, your \n",
      "submission will be accepted and the errata will be uploaded to our website or added \n",
      "to any list of existing errata under the Errata section of that title.\n",
      "To view the previously submitted errata, go to https://www.packtpub.com/books/\n",
      "content/support and enter the name of the book in the search field. The required \n",
      "information will appear under the Errata section.\n",
      "Another excellent way would be to visit www.TwoToReal.com where the authors try \n",
      "to provide support and answer all your questions.\n",
      "Piracy\n",
      "Piracy of copyright material on the Internet is an ongoing problem across all media. \n",
      "At Packt, we take the protection of our copyright and licenses very seriously. If you \n",
      "come across any illegal copies of our works, in any form, on the Internet, please \n",
      "provide us with the location address or website name immediately so that we can \n",
      "pursue a remedy.\n",
      "Please contact us at copyright@packtpub.com with a link to the suspected  \n",
      "pirated material.\n",
      "We appreciate your help in protecting our authors, and our ability to bring you \n",
      "valuable content.\n",
      "Questions\n",
      "You can contact us at questions@packtpub.com if you are having a problem with \n",
      "any aspect of the book, and we will do our best to address it.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 20, 'page_label': 'xii'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 21, 'page_label': '1'}\n",
      "[ 1 ]\n",
      "Getting Started with Python \n",
      "Machine Learning\n",
      "Machine learning teaches machines to learn to carry out tasks by themselves. It is \n",
      "that simple. The complexity comes with the details, and that is most likely the reason  \n",
      "you are reading this book.\n",
      "Maybe you have too much data and too little insight. You hope that using  \n",
      "machine learning algorithms you can solve this challenge, so you started digging \n",
      "into the algorithms. But after some time you were puzzled: Which of the myriad  \n",
      "of algorithms should you actually choose?\n",
      "Alternatively, maybe you are in general interested in machine learning and for \n",
      "some time you have been reading blogs and articles about it. Everything seemed \n",
      "to be magic and cool, so you started your exploration and fed some toy data into a \n",
      "decision tree or a support vector machine. However, after you successfully applied \n",
      "it to some other data, you wondered: Was the whole setting right? Did you get the \n",
      "optimal results? And how do you know whether there are no better algorithms? Or \n",
      "whether your data was the right one?\n",
      "Welcome to the club! Both of us (authors) were at those stages looking for \n",
      "information that tells the stories behind the theoretical textbooks about machine \n",
      "learning. It turned out that much of that information was \"black art\" not usually \n",
      "taught in standard text books. So in a sense, we wrote this book to our younger \n",
      "selves. A book that not only gives a quick introduction into machine learning, but \n",
      "also teaches lessons we learned along the way. We hope that it will also give you a \n",
      "smoother entry to one of the most exciting fields in Computer Science.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 22, 'page_label': '2'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  2 ]\n",
      "Machine learning and Python  a dream \n",
      "team\n",
      "The goal of machine learning is to teach machines (software) to carry out tasks by \n",
      "providing them a couple of examples (how to do or not do the task). Let's assume \n",
      "that each morning when you turn on your computer, you do the same task of \n",
      "moving e-mails around so that only e-mails belonging to the same topic end up in \n",
      "the same folder. After some time, you might feel bored and think of automating this \n",
      "chore. One way would be to start analyzing your brain and write down all rules \n",
      "your brain processes while you are shuffling your e-mails. However, this will be \n",
      "quite cumbersome and always imperfect. While you will miss some rules, you will \n",
      "over-specify others. A better and more future-proof way would be to automate this \n",
      "process by choosing a set of e-mail meta info and body/folder name pairs and let an \n",
      "algorithm come up with the best rule set. The pairs would be your training data, and \n",
      "the resulting rule set (also called model) could then be applied to future e-mails that \n",
      "we have not yet seen. This is machine learning in its simplest form.\n",
      "Of course, machine learning (often also referred to as Data Mining or Predictive \n",
      "Analysis) is not a brand new field in itself. Quite the contrary, its success over the \n",
      "recent years can be attributed to the pragmatic way of using rock-solid techniques \n",
      "and insights from other successful fields like statistics. There the purpose is for \n",
      "us humans to get insights into the data, for example, by learning more about the \n",
      "underlying patterns and relationships. As you read more and more about successful \n",
      "applications of machine learning (you have checked out \n",
      "www.kaggle.com already, \n",
      "haven't you?), you will see that applied statistics is a common field among machine \n",
      "learning experts.\n",
      "As you will see later, the process of coming up with a decent ML approach is never \n",
      "a waterfall-like process. Instead, you will see yourself going back and forth in your \n",
      "analysis, trying out different versions of your input data on diverse sets of ML \n",
      "algorithms. It is this explorative nature that lends itself perfectly to Python. Being \n",
      "an interpreted high-level programming language, it seems that Python has been \n",
      "designed exactly for this process of trying out different things. What is more, it \n",
      "does this even fast. Sure, it is slower than C or similar statically typed programming \n",
      "languages. Nevertheless, with the myriad of easy-to-use libraries that are often \n",
      "written in C, you don't have to sacrifice speed for agility.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 23, 'page_label': '3'}\n",
      "[ 3 ]\n",
      "What the book will teach you  \n",
      "(and what it will not)\n",
      "This book will give you a broad overview of what types of learning algorithms  \n",
      "are currently most used in the diverse fields of machine learning, and where to \n",
      "watch out when applying them. From our own experience, however, we know that \n",
      "doing the \"cool\" stuff, that is, using and tweaking machine learning algorithms such \n",
      "as support vector machines, nearest neighbor search, or ensembles thereof, will \n",
      "only consume a tiny fraction of the overall time of a good machine learning expert. \n",
      "Looking at the following typical workflow, we see that most of the time will be spent \n",
      "in rather mundane tasks:\n",
      " Reading in the data and cleaning it\n",
      " Exploring and understanding the input data\n",
      " Analyzing how best to present the data to the learning algorithm\n",
      " Choosing the right model and learning algorithm\n",
      " Measuring the performance correctly\n",
      "When talking about exploring and understanding the input data, we will need a bit \n",
      "of statistics and basic math. However, while doing that, you will see that those topics \n",
      "that seemed to be so dry in your math class can actually be really exciting when you \n",
      "use them to look at interesting data.\n",
      "The journey starts when you read in the data. When you have to answer questions \n",
      "such as how to handle invalid or missing values, you will see that this is more an  \n",
      "art than a precise science. And a very rewarding one, as doing this part right will \n",
      "open your data to more machine learning algorithms and thus increase the likelihood \n",
      "of success.\n",
      "With the data being ready in your program's data structures, you will want to get \n",
      "a real feeling of what animal you are working with. Do you have enough data to \n",
      "answer your questions? If not, you might want to think about additional ways to  \n",
      "get more of it. Do you even have too much data? Then you probably want to think \n",
      "about how best to extract a sample of it.\n",
      "Often you will not feed the data directly into your machine learning algorithm. \n",
      "Instead you will find that you can refine parts of the data before training. Many times \n",
      "the machine learning algorithm will reward you with increased performance. You \n",
      "will even find that a simple algorithm with refined data generally outperforms a very \n",
      "sophisticated algorithm with raw data. This part of the machine learning workflow \n",
      "is called feature engineering, and is most of the time a very exciting and rewarding \n",
      "challenge. You will immediately see the results of being creative and intelligent.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 24, 'page_label': '4'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  4 ]\n",
      "Choosing the right learning algorithm, then, is not simply a shootout of the three or \n",
      "four that are in your toolbox (there will be more you will see). It is more a thoughtful \n",
      "process of weighing different performance and functional requirements. Do you \n",
      "need a fast result and are willing to sacrifice quality? Or would you rather spend \n",
      "more time to get the best possible result? Do you have a clear idea of the future data \n",
      "or should you be a bit more conservative on that side?\n",
      "Finally, measuring the performance is the part where most mistakes are waiting for \n",
      "the aspiring machine learner. There are easy ones, such as testing your approach \n",
      "with the same data on which you have trained. But there are more difficult ones, \n",
      "when you have imbalanced training data. Again, data is the part that determines \n",
      "whether your undertaking will fail or succeed.\n",
      "We see that only the fourth point is dealing with the fancy algorithms. Nevertheless, \n",
      "we hope that this book will convince you that the other four tasks are not simply \n",
      "chores, but can be equally exciting. Our hope is that by the end of the book, you  \n",
      "will have truly fallen in love with data instead of learning algorithms.\n",
      "To that end, we will not overwhelm you with the theoretical aspects of the diverse \n",
      "ML algorithms, as there are already excellent books in that area (you will find \n",
      "pointers in the Appendix). Instead, we will try to provide an intuition of the \n",
      "underlying approaches in the individual chaptersjust enough for you to get the \n",
      "idea and be able to undertake your first steps. Hence, this book is by no means the \n",
      "definitive guide to machine learning. It is more of a starter kit. We hope that it ignites \n",
      "your curiosity enough to keep you eager in trying to learn more and more about this \n",
      "interesting field.\n",
      "In the rest of this chapter, we will set up and get to know the basic Python libraries \n",
      "NumPy and SciPy and then train our first machine learning using scikit-learn. \n",
      "During that endeavor, we will introduce basic ML concepts that will be used \n",
      "throughout the book. The rest of the chapters will then go into more detail through \n",
      "the five steps described earlier, highlighting different aspects of machine learning in \n",
      "Python using diverse application scenarios.\n",
      "What to do when you are stuck\n",
      "We try to convey every idea necessary to reproduce the steps throughout this  \n",
      "book. Nevertheless, there will be situations where you are stuck. The reasons  \n",
      "might range from simple typos over odd combinations of package versions to \n",
      "problems in understanding.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 25, 'page_label': '5'}\n",
      "[ 5 ]\n",
      "In this situation, there are many different ways to get help. Most likely, your problem \n",
      "will already be raised and solved in the following excellent Q&A sites:\n",
      "http://metaoptimize.com/qa: This Q&A site is laser-focused on machine learning \n",
      "topics. For almost every question, it contains above average answers from machine \n",
      "learning experts. Even if you don't have any questions, it is a good habit to check it \n",
      "out every now and then and read through some of the answers.\n",
      "http://stats.stackexchange.com: This Q&A site is named Cross Validated, \n",
      "similar to MetaOptimize, but is focused more on statistical problems.\n",
      "http://stackoverflow.com: This Q&A site is much like the previous ones,  \n",
      "but with broader focus on general programming topics. It contains, for example, \n",
      "more questions on some of the packages that we will use in this book, such as  \n",
      "SciPy or matplotlib.\n",
      "#machinelearning on https://freenode.net/: This is the IRC channel focused \n",
      "on machine learning topics. It is a small but very active and helpful community of \n",
      "machine learning experts.\n",
      "http://www.TwoToReal.com: This is the instant Q&A site written by the authors to \n",
      "support you in topics that don't fit in any of the preceding buckets. If you post your \n",
      "question, one of the authors will get an instant message if he is online and be drawn \n",
      "in a chat with you.\n",
      "As stated in the beginning, this book tries to help you get started quickly on your \n",
      "machine learning journey. Therefore, we highly encourage you to build up your own \n",
      "list of machine learning related blogs and check them out regularly. This is the best \n",
      "way to get to know what works and what doesn't.\n",
      "The only blog we want to highlight right here (more in the Appendix) is http://\n",
      "blog.kaggle.com, the blog of the Kaggle company, which is carrying out machine \n",
      "learning competitions. Typically, they encourage the winners of the competitions to \n",
      "write down how they approached the competition, what strategies did not work, and \n",
      "how they arrived at the winning strategy. Even if you don't read anything else, this is \n",
      "a must.\n",
      "Getting started\n",
      "Assuming that you have Python already installed (everything at least as recent as 2.7 \n",
      "should be fine), we need to install NumPy and SciPy for numerical operations, as well \n",
      "as matplotlib for visualization.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 26, 'page_label': '6'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  6 ]\n",
      "Introduction to NumPy, SciPy, and matplotlib\n",
      "Before we can talk about concrete machine learning algorithms, we have to talk \n",
      "about how best to store the data we will chew through. This is important as the most \n",
      "advanced learning algorithm will not be of any help to us if it will never finish. This \n",
      "may be simply because accessing the data is too slow. Or maybe its representation \n",
      "forces the operating system to swap all day. Add to this that Python is an interpreted \n",
      "language (a highly optimized one, though) that is slow for many numerically \n",
      "heavy algorithms compared to C or FORTRAN. So we might ask why on earth so \n",
      "many scientists and companies are betting their fortune on Python even in highly \n",
      "computation-intensive areas?\n",
      "The answer is that, in Python, it is very easy to off-load number crunching tasks to \n",
      "the lower layer in the form of C or FORTRAN extensions. And that is exactly what \n",
      "NumPy and SciPy do (http://scipy.org/Download). In this tandem, NumPy \n",
      "provides the support of highly optimized multidimensional arrays, which are the \n",
      "basic data structure of most state-of-the-art algorithms. SciPy uses those arrays to \n",
      "provide a set of fast numerical recipes. Finally, matplotlib (http://matplotlib.\n",
      "org/) is probably the most convenient and feature-rich library to plot high-quality \n",
      "graphs using Python.\n",
      "Installing Python\n",
      "Luckily, for all major operating systems, that is, Windows, Mac, and Linux, there \n",
      "are targeted installers for NumPy, SciPy, and matplotlib. If you are unsure about \n",
      "the installation process, you might want to install Anaconda Python distribution \n",
      "(which you can access at https://store.continuum.io/cshop/anaconda/), which \n",
      "is driven by Travis Oliphant, a founding contributor of SciPy. What sets Anaconda \n",
      "apart from other distributions such as Enthought Canopy (which you can download \n",
      "from https://www.enthought.com/downloads/) or Python(x,y) (accessible at \n",
      "http://code.google.com/p/pythonxy/wiki/Downloads), is that Anaconda is \n",
      "already fully Python 3 compatiblethe Python version we will be using throughout \n",
      "the book.\n",
      "Chewing data efficiently with NumPy and intelligently with SciPy\n",
      "Let's walk quickly through some basic NumPy examples and then take a look at \n",
      "what SciPy provides on top of it. On the way, we will get our feet wet with plotting \n",
      "using the marvelous Matplotlib package.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 27, 'page_label': '7'}\n",
      "[ 7 ]\n",
      "For an in-depth explanation, you might want to take a look at some of the more \n",
      "interesting examples of what NumPy has to offer at http://www.scipy.org/\n",
      "Tentative_NumPy_Tutorial.\n",
      "You will also find the NumPy Beginner's Guide - Second Edition, Ivan Idris, by  \n",
      "Packt Publishing, to be very valuable. Additional tutorial style guides can be  \n",
      "found at http://scipy-lectures.github.com, and the official SciPy tutorial  \n",
      "at http://docs.scipy.org/doc/scipy/reference/tutorial.\n",
      "In this book, we will use NumPy in version 1.8.1 and \n",
      "SciPy in version 0.14.0.\n",
      "Learning NumPy\n",
      "So let's import NumPy and play a bit with it. For that, we need to start the Python \n",
      "interactive shell:\n",
      ">>> import numpy\n",
      ">>> numpy.version.full_version\n",
      "1.8.1\n",
      "As we do not want to pollute our namespace, we certainly should not use the \n",
      "following code:\n",
      ">>> from numpy import *\n",
      "Because, for instance, numpy.array will potentially shadow the array package that is \n",
      "included in standard Python. Instead, we will use the following convenient shortcut:\n",
      ">>> import numpy as np\n",
      ">>> a = np.array([0,1,2,3,4,5])\n",
      ">>> a\n",
      "array([0, 1, 2, 3, 4, 5])\n",
      ">>> a.ndim\n",
      "1\n",
      ">>> a.shape\n",
      "(6,)\n",
      "So, we just created an array like we would create a list in Python. However, the \n",
      "NumPy arrays have additional information about the shape. In this case, it is a  \n",
      "one-dimensional array of six elements. No surprise so far.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 28, 'page_label': '8'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  8 ]\n",
      "We can now transform this array to a two-dimensional matrix:\n",
      ">>> b = a.reshape((3,2))\n",
      ">>> b\n",
      "array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5]])\n",
      ">>> b.ndim\n",
      "2\n",
      ">>> b.shape\n",
      "(3, 2)\n",
      "The funny thing starts when we realize just how much the NumPy package is \n",
      "optimized. For example, doing this avoids copies wherever possible:\n",
      ">>> b[1][0] = 77\n",
      ">>> b\n",
      "array([[ 0,  1],\n",
      "       [77,  3],\n",
      "       [ 4,  5]])\n",
      ">>> a\n",
      "array([ 0,  1, 77,  3,  4,  5])\n",
      "In this case, we have modified value 2 to 77 in b, and immediately see the same \n",
      "change reflected in a as well. Keep in mind that whenever you need a true copy,  \n",
      "you can always perform:\n",
      ">>> c = a.reshape((3,2)).copy()\n",
      ">>> c\n",
      "array([[ 0,  1],\n",
      "       [77,  3],\n",
      "       [ 4,  5]])\n",
      ">>> c[0][0] = -99\n",
      ">>> a\n",
      "array([ 0,  1, 77,  3,  4,  5])\n",
      ">>> c\n",
      "array([[-99,   1],\n",
      "       [ 77,   3],\n",
      "       [  4,   5]])\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 29, 'page_label': '9'}\n",
      "[ 9 ]\n",
      "Note that here, c and a are totally independent copies.\n",
      "Another big advantage of NumPy arrays is that the operations are propagated to the \n",
      "individual elements. For example, multiplying a NumPy array will result in an array \n",
      "of the same size with all of its elements being multiplied:\n",
      ">>> d = np.array([1,2,3,4,5])\n",
      ">>> d*2\n",
      "array([ 2,  4,  6,  8, 10])\n",
      "Similarly, for other operations:\n",
      ">>> d**2\n",
      "array([ 1,  4,  9, 16, 25])\n",
      "Contrast that to ordinary Python lists:\n",
      ">>> [1,2,3,4,5]*2\n",
      "[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n",
      ">>> [1,2,3,4,5]**2\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\n",
      "Of course by using NumPy arrays, we sacrifice the agility Python lists offer. Simple \n",
      "operations such as adding or removing are a bit complex for NumPy arrays. Luckily, \n",
      "we have both at our hands and we will use the right one for the task at hand.\n",
      "Indexing\n",
      "Part of the power of NumPy comes from the versatile ways in which its arrays can  \n",
      "be accessed.\n",
      "In addition to normal list indexing, it allows you to use arrays themselves as indices \n",
      "by performing:\n",
      ">>> a[np.array([2,3,4])]\n",
      "array([77,  3,  4])\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 30, 'page_label': '10'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  10 ]\n",
      "Together with the fact that conditions are also propagated to individual elements,  \n",
      "we gain a very convenient way to access our data:\n",
      ">>> a>4\n",
      "array([False, False,  True, False, False,  True], dtype=bool)\n",
      ">>> a[a>4]\n",
      "array([77,  5])\n",
      "By performing the following command, this can be used to trim outliers:\n",
      ">>> a[a>4] = 4\n",
      ">>> a\n",
      "array([0, 1, 4, 3, 4, 4])\n",
      "As this is a frequent use case, there is the special clip function for it, clipping the \n",
      "values at both ends of an interval with one function call:\n",
      ">>> a.clip(0,4)\n",
      "array([0, 1, 4, 3, 4, 4])\n",
      "Handling nonexisting values\n",
      "The power of NumPy's indexing capabilities comes in handy when preprocessing \n",
      "data that we have just read in from a text file. Most likely, that will contain invalid \n",
      "values that we will mark as not being a real number using numpy.NAN:\n",
      ">>> c = np.array([1, 2, np.NAN, 3, 4]) # let's pretend we have read this \n",
      "from a text file\n",
      ">>> c\n",
      "array([  1.,   2.,  nan,   3.,   4.])\n",
      ">>> np.isnan(c)\n",
      "array([False, False,  True, False, False], dtype=bool)\n",
      ">>> c[~np.isnan(c)]\n",
      "array([ 1.,  2.,  3.,  4.])\n",
      ">>> np.mean(c[~np.isnan(c)])\n",
      "2.5\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 31, 'page_label': '11'}\n",
      "[ 11 ]\n",
      "Comparing the runtime\n",
      "Let's compare the runtime behavior of NumPy compared with normal Python lists. \n",
      "In the following code, we will calculate the sum of all squared numbers from 1 to \n",
      "1000 and see how much time it will take. We perform it 10,000 times and report the \n",
      "total time so that our measurement is accurate enough.\n",
      "import timeit\n",
      "normal_py_sec = timeit.timeit('sum(x*x for x in range(1000))',\n",
      "                              number=10000)\n",
      "naive_np_sec = timeit.timeit(\n",
      "                'sum(na*na)',\n",
      "                setup=\"import numpy as np; na=np.arange(1000)\",\n",
      "                number=10000)\n",
      "good_np_sec = timeit.timeit(\n",
      "                'na.dot(na)',\n",
      "                setup=\"import numpy as np; na=np.arange(1000)\",\n",
      "                number=10000)\n",
      "print(\"Normal Python: %f sec\" % normal_py_sec)\n",
      "print(\"Naive NumPy: %f sec\" % naive_np_sec)\n",
      "print(\"Good NumPy: %f sec\" % good_np_sec)\n",
      "Normal Python: 1.050749 sec\n",
      "Naive NumPy: 3.962259 sec\n",
      "Good NumPy: 0.040481 sec\n",
      "We make two interesting observations. Firstly, by just using NumPy as data storage \n",
      "(Naive NumPy) takes 3.5 times longer, which is surprising since we believe it must \n",
      "be much faster as it is written as a C extension. One reason for this is that the access \n",
      "of individual elements from Python itself is rather costly. Only when we are able \n",
      "to apply algorithms inside the optimized extension code is when we get speed \n",
      "improvements. The other observation is quite a tremendous one: using the dot() \n",
      "function of NumPy, which does exactly the same, allows us to be more than 25 times \n",
      "faster. In summary, in every algorithm we are about to implement, we should always \n",
      "look how we can move loops over individual elements from Python to some of the \n",
      "highly optimized NumPy or SciPy extension functions.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 32, 'page_label': '12'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  12 ]\n",
      "However, the speed comes at a price. Using NumPy arrays, we no longer have the \n",
      "incredible flexibility of Python lists, which can hold basically anything. NumPy \n",
      "arrays always have only one data type.\n",
      ">>> a = np.array([1,2,3])\n",
      ">>> a.dtype\n",
      "dtype('int64')\n",
      "If we try to use elements of different types, such as the ones shown in the following \n",
      "code, NumPy will do its best to coerce them to be the most reasonable common  \n",
      "data type:\n",
      ">>> np.array([1, \"stringy\"])\n",
      "array(['1', 'stringy'], dtype='<U7')\n",
      ">>> np.array([1, \"stringy\", set([1,2,3])])\n",
      "array([1, stringy, {1, 2, 3}], dtype=object)\n",
      "Learning SciPy\n",
      "On top of the efficient data structures of NumPy, SciPy offers a magnitude of \n",
      "algorithms working on those arrays. Whatever numerical heavy algorithm you take \n",
      "from current books on numerical recipes, most likely you will find support for them \n",
      "in SciPy in one way or the other. Whether it is matrix manipulation, linear algebra, \n",
      "optimization, clustering, spatial operations, or even fast Fourier transformation, the \n",
      "toolbox is readily filled. Therefore, it is a good habit to always inspect the scipy \n",
      "module before you start implementing a numerical algorithm.\n",
      "For convenience, the complete namespace of NumPy is also accessible via SciPy. So, \n",
      "from now on, we will use NumPy's machinery via the SciPy namespace. You can \n",
      "check this easily comparing the function references of any base function, such as:\n",
      ">>> import scipy, numpy\n",
      ">>> scipy.version.full_version\n",
      "0.14.0\n",
      ">>> scipy.dot is numpy.dot\n",
      "True\n",
      "The diverse algorithms are grouped into the following toolboxes:\n",
      "SciPy packages Functionalities\n",
      "cluster  Hierarchical clustering (cluster.hierarchy)\n",
      " Vector quantization / k-means (cluster.vq)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 33, 'page_label': '13'}\n",
      "[ 13 ]\n",
      "SciPy packages Functionalities\n",
      "constants  Physical and mathematical constants\n",
      " Conversion methods\n",
      "fftpack Discrete Fourier transform algorithms\n",
      "integrate Integration routines\n",
      "interpolate Interpolation (linear, cubic, and so on)\n",
      "io Data input and output\n",
      "linalg Linear algebra routines using the optimized BLAS and LAPACK \n",
      "libraries\n",
      "ndimage n-dimensional image package\n",
      "odr Orthogonal distance regression\n",
      "optimize Optimization (finding minima and roots)\n",
      "signal Signal processing\n",
      "sparse Sparse matrices\n",
      "spatial Spatial data structures and algorithms\n",
      "special Special mathematical functions such as Bessel or Jacobian\n",
      "stats Statistics toolkit\n",
      "The toolboxes most interesting to our endeavor are scipy.stats, scipy.\n",
      "interpolate, scipy.cluster, and scipy.signal. For the sake of brevity,  \n",
      "we will briefly explore some features of the stats package and leave the others  \n",
      "to be explained when they show up in the individual chapters.\n",
      "Our first (tiny) application of machine learning\n",
      "Let's get our hands dirty and take a look at our hypothetical web start-up, MLaaS, \n",
      "which sells the service of providing machine learning algorithms via HTTP. With \n",
      "increasing success of our company, the demand for better infrastructure increases \n",
      "to serve all incoming web requests successfully. We don't want to allocate too \n",
      "many resources as that would be too costly. On the other side, we will lose money, \n",
      "if we have not reserved enough resources to serve all incoming requests. Now, \n",
      "the question is, when will we hit the limit of our current infrastructure, which we \n",
      "estimated to be at 100,000 requests per hour. We would like to know in advance \n",
      "when we have to request additional servers in the cloud to serve all the incoming \n",
      "requests successfully without paying for unused ones.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 34, 'page_label': '14'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  14 ]\n",
      "Reading in the data\n",
      "We have collected the web stats for the last month and aggregated them in ch01/\n",
      "data/web_traffic.tsv (.tsv because it contains tab-separated values). They are \n",
      "stored as the number of hits per hour. Each line contains the hour consecutively and \n",
      "the number of web hits in that hour.\n",
      "The first few lines look like the following:\n",
      "Using SciPy's genfromtxt(), we can easily read in the data using the following code:\n",
      ">>> import scipy as sp\n",
      ">>> data = sp.genfromtxt(\"web_traffic.tsv\", delimiter=\"\\t\")\n",
      "We have to specify tab as the delimiter so that the columns are correctly determined.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 35, 'page_label': '15'}\n",
      "[ 15 ]\n",
      "A quick check shows that we have correctly read in the data:\n",
      ">>> print(data[:10])\n",
      "[[  1.00000000e+00   2.27200000e+03]\n",
      " [  2.00000000e+00              nan]\n",
      " [  3.00000000e+00   1.38600000e+03]\n",
      " [  4.00000000e+00   1.36500000e+03]\n",
      " [  5.00000000e+00   1.48800000e+03]\n",
      " [  6.00000000e+00   1.33700000e+03]\n",
      " [  7.00000000e+00   1.88300000e+03]\n",
      " [  8.00000000e+00   2.28300000e+03]\n",
      " [  9.00000000e+00   1.33500000e+03]\n",
      " [  1.00000000e+01   1.02500000e+03]]\n",
      ">>> print(data.shape)\n",
      "(743, 2)\n",
      "As you can see, we have 743 data points with two dimensions.\n",
      "Preprocessing and cleaning the data\n",
      "It is more convenient for SciPy to separate the dimensions into two vectors, each \n",
      "of size 743. The first vector, x, will contain the hours, and the other, y, will contain \n",
      "the Web hits in that particular hour. This splitting is done using the special index \n",
      "notation of SciPy, by which we can choose the columns individually:\n",
      "x = data[:,0]\n",
      "y = data[:,1]\n",
      "There are many more ways in which data can be selected from a SciPy array.  \n",
      "Check out http://www.scipy.org/Tentative_NumPy_Tutorial for more  \n",
      "details on indexing, slicing, and iterating.\n",
      "One caveat is still that we have some values in y that contain invalid values, nan. The \n",
      "question is what we can do with them. Let's check how many hours contain invalid \n",
      "data, by running the following code:\n",
      ">>> sp.sum(sp.isnan(y))\n",
      "8\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 36, 'page_label': '16'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  16 ]\n",
      "As you can see, we are missing only 8 out of 743 entries, so we can afford to remove \n",
      "them. Remember that we can index a SciPy array with another array. Sp.isnan(y) \n",
      "returns an array of Booleans indicating whether an entry is a number or not. Using \n",
      "~, we logically negate that array so that we choose only those elements from x and y \n",
      "where y contains valid numbers:\n",
      ">>> x = x[~sp.isnan(y)]\n",
      ">>> y = y[~sp.isnan(y)]\n",
      "To get the first impression of our data, let's plot the data in a scatter plot using \n",
      "matplotlib. matplotlib contains the pyplot package, which tries to mimic MATLAB's \n",
      "interface, which is a very convenient and easy to use one as you can see in the \n",
      "following code:\n",
      ">>> import matplotlib.pyplot as plt\n",
      ">>> # plot the (x,y) points with dots of size 10\n",
      ">>> plt.scatter(x, y, s=10)\n",
      ">>> plt.title(\"Web traffic over the last month\")\n",
      ">>> plt.xlabel(\"Time\")\n",
      ">>> plt.ylabel(\"Hits/hour\")\n",
      ">>> plt.xticks([w*7*24 for w in range(10)],\n",
      "               ['week %i' % w for w in range(10)])\n",
      ">>> plt.autoscale(tight=True)\n",
      ">>> # draw a slightly opaque, dashed grid\n",
      ">>> plt.grid(True, linestyle='-', color='0.75')\n",
      ">>> plt.show()\n",
      "You can find more tutorials on plotting at http://matplotlib.org/\n",
      "users/pyplot_tutorial.html.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 37, 'page_label': '17'}\n",
      "[ 17 ]\n",
      "In the resulting chart, we can see that while in the first weeks the traffic stayed more \n",
      "or less the same, the last week shows a steep increase:\n",
      "Choosing the right model and learning algorithm\n",
      "Now that we have a first impression of the data, we return to the initial question: \n",
      "How long will our server handle the incoming web traffic? To answer this we have \n",
      "to do the following:\n",
      "1. Find the real model behind the noisy data points.\n",
      "2. Following this, use the model to extrapolate into the future to find the point \n",
      "in time where our infrastructure has to be extended.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 38, 'page_label': '18'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  18 ]\n",
      "Before building our first model\n",
      "When we talk about models, you can think of them as simplified theoretical \n",
      "approximations of complex reality. As such there is always some inferiority \n",
      "involved, also called the approximation error. This error will guide us in choosing \n",
      "the right model among the myriad of choices we have. And this error will be \n",
      "calculated as the squared distance of the model's prediction to the real data; for \n",
      "example, for a learned model function f, the error is calculated as follows:\n",
      "def error(f, x, y):\n",
      "    return sp.sum((f(x)-y)**2)\n",
      "The vectors x and y contain the web stats data that we have extracted earlier. It is  \n",
      "the beauty of SciPy's vectorized functions that we exploit here with f(x). The trained \n",
      "model is assumed to take a vector and return the results again as a vector of the same \n",
      "size so that we can use it to calculate the difference to y.\n",
      "Starting with a simple straight line\n",
      "Let's assume for a second that the underlying model is a straight line. Then the \n",
      "challenge is how to best put that line into the chart so that it results in the smallest \n",
      "approximation error. SciPy's polyfit() function does exactly that. Given data x and \n",
      "y and the desired order of the polynomial (a straight line has order 1), it finds the \n",
      "model function that minimizes the error function defined earlier:\n",
      "fp1, residuals, rank, sv, rcond = sp.polyfit(x, y, 1, full=True)\n",
      "The polyfit() function returns the parameters of the fitted model function, fp1. \n",
      "And by setting full=True, we also get additional background information on the \n",
      "fitting process. Of this, only residuals are of interest, which is exactly the error of  \n",
      "the approximation:\n",
      ">>> print(\"Model parameters: %s\" % fp1)\n",
      "Model parameters: [   2.59619213  989.02487106]\n",
      ">>> print(residuals)\n",
      "[  3.17389767e+08]\n",
      "This means the best straight line fit is the following function\n",
      "f(x) = 2.59619213 * x + 989.02487106.\n",
      "We then use poly1d() to create a model function from the model parameters:\n",
      ">>> f1 = sp.poly1d(fp1)\n",
      ">>> print(error(f1, x, y))\n",
      "317389767.34\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 39, 'page_label': '19'}\n",
      "[ 19 ]\n",
      "We have used full=True to retrieve more details on the fitting process. Normally, \n",
      "we would not need it, in which case only the model parameters would be returned.\n",
      "We can now use f1() to plot our first trained model. In addition to the preceding \n",
      "plotting instructions, we simply add the following code:\n",
      "fx = sp.linspace(0,x[-1], 1000) # generate X-values for plotting\n",
      "plt.plot(fx, f1(fx), linewidth=4)\n",
      "plt.legend([\"d=%i\" % f1.order], loc=\"upper left\")\n",
      "This will produce the following plot:\n",
      "It seems like the first 4 weeks are not that far off, although we clearly see that there is \n",
      "something wrong with our initial assumption that the underlying model is a straight \n",
      "line. And then, how good or how bad actually is the error of 317,389,767.34?\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 40, 'page_label': '20'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  20 ]\n",
      "The absolute value of the error is seldom of use in isolation. However, when \n",
      "comparing two competing models, we can use their errors to judge which one of \n",
      "them is better. Although our first model clearly is not the one we would use, it serves \n",
      "a very important purpose in the workflow. We will use it as our baseline until we \n",
      "find a better one. Whatever model we come up with in the future, we will compare it \n",
      "against the current baseline.\n",
      "Towards some advanced stuff\n",
      "Let's now fit a more complex model, a polynomial of degree 2, to see whether it \n",
      "better understands our data:\n",
      ">>> f2p = sp.polyfit(x, y, 2)\n",
      ">>> print(f2p)\n",
      "array([  1.05322215e-02,  -5.26545650e+00,   1.97476082e+03])\n",
      ">>> f2 = sp.poly1d(f2p)\n",
      ">>> print(error(f2, x, y))\n",
      "179983507.878\n",
      "You will get the following plot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 41, 'page_label': '21'}\n",
      "[ 21 ]\n",
      "The error is 179,983,507.878, which is almost half the error of the straight line model. \n",
      "This is good but unfortunately this comes with a price: We now have a more complex \n",
      "function, meaning that we have one parameter more to tune inside polyfit(). The \n",
      "fitted polynomial is as follows:\n",
      "f(x) = 0.0105322215 * x**2  - 5.26545650 * x + 1974.76082\n",
      "So, if more complexity gives better results, why not increase the complexity even \n",
      "more? Let's try it for degrees 3, 10, and 100.\n",
      "Interestingly, we do not see d=53 for the polynomial that had been fitted with  \n",
      "100 degrees. Instead, we see lots of warnings on the console:\n",
      "RankWarning: Polyfit may be poorly conditioned\n",
      "This means because of numerical errors, polyfit cannot determine a good fit with  \n",
      "100 degrees. Instead, it figured that 53 must be good enough.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 42, 'page_label': '22'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  22 ]\n",
      "It seems like the curves capture and better the fitted data the more complex they get. \n",
      "And also, the errors seem to tell the same story:\n",
      "Error d=1: 317,389,767.339778\n",
      "Error d=2: 179,983,507.878179\n",
      "Error d=3: 139,350,144.031725\n",
      "Error d=10: 121,942,326.363461\n",
      "Error d=53: 109,318,004.475556\n",
      "However, taking a closer look at the fitted curves, we start to wonder whether they also \n",
      "capture the true process that generated that data. Framed differently, do our models \n",
      "correctly represent the underlying mass behavior of customers visiting our website? \n",
      "Looking at the polynomial of degree 10 and 53, we see wildly oscillating behavior. It \n",
      "seems that the models are fitted too much to the data. So much that it is now capturing \n",
      "not only the underlying process but also the noise. This is called overfitting.\n",
      "At this point, we have the following choices:\n",
      " Choosing one of the fitted polynomial models.\n",
      " Switching to another more complex model class. Splines?\n",
      " Thinking differently about the data and start again.\n",
      "Out of the five fitted models, the first order model clearly is too simple, and the \n",
      "models of order 10 and 53 are clearly overfitting. Only the second and third order \n",
      "models seem to somehow match the data. However, if we extrapolate them at both \n",
      "borders, we see them going berserk.\n",
      "Switching to a more complex class seems also not to be the right way to go. What \n",
      "arguments would back which class? At this point, we realize that we probably have \n",
      "not fully understood our data.\n",
      "Stepping back to go forward  another look at our data\n",
      "So, we step back and take another look at the data. It seems that there is an inflection \n",
      "point between weeks 3 and 4. So let's separate the data and train two lines using \n",
      "week 3.5 as a separation point:\n",
      "inflection = 3.5*7*24 # calculate the inflection point in hours\n",
      "xa = x[:inflection] # data before the inflection point\n",
      "ya = y[:inflection]\n",
      "xb = x[inflection:] # data after\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 43, 'page_label': '23'}\n",
      "[ 23 ]\n",
      "yb = y[inflection:]\n",
      "fa = sp.poly1d(sp.polyfit(xa, ya, 1))\n",
      "fb = sp.poly1d(sp.polyfit(xb, yb, 1))\n",
      "fa_error = error(fa, xa, ya)\n",
      "fb_error = error(fb, xb, yb)\n",
      "print(\"Error inflection=%f\" % (fa_error + fb_error))\n",
      "Error inflection=132950348.197616\n",
      "From the first line, we train with the data up to week 3, and in the second line we \n",
      "train with the remaining data.\n",
      "Clearly, the combination of these two lines seems to be a much better fit to the data \n",
      "than anything we have modeled before. But still, the combined error is higher than \n",
      "the higher order polynomials. Can we trust the error at the end?\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 44, 'page_label': '24'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  24 ]\n",
      "Asked differently, why do we trust the straight line fitted only at the last week of our \n",
      "data more than any of the more complex models? It is because we assume that it will \n",
      "capture future data better. If we plot the models into the future, we see how right we \n",
      "are (d=1 is again our initial straight line).\n",
      "The models of degree 10 and 53 don't seem to expect a bright future of our  \n",
      "start-up. They tried so hard to model the given data correctly that they are clearly \n",
      "useless to extrapolate beyond. This is called overfitting. On the other hand, the  \n",
      "lower degree models seem not to be capable of capturing the data good enough.  \n",
      "This is called underfitting.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 45, 'page_label': '25'}\n",
      "[ 25 ]\n",
      "So let's play fair to models of degree 2 and above and try out how they behave if we \n",
      "fit them only to the data of the last week. After all, we believe that the last week says \n",
      "more about the future than the data prior to it. The result can be seen in the following \n",
      "psychedelic chart, which further shows how badly the problem of overfitting is.\n",
      "Still, judging from the errors of the models when trained only on the data from \n",
      "week 3.5 and later, we still should choose the most complex one (note that we also \n",
      "calculate the error only on the time after the inflection point):\n",
      "Error d=1:   22,143,941.107618\n",
      "Error d=2:   19,768,846.989176\n",
      "Error d=3:   19,766,452.361027\n",
      "Error d=10:  18,949,339.348539\n",
      "Error d=53:  18,300,702.038119\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 46, 'page_label': '26'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  26 ]\n",
      "Training and testing\n",
      "If we only had some data from the future that we could use to measure our models \n",
      "against, then we should be able to judge our model choice only on the resulting \n",
      "approximation error.\n",
      "Although we cannot look into the future, we can and should simulate a similar effect \n",
      "by holding out a part of our data. Let's remove, for instance, a certain percentage of \n",
      "the data and train on the remaining one. Then we used the held-out data to calculate \n",
      "the error. As the model has been trained not knowing the held-out data, we should \n",
      "get a more realistic picture of how the model will behave in the future.\n",
      "The test errors for the models trained only on the time after inflection point now \n",
      "show a completely different picture:\n",
      "Error d=1: 6397694.386394\n",
      "Error d=2: 6010775.401243\n",
      "Error d=3: 6047678.658525\n",
      "Error d=10: 7037551.009519\n",
      "Error d=53: 7052400.001761\n",
      "Have a look at the following plot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 47, 'page_label': '27'}\n",
      "[ 27 ]\n",
      "It seems that we finally have a clear winner: The model with degree 2 has the lowest \n",
      "test error, which is the error when measured using data that the model did not see \n",
      "during training. And this gives us hope that we won't get bad surprises when future \n",
      "data arrives.\n",
      "Answering our initial question\n",
      "Finally we have arrived at a model which we think represents the underlying  \n",
      "process best; it is now a simple task of finding out when our infrastructure will  \n",
      "reach 100,000 requests per hour. We have to calculate when our model function \n",
      "reaches the value 100,000.\n",
      "Having a polynomial of degree 2, we could simply compute the inverse of the \n",
      "function and calculate its value at 100,000. Of course, we would like to have an \n",
      "approach that is applicable to any model function easily.\n",
      "This can be done by subtracting 100,000 from the polynomial, which results in \n",
      "another polynomial, and finding its root. SciPy's optimize module has the function \n",
      "fsolve that achieves this, when providing an initial starting position with parameter \n",
      "x0. As every entry in our input data file corresponds to one hour, and we have 743 of \n",
      "them, we set the starting position to some value after that. Let fbt2 be the winning \n",
      "polynomial of degree 2.\n",
      ">>> fbt2 = sp.poly1d(sp.polyfit(xb[train], yb[train], 2))\n",
      ">>> print(\"fbt2(x)= \\n%s\" % fbt2)\n",
      "fbt2(x)=\n",
      "       2\n",
      "0.086 x - 94.02 x + 2.744e+04\n",
      ">>> print(\"fbt2(x)-100,000= \\n%s\" % (fbt2-100000))\n",
      "fbt2(x)-100,000=\n",
      "       2\n",
      "0.086 x - 94.02 x - 7.256e+04\n",
      ">>> from scipy.optimize import fsolve\n",
      ">>> reached_max = fsolve(fbt2-100000, x0=800)/(7*24)\n",
      ">>> print(\"100,000 hits/hour expected at week %f\" % reached_max[0])\n",
      "It is expected to have 100,000 hits/hour at week 9.616071, so our model tells  \n",
      "us that, given the current user behavior and traction of our start-up, it will  \n",
      "take another month until we have reached our capacity threshold.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 48, 'page_label': '28'}\n",
      "Getting Started with Python Machine Learning\n",
      "[  28 ]\n",
      "Of course, there is a certain uncertainty involved with our prediction. To get a real \n",
      "picture of it, one could draw in more sophisticated statistics to find out about the \n",
      "variance we have to expect when looking farther and farther into the future.\n",
      "And then there are the user and underlying user behavior dynamics that we cannot \n",
      "model accurately. However, at this point, we are fine with the current predictions. \n",
      "After all, we can prepare all time-consuming actions now. If we then monitor our \n",
      "web traffic closely, we will see in time when we have to allocate new resources.\n",
      "Summary\n",
      "Congratulations! You just learned two important things, of which the most \n",
      "important one is that as a typical machine learning operator, you will spend most of \n",
      "your time in understanding and refining the dataexactly what we just did in our \n",
      "first tiny machine learning example. And we hope that this example helped you to \n",
      "start switching your mental focus from algorithms to data. Then you learned how \n",
      "important it is to have the correct experiment setup and that it is vital to not mix up \n",
      "training and testing.\n",
      "Admittedly, the use of polynomial fitting is not the coolest thing in the machine \n",
      "learning world. We have chosen it to not distract you by the coolness of some  \n",
      "shiny algorithm when we conveyed the two most important messages we just \n",
      "summarized earlier.\n",
      "So, let's move to the next chapter in which we will dive deep into scikit-learn, the \n",
      "marvelous machine learning toolkit, give an overview of different types of learning, \n",
      "and show you the beauty of feature engineering.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 49, 'page_label': '29'}\n",
      "[ 29 ]\n",
      "Classifying with Real-world \n",
      "Examples\n",
      "The topic of this chapter is classification. You have probably already used this form \n",
      "of machine learning as a consumer, even if you were not aware of it. If you have any \n",
      "modern e-mail system, it will likely have the ability to automatically detect spam. \n",
      "That is, the system will analyze all incoming e-mails and mark them as either spam \n",
      "or not-spam. Often, you, the end user, will be able to manually tag e-mails as spam or \n",
      "not, in order to improve its spam detection ability. This is a form of machine learning \n",
      "where the system is taking examples of two types of messages: spam and ham (the \n",
      "typical term for \"non spam e-mails\") and using these examples to automatically classify \n",
      "incoming e-mails.\n",
      "The general method of classification is to use a set of examples of each class to learn \n",
      "rules that can be applied to new examples. This is one of the most important machine \n",
      "learning modes and is the topic of this chapter.\n",
      "Working with text such as e-mails requires a specific set of techniques and skills, and \n",
      "we discuss those in the next chapter. For the moment, we will work with a smaller, \n",
      "easier-to-handle dataset. The example question for this chapter is, \"Can a machine \n",
      "distinguish between flower species based on images?\" We will use two datasets \n",
      "where measurements of flower morphology are recorded along with the species for \n",
      "several specimens.\n",
      "We will explore these small datasets using a few simple algorithms. At first,  \n",
      "we will write classification code ourselves in order to understand the concepts,  \n",
      "but we will quickly switch to using scikit-learn whenever possible. The goal is  \n",
      "to first understand the basic principles of classification and then progress to using  \n",
      "a state-of-the-art implementation.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 50, 'page_label': '30'}\n",
      "Classifying with Real-world Examples\n",
      "[  30 ]\n",
      "The Iris dataset\n",
      "The Iris dataset is a classic dataset from the 1930s; it is one of the first modern \n",
      "examples of statistical classification.\n",
      "The dataset is a collection of morphological measurements of several Iris flowers. \n",
      "These measurements will enable us to distinguish multiple species of the flowers. \n",
      "Today, species are identified by their DNA fingerprints, but in the 1930s, DNA's  \n",
      "role in genetics had not yet been discovered.\n",
      "The following four attributes of each plant were measured:\n",
      " sepal length\n",
      " sepal width\n",
      " petal length\n",
      " petal width\n",
      "In general, we will call the individual numeric measurements we use to describe \n",
      "our data features. These features can be directly measured or computed from \n",
      "intermediate data.\n",
      "This dataset has four features. Additionally, for each plant, the species was recorded. \n",
      "The problem we want to solve is, \"Given these examples, if we see a new flower out in \n",
      "the field, could we make a good prediction about its species from its measurements?\"\n",
      "This is the supervised learning or classification problem: given labeled examples, \n",
      "can we design a rule to be later applied to other examples? A more familiar example \n",
      "to modern readers who are not botanists is spam filtering, where the user can mark \n",
      "e-mails as spam, and systems use these as well as the non-spam e-mails to determine \n",
      "whether a new, incoming message is spam or not.\n",
      "Later in the book, we will look at problems dealing with text (starting in the next \n",
      "chapter). For the moment, the Iris dataset serves our purposes well. It is small  \n",
      "(150 examples, four features each) and can be easily visualized and manipulated.\n",
      "Visualization is a good first step\n",
      "Datasets, later in the book, will grow to thousands of features. With only four in our \n",
      "starting example, we can easily plot all two-dimensional projections on a single page. \n",
      "We will build intuitions on this small example, which can then be extended to large \n",
      "datasets with many more features. As we saw in the previous chapter, visualizations \n",
      "are excellent at the initial exploratory phase of the analysis as they allow you to learn \n",
      "the general features of your problem as well as catch problems that occurred with \n",
      "data collection early.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 51, 'page_label': '31'}\n",
      "[ 31 ]\n",
      "Each subplot in the following plot shows all points projected into two of the \n",
      "dimensions. The outlying group (triangles) are the Iris Setosa plants, while Iris \n",
      "Versicolor plants are in the center (circle) and Iris Virginica are plotted with x marks. \n",
      "We can see that there are two large groups: one is of Iris Setosa and another is a \n",
      "mixture of Iris Versicolor and Iris Virginica.\n",
      "In the following code snippet, we present the code to load the data and generate  \n",
      "the plot:\n",
      ">>> from matplotlib import pyplot as plt\n",
      ">>> import numpy as np\n",
      ">>> # We load the data with load_iris from sklearn\n",
      ">>> from sklearn.datasets import load_iris\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 52, 'page_label': '32'}\n",
      "Classifying with Real-world Examples\n",
      "[  32 ]\n",
      ">>> data = load_iris()\n",
      ">>> # load_iris returns an object with several fields\n",
      ">>> features = data.data\n",
      ">>> feature_names = data.feature_names\n",
      ">>> target = data.target\n",
      ">>> target_names = data.target_names\n",
      ">>> for t in range(3):\n",
      "...    if t == 0:\n",
      "...        c = 'r'\n",
      "...        marker = '>'\n",
      "...    elif t == 1:\n",
      "...        c = 'g'\n",
      "...        marker = 'o'\n",
      "...    elif t == 2:\n",
      "...        c = 'b'\n",
      "...        marker = 'x'\n",
      "...    plt.scatter(features[target == t,0],\n",
      "...                features[target == t,1],\n",
      "...                marker=marker,\n",
      "...                c=c)\n",
      "Building our first classification model\n",
      "If the goal is to separate the three types of flowers, we can immediately make a few \n",
      "suggestions just by looking at the data. For example, petal length seems to be able \n",
      "to separate Iris Setosa from the other two flower species on its own. We can write a \n",
      "little bit of code to discover where the cut-off is:\n",
      ">>> # We use NumPy fancy indexing to get an array of strings:\n",
      ">>> labels = target_names[target]\n",
      ">>> # The petal length is the feature at position 2\n",
      ">>> plength = features[:, 2]\n",
      ">>> # Build an array of booleans:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 53, 'page_label': '33'}\n",
      "[ 33 ]\n",
      ">>> is_setosa = (labels == 'setosa')\n",
      ">>> # This is the important step:\n",
      ">>> max_setosa = plength[is_setosa].max()\n",
      ">>> min_non_setosa = plength[~is_setosa].min()\n",
      ">>> print('Maximum of setosa: {0}.'.format(max_setosa))\n",
      "Maximum of setosa: 1.9.\n",
      ">>> print('Minimum of others: {0}.'.format(min_non_setosa))\n",
      "Minimum of others: 3.0.\n",
      "Therefore, we can build a simple model: if the petal length is smaller than 2, then \n",
      "this is an Iris Setosa flower; otherwise it is either Iris Virginica or Iris Versicolor. This \n",
      "is our first model and it works very well in that it separates Iris Setosa flowers from \n",
      "the other two species without making any mistakes. In this case, we did not actually \n",
      "do any machine learning. Instead, we looked at the data ourselves, looking for a \n",
      "separation between the classes. Machine learning happens when we write code to \n",
      "look for this separation automatically.\n",
      "The problem of recognizing Iris Setosa apart from the other two species was \n",
      "very easy. However, we cannot immediately see what the best threshold is for \n",
      "distinguishing Iris Virginica from Iris Versicolor. We can even see that we will never \n",
      "achieve perfect separation with these features. We could, however, look for the best \n",
      "possible separation, the separation that makes the fewest mistakes. For this, we will \n",
      "perform a little computation.\n",
      "We first select only the non-Setosa features and labels:\n",
      ">>> # ~ is the boolean negation operator\n",
      ">>> features = features[~is_setosa]\n",
      ">>> labels = labels[~is_setosa]\n",
      ">>> # Build a new target variable, is_virginica\n",
      ">>> is_virginica = (labels == 'virginica')\n",
      "Here we are heavily using NumPy operations on arrays. The is_setosa array is a \n",
      "Boolean array and we use it to select a subset of the other two arrays, features and \n",
      "labels. Finally, we build a new boolean array, virginica, by using an equality \n",
      "comparison on labels.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 54, 'page_label': '34'}\n",
      "Classifying with Real-world Examples\n",
      "[  34 ]\n",
      "Now, we run a loop over all possible features and thresholds to see which one  \n",
      "results in better accuracy. Accuracy is simply the fraction of examples that the  \n",
      "model classifies correctly.\n",
      ">>> # Initialize best_acc to impossibly low value\n",
      ">>> best_acc = -1.0\n",
      ">>> for fi in range(features.shape[1]):\n",
      "...  # We are going to test all possible thresholds\n",
      "...  thresh = features[:,fi]\n",
      "...  for t in thresh:\n",
      "...    # Get the vector for feature `fi`\n",
      "...    feature_i = features[:, fi]\n",
      "...    # apply threshold `t`\n",
      "...    pred = (feature_i > t)\n",
      "...    acc = (pred == is_virginica).mean()\n",
      "...    rev_acc = (pred == ~is_virginica).mean()\n",
      "...    if rev_acc > acc:\n",
      "...        reverse = True\n",
      "...        acc = rev_acc\n",
      "...    else:\n",
      "...        reverse = False\n",
      "...\n",
      "...    if acc > best_acc:\n",
      "...      best_acc = acc\n",
      "...      best_fi = fi\n",
      "...      best_t = t\n",
      "...      best_reverse = reverse\n",
      "We need to test two types of thresholds for each feature and value: we test a greater \n",
      "than threshold and the reverse comparison. This is why we need the rev_acc variable \n",
      "in the preceding code; it holds the accuracy of reversing the comparison.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 55, 'page_label': '35'}\n",
      "[ 35 ]\n",
      "The last few lines select the best model. First, we compare the predictions, pred, \n",
      "with the actual labels, is_virginica. The little trick of computing the mean of the \n",
      "comparisons gives us the fraction of correct results, the accuracy. At the end of the \n",
      "for loop, all the possible thresholds for all the possible features have been tested, \n",
      "and the variables best_fi, best_t, and best_reverse hold our model. This is all \n",
      "the information we need to be able to classify a new, unknown object, that is, to \n",
      "assign a class to it. The following code implements exactly this method:\n",
      "def is_virginica_test(fi, t, reverse, example):\n",
      "    \"Apply threshold model to a new example\"\n",
      "    test = example[fi] > t\n",
      "    if reverse:\n",
      "        test = not test\n",
      "    return test\n",
      "What does this model look like? If we run the code on the whole data, the model that \n",
      "is identified as the best makes decisions by splitting on the petal width. One way \n",
      "to gain intuition about how this works is to visualize the decision boundary. That \n",
      "is, we can see which feature values will result in one decision versus the other and \n",
      "exactly where the boundary is. In the following screenshot, we see two regions: one \n",
      "is white and the other is shaded in grey. Any datapoint that falls on the white region \n",
      "will be classified as Iris Virginica, while any point that falls on the shaded side will \n",
      "be classified as Iris Versicolor.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 56, 'page_label': '36'}\n",
      "Classifying with Real-world Examples\n",
      "[  36 ]\n",
      "In a threshold model, the decision boundary will always be a line that is parallel to \n",
      "one of the axes. The plot in the preceding screenshot shows the decision boundary and \n",
      "the two regions where points are classified as either white or grey. It also shows (as a \n",
      "dashed line) an alternative threshold, which will achieve exactly the same accuracy. \n",
      "Our method chose the first threshold it saw, but that was an arbitrary choice.\n",
      "Evaluation  holding out data and cross-validation\n",
      "The model discussed in the previous section is a simple model; it achieves 94 percent \n",
      "accuracy of the whole data. However, this evaluation may be overly optimistic. We \n",
      "used the data to define what the threshold will be, and then we used the same data \n",
      "to evaluate the model. Of course, the model will perform better than anything else \n",
      "we tried on this dataset. The reasoning is circular.\n",
      "What we really want to do is estimate the ability of the model to generalize to new \n",
      "instances. We should measure its performance in instances that the algorithm has \n",
      "not seen at training. Therefore, we are going to do a more rigorous evaluation and \n",
      "use held-out data. For this, we are going to break up the data into two groups: on \n",
      "one group, we'll train the model, and on the other, we'll test the one we held out \n",
      "of training. The full code, which is an adaptation of the code presented earlier, is \n",
      "available on the online support repository. Its output is as follows:\n",
      "Training accuracy was 96.0%.\n",
      "Testing accuracy was 90.0% (N = 50).\n",
      "The result on the training data (which is a subset of the whole data) is apparently \n",
      "even better than before. However, what is important to note is that the result in \n",
      "the testing data is lower than that of the training error. While this may surprise an \n",
      "inexperienced machine learner, it is expected that testing accuracy will be lower than \n",
      "the training accuracy. To see why, look back at the plot that showed the decision \n",
      "boundary. Consider what would have happened if some of the examples close to the \n",
      "boundary were not there or that one of them between the two lines was missing. It is \n",
      "easy to imagine that the boundary will then move a little bit to the right or to the left \n",
      "so as to put them on the wrong side of the border.\n",
      "The accuracy on the training data, the training accuracy, is almost \n",
      "always an overly optimistic estimate of how well your algorithm is \n",
      "doing. We should always measure and report the testing accuracy, \n",
      "which is the accuracy on a collection of examples that were not used \n",
      "for training.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 57, 'page_label': '37'}\n",
      "[ 37 ]\n",
      "These concepts will become more and more important as the models become  \n",
      "more complex. In this example, the difference between the accuracy measured on \n",
      "training data and on testing data is not very large. When using a complex model, \n",
      "it is possible to get 100 percent accuracy in training and do no better than random \n",
      "guessing on testing!\n",
      "One possible problem with what we did previously, which was to hold out data \n",
      "from training, is that we only used half the data for training. Perhaps it would \n",
      "have been better to use more training data. On the other hand, if we then leave too \n",
      "little data for testing, the error estimation is performed on a very small number of \n",
      "examples. Ideally, we would like to use all of the data for training and all of the data \n",
      "for testing as well, which is impossible.\n",
      "We can achieve a good approximation of this impossible ideal by a method called \n",
      "cross-validation. One simple form of cross-validation is leave-one-out cross-validation. \n",
      "We will take an example out of the training data, learn a model without this \n",
      "example, and then test whether the model classifies this example correctly.  \n",
      "This process is then repeated for all the elements in the dataset.\n",
      "The following code implements exactly this type of cross-validation:\n",
      ">>> correct = 0.0\n",
      ">>> for ei in range(len(features)):\n",
      "      # select all but the one at position `ei`:\n",
      "      training = np.ones(len(features), bool)\n",
      "      training[ei] = False\n",
      "      testing = ~training\n",
      "      model = fit_model(features[training], is_virginica[training])\n",
      "      predictions = predict(model, features[testing])\n",
      "      correct += np.sum(predictions == is_virginica[testing])\n",
      ">>> acc = correct/float(len(features))\n",
      ">>> print('Accuracy: {0:.1%}'.format(acc))\n",
      "Accuracy: 87.0%\n",
      "At the end of this loop, we will have tested a series of models on all the examples \n",
      "and have obtained a final average result. When using cross-validation, there is no \n",
      "circularity problem because each example was tested on a model which was built \n",
      "without taking that datapoint into account. Therefore, the cross-validated estimate  \n",
      "is a reliable estimate of how well the models would generalize to new data.\n",
      "The major problem with leave-one-out cross-validation is that we are now forced to \n",
      "perform many times more work. In fact, you must learn a whole new model for each \n",
      "and every example and this cost will increase as our dataset grows.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 58, 'page_label': '38'}\n",
      "Classifying with Real-world Examples\n",
      "[  38 ]\n",
      "We can get most of the benefits of leave-one-out at a fraction of the cost by using \n",
      "x-fold cross-validation, where x stands for a small number. For example, to perform \n",
      "five-fold cross-validation, we break up the data into five groups, so-called five folds.\n",
      "Then you learn five models: each time you will leave one fold out of the training \n",
      "data. The resulting code will be similar to the code given earlier in this section, but \n",
      "we leave 20 percent of the data out instead of just one element. We test each of these \n",
      "models on the left-out fold and average the results.\n",
      "The preceding figure illustrates this process for five blocks: the dataset is split into \n",
      "five pieces. For each fold, you hold out one of the blocks for testing and train on the \n",
      "other four. You can use any number of folds you wish. There is a trade-off between \n",
      "computational efficiency (the more folds, the more computation is necessary) and \n",
      "accurate results (the more folds, the closer you are to using the whole of the data for \n",
      "training). Five folds is often a good compromise. This corresponds to training with 80 \n",
      "percent of your data, which should already be close to what you will get from using \n",
      "all the data. If you have little data, you can even consider using 10 or 20 folds. In the \n",
      "extreme case, if you have as many folds as datapoints, you are simply performing \n",
      "leave-one-out cross-validation. On the other hand, if computation time is an issue \n",
      "and you have more data, 2 or 3 folds may be the more appropriate choice.\n",
      "When generating the folds, you need to be careful to keep them balanced. For \n",
      "example, if all of the examples in one fold come from the same class, then the results \n",
      "will not be representative. We will not go into the details of how to do this, because \n",
      "the machine learning package scikit-learn will handle them for you.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 59, 'page_label': '39'}\n",
      "[ 39 ]\n",
      "We have now generated several models instead of just one. So, \"What final model \n",
      "do we return and use for new data?\" The simplest solution is now to train a single \n",
      "overall model on all your training data. The cross-validation loop gives you an \n",
      "estimate of how well this model should generalize.\n",
      "A cross-validation schedule allows you to use all your data \n",
      "to estimate whether your methods are doing well. At the end \n",
      "of the cross-validation loop, you can then use all your data to \n",
      "train a final model.\n",
      "Although it was not properly recognized when machine learning was starting out as \n",
      "a field, nowadays, it is seen as a very bad sign to even discuss the training accuracy \n",
      "of a classification system. This is because the results can be very misleading and even \n",
      "just presenting them marks you as a newbie in machine learning. We always want \n",
      "to measure and compare either the error on a held-out dataset or the error estimated \n",
      "using a cross-validation scheme.\n",
      "Building more complex classifiers\n",
      "In the previous section, we used a very simple model: a threshold on a single feature. \n",
      "Are there other types of systems? Yes, of course! Many others. Throughout this \n",
      "book, you will see many other types of models and we're not even going to cover \n",
      "everything that is out there.\n",
      "To think of the problem at a higher abstraction level, \"What makes up a classification \n",
      "model?\" We can break it up into three parts:\n",
      " The structure of the model: How exactly will a model make decisions?  \n",
      "In this case, the decision depended solely on whether a given feature was \n",
      "above or below a certain threshold value. This is too simplistic for all but  \n",
      "the simplest problems.\n",
      " The search procedure: How do we find the model we need to use? In our \n",
      "case, we tried every possible combination of feature and threshold. You can \n",
      "easily imagine that as models get more complex and datasets get larger, it \n",
      "rapidly becomes impossible to attempt all combinations and we are forced \n",
      "to use approximate solutions. In other cases, we need to use advanced \n",
      "optimization methods to find a good solution (fortunately, scikit-learn \n",
      "already implements these for you, so using them is easy even if the code \n",
      "behind them is very advanced).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 60, 'page_label': '40'}\n",
      "Classifying with Real-world Examples\n",
      "[  40 ]\n",
      " The gain or loss function: How do we decide which of the possibilities \n",
      "tested should be returned? Rarely do we find the perfect solution, the model \n",
      "that never makes any mistakes, so we need to decide which one to use. We \n",
      "used accuracy, but sometimes it will be better to optimize so that the model \n",
      "makes fewer errors of a specific kind. For example, in spam filtering, it may \n",
      "be worse to delete a good e-mail than to erroneously let a bad e-mail through. \n",
      "In that case, we may want to choose a model that is conservative in throwing \n",
      "out e-mails rather than the one that just makes the fewest mistakes overall. \n",
      "We can discuss these issues in terms of gain (which we want to maximize) or \n",
      "loss (which we want to minimize). They are equivalent, but sometimes one is \n",
      "more convenient than the other.\n",
      "We can play around with these three aspects of classifiers and get different systems. \n",
      "A simple threshold is one of the simplest models available in machine learning \n",
      "libraries and only works well when the problem is very simple, such as with the \n",
      "Iris dataset. In the next section, we will tackle a more difficult classification task that \n",
      "requires a more complex structure.\n",
      "In our case, we optimized the threshold to minimize the number of errors. \n",
      "Alternatively, we might have different loss functions. It might be that one type of \n",
      "error is much costlier than the other. In a medical setting, false negatives and false \n",
      "positives are not equivalent. A false negative (when the result of a test comes back \n",
      "negative, but that is false) might lead to the patient not receiving treatment for a \n",
      "serious disease. A false positive (when the test comes back positive even though the \n",
      "patient does not actually have that disease) might lead to additional tests to confirm \n",
      "or unnecessary treatment (which can still have costs, including side effects from the \n",
      "treatment, but are often less serious than missing a diagnostic). Therefore, depending \n",
      "on the exact setting, different trade-offs can make sense. At one extreme, if the \n",
      "disease is fatal and the treatment is cheap with very few negative side-effects, then \n",
      "you want to minimize false negatives as much as you can.\n",
      "What the gain/cost function should be is always dependent on the \n",
      "exact problem you are working on. When we present a general-purpose \n",
      "algorithm, we often focus on minimizing the number of mistakes, \n",
      "achieving the highest accuracy. However, if some mistakes are costlier \n",
      "than others, it might be better to accept a lower overall accuracy to \n",
      "minimize the overall costs.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 61, 'page_label': '41'}\n",
      "[ 41 ]\n",
      "A more complex dataset and a more \n",
      "complex classifier\n",
      "We will now look at a slightly more complex dataset. This will motivate the \n",
      "introduction of a new classification algorithm and a few other ideas.\n",
      "Learning about the Seeds dataset\n",
      "We now look at another agricultural dataset, which is still small, but already too \n",
      "large to plot exhaustively on a page as we did with Iris. This dataset consists of \n",
      "measurements of wheat seeds. There are seven features that are present, which  \n",
      "are as follows:\n",
      " area A\n",
      " perimeter P\n",
      " compactness C = 4A/P\n",
      " length of kernel\n",
      " width of kernel\n",
      " asymmetry coefficient\n",
      " length of kernel groove\n",
      "There are three classes, corresponding to three wheat varieties: Canadian, Koma, \n",
      "and Rosa. As earlier, the goal is to be able to classify the species based on these \n",
      "morphological measurements. Unlike the Iris dataset, which was collected in the \n",
      "1930s, this is a very recent dataset and its features were automatically computed \n",
      "from digital images.\n",
      "This is how image pattern recognition can be implemented: you can take images, \n",
      "in digital form, compute a few relevant features from them, and use a generic \n",
      "classification system. In Chapter 10, Computer Vision, we will work through the \n",
      "computer vision side of this problem and compute features in images. For the \n",
      "moment, we will work with the features that are given to us.UCI Machine Learning Dataset Repository\n",
      "The University of California at Irvine (UCI) maintains an online \n",
      "repository of machine learning datasets (at the time of writing, they \n",
      "list 233 datasets). Both the Iris and the Seeds dataset used in this \n",
      "chapter were taken from there.\n",
      "The repository is available online at http://archive.ics.uci.\n",
      "edu/ml/.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 62, 'page_label': '42'}\n",
      "Classifying with Real-world Examples\n",
      "[  42 ]\n",
      "Features and feature engineering\n",
      "One interesting aspect of these features is that the compactness feature is not actually \n",
      "a new measurement, but a function of the previous two features, area and perimeter. \n",
      "It is often very useful to derive new combined features. Trying to create new features \n",
      "is generally called feature engineering. It is sometimes seen as less glamorous than \n",
      "algorithms, but it often matters more for performance (a simple algorithm on well-\n",
      "chosen features will perform better than a fancy algorithm on not-so-good features).\n",
      "In this case, the original researchers computed the compactness, which is a typical \n",
      "feature for shapes. It is also sometimes called roundness. This feature will have the \n",
      "same value for two kernels, one of which is twice as big as the other one, but with the \n",
      "same shape. However, it will have different values for kernels that are very round \n",
      "(when the feature is close to one) when compared to kernels that are elongated \n",
      "(when the feature is closer to zero).\n",
      "The goals of a good feature are to simultaneously vary with what matters (the \n",
      "desired output) and be invariant with what does not. For example, compactness does \n",
      "not vary with size, but varies with the shape. In practice, it might be hard to achieve \n",
      "both objectives perfectly, but we want to approximate this ideal.\n",
      "You will need to use background knowledge to design good features. Fortunately, \n",
      "for many problem domains, there is already a vast literature of possible features and \n",
      "feature-types that you can build upon. For images, all of the previously mentioned \n",
      "features are typical and computer vision libraries will compute them for you. In \n",
      "text-based problems too, there are standard solutions that you can mix and match \n",
      "(we will also see this in the next chapter). When possible, you should use your \n",
      "knowledge of the problem to design a specific feature or to select which ones from \n",
      "the literature are more applicable to the data at hand.\n",
      "Even before you have data, you must decide which data is worthwhile to collect. \n",
      "Then, you hand all your features to the machine to evaluate and compute the  \n",
      "best classifier.\n",
      "A natural question is whether we can select good features automatically. This \n",
      "problem is known as feature selection. There are many methods that have been \n",
      "proposed for this problem, but in practice very simple ideas work best. For the small \n",
      "problems we are currently exploring, it does not make sense to use feature selection, \n",
      "but if you had thousands of features, then throwing out most of them might make \n",
      "the rest of the process much faster.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 63, 'page_label': '43'}\n",
      "[ 43 ]\n",
      "Nearest neighbor classification\n",
      "For use with this dataset, we will introduce a new classifier: the nearest neighbor \n",
      "classifier. The nearest neighbor classifier is very simple. When classifying a new \n",
      "element, it looks at the training data for the object that is closest to it, its nearest \n",
      "neighbor. Then, it returns its label as the answer. Notice that this model performs \n",
      "perfectly on its training data! For each point, its closest neighbor is itself, and so  \n",
      "its label matches perfectly (unless two examples with different labels have exactly  \n",
      "the same feature values, which will indicate that the features you are using are  \n",
      "not very descriptive). Therefore, it is essential to test the classification using a  \n",
      "cross-validation protocol.\n",
      "The nearest neighbor method can be generalized to look not at a single neighbor, \n",
      "but to multiple ones and take a vote amongst the neighbors. This makes the method \n",
      "more robust to outliers or mislabeled data.\n",
      "Classifying with scikit-learn\n",
      "We have been using handwritten classification code, but Python is a very \n",
      "appropriate language for machine learning because of its excellent libraries. In \n",
      "particular, scikit-learn has become the standard library for many machine learning \n",
      "tasks, including classification. We are going to use its implementation of nearest \n",
      "neighbor classification in this section.\n",
      "The scikit-learn classification API is organized around classifier objects. These objects \n",
      "have the following two essential methods:\n",
      " fit(features, labels): This is the learning step and fits the parameters of \n",
      "the model\n",
      " predict(features): This method can only be called after fit and returns a \n",
      "prediction for one or more inputs\n",
      "Here is how we could use its implementation of k-nearest neighbors for our data. We \n",
      "start by importing the KneighborsClassifier object from the sklearn.neighbors \n",
      "submodule:\n",
      ">>> from sklearn.neighbors import KNeighborsClassifier\n",
      "The scikit-learn module is imported as sklearn (sometimes you will also find that \n",
      "scikit-learn is referred to using this short name instead of the full name). All of the \n",
      "sklearn functionality is in submodules, such as sklearn.neighbors.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 64, 'page_label': '44'}\n",
      "Classifying with Real-world Examples\n",
      "[  44 ]\n",
      "We can now instantiate a classifier object. In the constructor, we specify the number \n",
      "of neighbors to consider, as follows:\n",
      ">>> classifier = KNeighborsClassifier(n_neighbors=1)\n",
      "If we do not specify the number of neighbors, it defaults to 5, which is often a good \n",
      "choice for classification.\n",
      "We will want to use cross-validation (of course) to look at our data. The scikit-learn \n",
      "module also makes this easy:\n",
      ">>> from sklearn.cross_validation import KFold\n",
      ">>> kf = KFold(len(features), n_folds=5, shuffle=True)\n",
      ">>> # `means` will be a list of mean accuracies (one entry per fold)\n",
      ">>> means = []\n",
      ">>> for training,testing in kf:\n",
      "...    # We fit a model for this fold, then apply it to the\n",
      "...    # testing data with `predict`:\n",
      "...    classifier.fit(features[training], labels[training])\n",
      "...    prediction = classifier.predict(features[testing])\n",
      "...\n",
      "...    # np.mean on an array of booleans returns fraction\n",
      "...    # of correct decisions for this fold:\n",
      "...    curmean = np.mean(prediction == labels[testing])\n",
      "...    means.append(curmean)\n",
      ">>> print(\"Mean accuracy: {:.1%}\".format(np.mean(means)))\n",
      "Mean accuracy: 90.5%\n",
      "Using five folds for cross-validation, for this dataset, with this algorithm, we obtain \n",
      "90.5 percent accuracy. As we discussed in the earlier section, the cross-validation \n",
      "accuracy is lower than the training accuracy, but this is a more credible estimate  \n",
      "of the performance of the model.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 65, 'page_label': '45'}\n",
      "[ 45 ]\n",
      "Looking at the decision boundaries\n",
      "We will now examine the decision boundary. In order to plot these on paper, we will \n",
      "simplify and look at only two dimensions. Take a look at the following plot:\n",
      "Canadian examples are shown as diamonds, Koma seeds as circles, and Rosa seeds \n",
      "as triangles. Their respective areas are shown as white, black, and grey. You might \n",
      "be wondering why the regions are so horizontal, almost weirdly so. The problem is \n",
      "that the x axis (area) ranges from 10 to 22, while the y axis (compactness) ranges from \n",
      "0.75 to 1.0. This means that a small change in x is actually much larger than a small \n",
      "change in y. So, when we compute the distance between points, we are, for the most \n",
      "part, only taking the x axis into account. This is also a good example of why it is a \n",
      "good idea to visualize our data and look for red flags or surprises.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 66, 'page_label': '46'}\n",
      "Classifying with Real-world Examples\n",
      "[  46 ]\n",
      "If you studied physics (and you remember your lessons), you might have already \n",
      "noticed that we had been summing up lengths, areas, and dimensionless quantities, \n",
      "mixing up our units (which is something you never want to do in a physical system). \n",
      "We need to normalize all of the features to a common scale. There are many solutions \n",
      "to this problem; a simple one is to normalize to z-scores. The z-score of a value is  \n",
      "how far away from the mean it is, in units of standard deviation. It comes down  \n",
      "to this operation:\n",
      "ff \n",
      "\n",
      "=\n",
      "In this formula, f is the old feature value, f' is the normalized feature value,  is the \n",
      "mean of the feature, and  is the standard deviation. Both  and  are estimated from \n",
      "training data. Independent of what the original values were, after z-scoring, a value \n",
      "of zero corresponds to the training mean, positive values are above the mean, and \n",
      "negative values are below it.\n",
      "The scikit-learn module makes it very easy to use this normalization as a \n",
      "preprocessing step. We are going to use a pipeline of transformations: the first \n",
      "element will do the transformation and the second element will do the classification. \n",
      "We start by importing both the pipeline and the feature scaling classes as follows:\n",
      ">>> from sklearn.pipeline import Pipeline\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      "Now, we can combine them.\n",
      ">>> classifier = KNeighborsClassifier(n_neighbors=1)\n",
      ">>> classifier = Pipeline([('norm', StandardScaler()),\n",
      "...         ('knn', classifier)])\n",
      "The Pipeline constructor takes a list of pairs (str,clf). Each pair corresponds to a \n",
      "step in the pipeline: the first element is a string naming the step, while the second \n",
      "element is the object that performs the transformation. Advanced usage of the object \n",
      "uses these names to refer to different steps.\n",
      "After normalization, every feature is in the same units (technically, every feature is \n",
      "now dimensionless; it has no units) and we can more confidently mix dimensions. \n",
      "In fact, if we now run our nearest neighbor classifier, we obtain 93 percent accuracy, \n",
      "estimated with the same five-fold cross-validation code shown previously!\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 67, 'page_label': '47'}\n",
      "[ 47 ]\n",
      "Look at the decision space again in two dimensions:\n",
      "The boundaries are now different and you can see that both dimensions  \n",
      "make a difference for the outcome. In the full dataset, everything is happening  \n",
      "on a seven-dimensional space, which is very hard to visualize, but the same  \n",
      "principle applies; while a few dimensions are dominant in the original data,  \n",
      "after normalization, they are all given the same importance.\n",
      "Binary and multiclass classification\n",
      "The first classifier we used, the threshold classifier, was a simple binary classifier. Its \n",
      "result is either one class or the other, as a point is either above the threshold value or \n",
      "it is not. The second classifier we used, the nearest neighbor classifier, was a natural \n",
      "multiclass classifier, its output can be one of the several classes.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 68, 'page_label': '48'}\n",
      "Classifying with Real-world Examples\n",
      "[  48 ]\n",
      "It is often simpler to define a simple binary method than the one that works on \n",
      "multiclass problems. However, we can reduce any multiclass problem to a series of \n",
      "binary decisions. This is what we did earlier in the Iris dataset, in a haphazard way: \n",
      "we observed that it was easy to separate one of the initial classes and focused on the \n",
      "other two, reducing the problem to two binary decisions:\n",
      "1. Is it an Iris Setosa (yes or no)?\n",
      "2. If not, check whether it is an Iris Virginica (yes or no).\n",
      "Of course, we want to leave this sort of reasoning to the computer. As usual, there \n",
      "are several solutions to this multiclass reduction.\n",
      "The simplest is to use a series of one versus the rest classifiers. For each possible \n",
      "label , we build a classifier of the type is this  or something else? When applying \n",
      "the rule, exactly one of the classifiers will say yes and we will have our solution. \n",
      "Unfortunately, this does not always happen, so we have to decide how to deal  \n",
      "with either multiple positive answers or no positive answers.\n",
      "Alternatively, we can build a classification tree. Split the possible labels into two, \n",
      "and build a classifier that asks, \"Should this example go in the left or the right \n",
      "bin?\" We can perform this splitting recursively until we obtain a single label. The \n",
      "preceding diagram depicts the tree of reasoning for the Iris dataset. Each diamond is \n",
      "a single binary classifier. It is easy to imagine that we could make this tree larger and \n",
      "encompass more decisions. This means that any classifier that can be used for binary \n",
      "classification can also be adapted to handle any number of classes in a simple way.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 69, 'page_label': '49'}\n",
      "[ 49 ]\n",
      "There are many other possible ways of turning a binary method into a multiclass one. \n",
      "There is no single method that is clearly better in all cases. The scikit-learn module \n",
      "implements several of these methods in the sklearn.multiclass submodule.\n",
      "Some classifiers are binary systems, while many real-life problems are \n",
      "naturally multiclass. Several simple protocols reduce a multiclass problem \n",
      "to a series of binary decisions and allow us to apply the binary models to \n",
      "our multiclass problem. This means methods that are apparently only for \n",
      "binary data can be applied to multiclass data with little extra effort.\n",
      "Summary\n",
      "Classification means generalizing from examples to build a model (that is, a rule \n",
      "that can automatically be applied to new, unclassified objects). It is one of the \n",
      "fundamental tools in machine learning and we will see many more examples  \n",
      "of this in the forthcoming chapters.\n",
      "In a sense, this was a very theoretical chapter, as we introduced generic concepts \n",
      "with simple examples. We went over a few operations with the Iris dataset. This is a \n",
      "small dataset. However, it has the advantage that we were able to plot it out and see \n",
      "what we were doing in detail. This is something that will be lost when we move on \n",
      "to problems with many dimensions and many thousands of examples. The intuitions \n",
      "we gained here will all still be valid.\n",
      "You also learned that the training error is a misleading, over-optimistic estimate of how \n",
      "well the model does. We must, instead, evaluate it on testing data that has not been \n",
      "used for training. In order to not waste too many examples in testing, a cross-validation \n",
      "schedule can get us the best of both worlds (at the cost of more computation).\n",
      "We also had a look at the problem of feature engineering. Features are not \n",
      "predefined for you, but choosing and designing features is an integral part of \n",
      "designing a machine learning pipeline. In fact, it is often the area where you can \n",
      "get the most improvements in accuracy, as better data beats fancier methods. The \n",
      "chapters on text-based classification, music genre recognition, and computer vision \n",
      "will provide examples for these specific settings.\n",
      "The next chapter looks at how to proceed when your data does not have predefined \n",
      "classes for classification.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 70, 'page_label': '50'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 71, 'page_label': '51'}\n",
      "[ 51 ]\n",
      "Clustering  Finding  \n",
      "Related Posts\n",
      "In the previous chapter, you learned how to find the classes or categories of \n",
      "individual datapoints. With a handful of training data items that were paired with \n",
      "their respective classes, you learned a model, which we can now use to classify \n",
      "future data items. We called this supervised learning because the learning was \n",
      "guided by a teacher; in our case, the teacher had the form of correct classifications.\n",
      "Let's now imagine that we do not possess those labels by which we can learn the \n",
      "classification model. This could be, for example, because they were too expensive to \n",
      "collect. Just imagine the cost if the only way to obtain millions of labels will be to ask \n",
      "humans to classify those manually. What could we have done in that case?\n",
      "Well, of course, we will not be able to learn a classification model. Still, we could find \n",
      "some pattern within the data itself. That is, let the data describe itself. This is what \n",
      "we will do in this chapter, where we consider the challenge of a question and answer \n",
      "website. When a user is browsing our site, perhaps because they were searching for \n",
      "particular information, the search engine will most likely point them to a specific \n",
      "answer. If the presented answers are not what they were looking for, the website \n",
      "should present (at least) the related answers so that they can quickly see what other \n",
      "answers are available and hopefully stay on our site.\n",
      "The nave approach will be to simply take the post, calculate its similarity to all  \n",
      "other posts and display the top n most similar posts as links on the page. Quickly, \n",
      "this will become very costly. Instead, we need a method that quickly finds all the \n",
      "related posts.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 72, 'page_label': '52'}\n",
      "Clustering  Finding Related Posts\n",
      "[  52 ]\n",
      "We will achieve this goal in this chapter using clustering. This is a method of \n",
      "arranging items so that similar items are in one cluster and dissimilar items are in \n",
      "distinct ones. The tricky thing that we have to tackle first is how to turn text into \n",
      "something on which we can calculate similarity. With such a similarity measurement, \n",
      "we will then proceed to investigate how we can leverage that to quickly arrive at a \n",
      "cluster that contains similar posts. Once there, we will only have to check out those \n",
      "documents that also belong to that cluster. To achieve this, we will introduce you to \n",
      "the marvelous SciKit library, which comes with diverse machine learning methods \n",
      "that we will also use in the following chapters.\n",
      "Measuring the relatedness of posts\n",
      "From the machine learning point of view, raw text is useless. Only if we manage to \n",
      "transform it into meaningful numbers, can we then feed it into our machine learning \n",
      "algorithms, such as clustering. This is true for more mundane operations on text such \n",
      "as similarity measurement.\n",
      "How not to do it\n",
      "One text similarity measure is the Levenshtein distance, which also goes by the name \n",
      "Edit Distance. Let's say we have two words, \"machine\" and \"mchiene\". The similarity \n",
      "between them can be expressed as the minimum set of edits that are necessary to turn \n",
      "one word into the other. In this case, the edit distance will be 2, as we have to add an \n",
      "\"a\" after the \"m\" and delete the first \"e\". This algorithm is, however, quite costly as it is \n",
      "bound by the length of the first word times the length of the second word.\n",
      "Looking at our posts, we could cheat by treating whole words as characters and \n",
      "performing the edit distance calculation on the word level. Let's say we have two \n",
      "posts (let's concentrate on the following title, for simplicity's sake) called \"How \n",
      "to format my hard disk\" and \"Hard disk format problems\", we will need an edit \n",
      "distance of 5 because of removing \"how\", \"to\", \"format\", \"my\" and then adding \n",
      "\"format\" and \"problems\" in the end. Thus, one could express the difference between \n",
      "two posts as the number of words that have to be added or deleted so that one text \n",
      "morphs into the other. Although we could speed up the overall approach quite a bit, \n",
      "the time complexity remains the same.\n",
      "But even if it would have been fast enough, there is another problem. In the earlier \n",
      "post, the word \"format\" accounts for an edit distance of 2, due to deleting it first, then \n",
      "adding it. So, our distance seems to be not robust enough to take word reordering \n",
      "into account.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 73, 'page_label': '53'}\n",
      "[ 53 ]\n",
      "How to do it\n",
      "More robust than edit distance is the so-called bag of word approach. It totally \n",
      "ignores the order of words and simply uses word counts as their basis. For each \n",
      "word in the post, its occurrence is counted and noted in a vector. Not surprisingly, \n",
      "this step is also called vectorization. The vector is typically huge as it contains as \n",
      "many elements as words occur in the whole dataset. Take, for instance, two example \n",
      "posts with the following word counts:\n",
      "Word Occurrences in post 1 Occurrences in post 2\n",
      "disk 1 1\n",
      "format 1 1\n",
      "how 1 0\n",
      "hard 1 1\n",
      "my 1 0\n",
      "problems 0 1\n",
      "to 1 0\n",
      "The columns Occurrences in post 1 and Occurrences in post 2 can now be treated as \n",
      "simple vectors. We can simply calculate the Euclidean distance between the vectors \n",
      "of all posts and take the nearest one (too slow, as we have found out earlier). And as \n",
      "such, we can use them later as our feature vectors in the clustering steps according to \n",
      "the following procedure:\n",
      "1. Extract salient features from each post and store it as a vector per post.\n",
      "2. Then compute clustering on the vectors.\n",
      "3. Determine the cluster for the post in question.\n",
      "4. From this cluster, fetch a handful of posts having a different similarity to the \n",
      "post in question. This will increase diversity.\n",
      "But there is some more work to be done before we get there. Before we can do that \n",
      "work, we need some data to work on.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 74, 'page_label': '54'}\n",
      "Clustering  Finding Related Posts\n",
      "[  54 ]\n",
      "Preprocessing  similarity measured as a \n",
      "similar number of common words\n",
      "As we have seen earlier, the bag of word approach is both fast and robust. It is, \n",
      "though, not without challenges. Let's dive directly into them.\n",
      "Converting raw text into a bag of words\n",
      "We do not have to write custom code for counting words and representing those \n",
      "counts as a vector. SciKit's \n",
      "CountVectorizer method does the job not only efficiently \n",
      "but also has a very convenient interface. SciKit's functions and classes are imported \n",
      "via the \n",
      "sklearn package:\n",
      ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
      ">>> vectorizer = CountVectorizer(min_df=1)\n",
      "The min_df parameter determines how CountVectorizer treats seldom words \n",
      "(minimum document frequency). If it is set to an integer, all words occurring less \n",
      "than that value will be dropped. If it is a fraction, all words that occur in less than \n",
      "that fraction of the overall dataset will be dropped. The max_df parameter works \n",
      "in a similar manner. If we print the instance, we see what other parameters SciKit \n",
      "provides together with their default values:\n",
      ">>> print(vectorizer) \n",
      "CountVectorizer(analyzer='word', binary=False, charset=None,\n",
      "        charset_error=None, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8',  \n",
      "input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "We see that, as expected, the counting is done at word level (analyzer=word) and \n",
      "words are determined by the regular expression pattern token_pattern. It will, \n",
      "for example, tokenize \"cross-validated\" into \"cross\" and \"validated\". Let's ignore the \n",
      "other parameters for now and consider the following two example subject lines:\n",
      ">>> content = [\"How to format my hard disk\", \" Hard disk format  \n",
      "problems \"]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 75, 'page_label': '55'}\n",
      "[ 55 ]\n",
      "We can now put this list of subject lines into the fit_transform() function of our \n",
      "vectorizer, which does all the hard vectorization work.\n",
      ">>> X = vectorizer.fit_transform(content)\n",
      ">>> vectorizer.get_feature_names() \n",
      "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']\n",
      "The vectorizer has detected seven words for which we can fetch the counts individually:\n",
      ">>> print(X.toarray().transpose())\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "This means that the first sentence contains all the words except \"problems\", while \n",
      "the second contains all but \"how\", \"my\", and \"to\". In fact, these are exactly the same \n",
      "columns as we have seen in the preceding table. From X, we can extract a feature \n",
      "vector that we will use to compare two documents with each other.\n",
      "We will start with a nave approach first, to point out some preprocessing \n",
      "peculiarities we have to account for. So let's pick a random post, for which we then \n",
      "create the count vector. We will then compare its distance to all the count vectors and \n",
      "fetch the post with the smallest one.\n",
      "Counting words\n",
      "Let's play with the toy dataset consisting of the following posts:\n",
      "Post filename Post content\n",
      "01.txt This is a toy post about machine learning. Actually, it contains not much \n",
      "interesting stuff.\n",
      "02.txt Imaging databases can get huge.\n",
      "03.txt Most imaging databases save images permanently.\n",
      "04.txt Imaging databases store images.\n",
      "05.txt Imaging databases store images. Imaging databases store images. \n",
      "Imaging databases store images.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 76, 'page_label': '56'}\n",
      "Clustering  Finding Related Posts\n",
      "[  56 ]\n",
      "In this post dataset, we want to find the most similar post for the short post  \n",
      "\"imaging databases\".\n",
      "Assuming that the posts are located in the directory DIR, we can feed \n",
      "CountVectorizer with it:\n",
      ">>> posts = [open(os.path.join(DIR, f)).read() for f in  \n",
      "os.listdir(DIR)]\n",
      ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
      ">>> vectorizer = CountVectorizer(min_df=1)\n",
      "We have to notify the vectorizer about the full dataset so that it knows upfront what \n",
      "words are to be expected:\n",
      ">>> X_train = vectorizer.fit_transform(posts)\n",
      ">>> num_samples, num_features = X_train.shape\n",
      ">>> print(\"#samples: %d, #features: %d\" % (num_samples,  \n",
      "num_features))\n",
      "#samples: 5, #features: 25\n",
      "Unsurprisingly, we have five posts with a total of 25 different words. The following \n",
      "words that have been tokenized will be counted:\n",
      ">>> print(vectorizer.get_feature_names()) \n",
      "[u'about', u'actually', u'capabilities', u'contains', u'data',  \n",
      "u'databases', u'images', u'imaging', u'interesting', u'is', u'it',  \n",
      "u'learning', u'machine', u'most', u'much', u'not', u'permanently',  \n",
      "u'post', u'provide', u'save', u'storage', u'store', u'stuff',  \n",
      "u'this', u'toy']\n",
      "Now we can vectorize our new post.\n",
      ">>> new_post = \"imaging databases\"\n",
      ">>> new_post_vec = vectorizer.transform([new_post])\n",
      "Note that the count vectors returned by the transform method are sparse. That is, \n",
      "each vector does not store one count value for each word, as most of those counts \n",
      "will be zero (the post does not contain the word). Instead, it uses the more memory-\n",
      "efficient implementation coo_matrix (for \"COOrdinate\"). Our new post, for instance, \n",
      "actually contains only two elements:\n",
      ">>> print(new_post_vec)\n",
      "  (0, 7)  1\n",
      "  (0, 5)  1\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 77, 'page_label': '57'}\n",
      "[ 57 ]\n",
      "Via its toarray() member, we can once again access the full ndarray:\n",
      ">>> print(new_post_vec.toarray())\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "We need to use the full array, if we want to use it as a vector for similarity \n",
      "calculations. For the similarity measurement (the nave one), we calculate the \n",
      "Euclidean distance between the count vectors of the new post and all the old posts:\n",
      ">>> import scipy as sp\n",
      ">>> def dist_raw(v1, v2):\n",
      "...     delta = v1-v2\n",
      "...     return sp.linalg.norm(delta.toarray())\n",
      "The norm() function calculates the Euclidean norm (shortest distance). This is just one \n",
      "obvious first pick and there are many more interesting ways to calculate the distance. \n",
      "Just take a look at the paper Distance Coefficients between Two Lists or Sets in The Python \n",
      "Papers Source Codes, in which Maurice Ling nicely presents 35 different ones.\n",
      "With dist_raw, we just need to iterate over all the posts and remember the  \n",
      "nearest one:\n",
      ">>> import sys\n",
      ">>> best_doc = None\n",
      ">>> best_dist = sys.maxint\n",
      ">>> best_i = None\n",
      ">>> for i, post in enumerate(num_samples):\n",
      "...     if post == new_post:\n",
      "...         continue\n",
      "...     post_vec = X_train.getrow(i)\n",
      "...     d = dist_raw(post_vec, new_post_vec)\n",
      "...     print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
      "...     if d<best_dist:\n",
      "...         best_dist = d\n",
      "...         best_i = i\n",
      ">>> print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n",
      "=== Post 0 with dist=4.00: This is a toy post about machine learning.  \n",
      "Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage  \n",
      "capabilities.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 78, 'page_label': '58'}\n",
      "Clustering  Finding Related Posts\n",
      "[  58 ]\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images  \n",
      "permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging  \n",
      "databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.41\n",
      "Congratulations, we have our first similarity measurement. Post 0 is most dissimilar \n",
      "from our new post. Quite understandably, it does not have a single word in common \n",
      "with the new post. We can also understand that Post 1 is very similar to the new \n",
      "post, but not the winner, as it contains one word more than Post 3, which is not \n",
      "contained in the new post.\n",
      "Looking at Post 3 and Post 4, however, the picture is not so clear any more. Post 4 is \n",
      "the same as Post 3 duplicated three times. So, it should also be of the same similarity \n",
      "to the new post as Post 3.\n",
      "Printing the corresponding feature vectors explains why:\n",
      ">>> print(X_train.getrow(3).toarray())\n",
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      ">>> print(X_train.getrow(4).toarray())\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n",
      "Obviously, using only the counts of the raw words is too simple. We will have to \n",
      "normalize them to get vectors of unit length.\n",
      "Normalizing word count vectors\n",
      "We will have to extend dist_raw to calculate the vector distance not on the raw \n",
      "vectors but on the normalized instead:\n",
      ">>> def dist_norm(v1, v2):\n",
      "...    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
      "...    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
      "...    delta = v1_normalized - v2_normalized\n",
      "...    return sp.linalg.norm(delta.toarray())\n",
      "This leads to the following similarity measurement:\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning.  \n",
      "Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage  \n",
      "capabilities.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 79, 'page_label': '59'}\n",
      "[ 59 ]\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images  \n",
      "permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging  \n",
      "databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n",
      "This looks a bit better now. Post 3 and Post 4 are calculated as being equally similar. \n",
      "One could argue whether that much repetition would be a delight to the reader, but \n",
      "from the point of counting the words in the posts this seems to be right.\n",
      "Removing less important words\n",
      "Let's have another look at Post 2. Of its words that are not in the new post, we have \n",
      "\"most\", \"save\", \"images\", and \"permanently\". They are actually quite different in the \n",
      "overall importance to the post. Words such as \"most\" appear very often in all sorts of \n",
      "different contexts and are called stop words. They do not carry as much information \n",
      "and thus should not be weighed as much as words such as \"images\", which doesn't \n",
      "occur often in different contexts. The best option would be to remove all the words \n",
      "that are so frequent that they do not help to distinguish between different texts. \n",
      "These words are called stop words.\n",
      "As this is such a common step in text processing, there is a simple parameter in \n",
      "CountVectorizer to achieve that:\n",
      ">>> vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
      "If you have a clear picture of what kind of stop words you would want to remove, \n",
      "you can also pass a list of them. Setting stop_words to english will use a set of  \n",
      "318 English stop words. To find out which ones, you can use get_stop_words():\n",
      ">>> sorted(vectorizer.get_stop_words())[0:20]\n",
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again',  \n",
      "'against', 'all', 'almost', 'alone', 'along', 'already', 'also',  \n",
      "'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n",
      "The new word list is seven words lighter:\n",
      "[u'actually', u'capabilities', u'contains', u'data', u'databases',  \n",
      "u'images', u'imaging', u'interesting', u'learning', u'machine',  \n",
      "u'permanently', u'post', u'provide', u'save', u'storage', u'store',  \n",
      "u'stuff', u'toy']\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 80, 'page_label': '60'}\n",
      "Clustering  Finding Related Posts\n",
      "[  60 ]\n",
      "Without stop words, we arrive at the following similarity measurement:\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning.  \n",
      "Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage  \n",
      "capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images  \n",
      "permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging  \n",
      "databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n",
      "Post 2 is now on par with Post 1. It has, however, changed not much overall since \n",
      "our posts are kept short for demonstration purposes. It will become vital when we \n",
      "look at real-world data.\n",
      "Stemming\n",
      "One thing is still missing. We count similar words in different variants as different \n",
      "words. Post 2, for instance, contains \"imaging\" and \"images\". It will make sense to \n",
      "count them together. After all, it is the same concept they are referring to.\n",
      "We need a function that reduces words to their specific word stem. SciKit does not \n",
      "contain a stemmer by default. With the Natural Language Toolkit (NLTK), we can \n",
      "download a free software toolkit, which provides a stemmer that we can easily plug \n",
      "into CountVectorizer.\n",
      "Installing and using NLTK\n",
      "How to install NLTK on your operating system is described in detail at http://\n",
      "nltk.org/install.html. Unfortunately, it is not yet officially supported for Python \n",
      "3, which means that also pip install will not work. We can, however, download the \n",
      "package from http://www.nltk.org/nltk3-alpha/ and install it manually after \n",
      "uncompressing using Python's setup.py install.\n",
      "To check whether your installation was successful, open a Python interpreter  \n",
      "and type:\n",
      ">>> import nltk\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 81, 'page_label': '61'}\n",
      "[ 61 ]\n",
      "You will find a very nice tutorial to NLTK in the book Python 3 Text \n",
      "Processing with NLTK 3 Cookbook, Jacob Perkins, Packt Publishing. To \n",
      "play a little bit with a stemmer, you can visit the web page http://\n",
      "text-processing.com/demo/stem/.\n",
      "NLTK comes with different stemmers. This is necessary, because every language has \n",
      "a different set of rules for stemming. For English, we can take SnowballStemmer.\n",
      ">>> import nltk.stem\n",
      ">>> s = nltk.stem.SnowballStemmer('english')\n",
      ">>> s.stem(\"graphics\")\n",
      "u'graphic'\n",
      ">>> s.stem(\"imaging\")\n",
      "u'imag'\n",
      ">>> s.stem(\"image\")\n",
      "u'imag'\n",
      ">>> s.stem(\"imagination\")\n",
      "u'imagin'\n",
      ">>> s.stem(\"imagine\")\n",
      "u'imagin'\n",
      "Note that stemming does not necessarily have to result in \n",
      "valid English words.\n",
      "It also works with verbs:\n",
      ">>> s.stem(\"buys\")\n",
      "u'buy'\n",
      ">>> s.stem(\"buying\")\n",
      "u'buy'\n",
      "This means, it works most of the time:\n",
      ">>> s.stem(\"bought\")\n",
      "u'bought'\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 82, 'page_label': '62'}\n",
      "Clustering  Finding Related Posts\n",
      "[  62 ]\n",
      "Extending the vectorizer with NLTK's stemmer\n",
      "We need to stem the posts before we feed them into CountVectorizer. The class \n",
      "provides several hooks with which we can customize the stage's preprocessing \n",
      "and tokenization. The preprocessor and tokenizer can be set as parameters in the \n",
      "constructor. We do not want to place the stemmer into any of them, because we \n",
      "will then have to do the tokenization and normalization by ourselves. Instead, we \n",
      "overwrite the build_analyzer method:\n",
      ">>> import nltk.stem\n",
      ">>> english_stemmer = nltk.stem.SnowballStemmer('english'))\n",
      ">>> class StemmedCountVectorizer(CountVectorizer):\n",
      "...     def build_analyzer(self):\n",
      "...         analyzer = super(StemmedCountVectorizer,  \n",
      "self).build_analyzer()\n",
      "...         return lambda doc: (english_stemmer.stem(w) for w in  \n",
      "analyzer(doc))\n",
      ">>> vectorizer = StemmedCountVectorizer(min_df=1,  \n",
      "stop_words='english')\n",
      "This will do the following process for each post:\n",
      "1. The first step is lower casing the raw post in the preprocessing step  \n",
      "(done in the parent class).\n",
      "2. Extracting all individual words in the tokenization step (done in the  \n",
      "parent class).\n",
      "3. This concludes with converting each word into its stemmed version.\n",
      "As a result, we now have one feature less, because \"images\" and \"imaging\" collapsed \n",
      "to one. Now, the set of feature names is as follows:\n",
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag',  \n",
      "u'interest', u'learn', u'machin', u'perman', u'post', u'provid',  \n",
      "u'save', u'storag', u'store', u'stuff', u'toy']\n",
      "Running our new stemmed vectorizer over our posts, we see that collapsing \n",
      "\"imaging\" and \"images\", revealed that actually Post 2 is the most similar post to our \n",
      "new post, as it contains the concept \"imag\" twice:\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning.  \n",
      "Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage  \n",
      "capabilities.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 83, 'page_label': '63'}\n",
      "[ 63 ]\n",
      "=== Post 2 with dist=0.63: Most imaging databases save images  \n",
      "permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging  \n",
      "databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n",
      "Stop words on steroids\n",
      "Now that we have a reasonable way to extract a compact vector from a noisy textual \n",
      "post, let's step back for a while to think about what the feature values actually mean.\n",
      "The feature values simply count occurrences of terms in a post. We silently assumed \n",
      "that higher values for a term also mean that the term is of greater importance to the \n",
      "given post. But what about, for instance, the word \"subject\", which naturally occurs \n",
      "in each and every single post? Alright, we can tell CountVectorizer to remove it \n",
      "as well by means of its max_df parameter. We can, for instance, set it to 0.9 so that \n",
      "all words that occur in more than 90 percent of all posts will always be ignored. \n",
      "But, what about words that appear in 89 percent of all posts? How low will we be \n",
      "willing to set max_df? The problem is that however we set it, there will always be the \n",
      "problem that some terms are just more discriminative than others.\n",
      "This can only be solved by counting term frequencies for every post and in addition \n",
      "discount those that appear in many posts. In other words, we want a high value for a \n",
      "given term in a given value, if that term occurs often in that particular post and very \n",
      "seldom anywhere else.\n",
      "This is exactly what term frequency  inverse document frequency (TF-IDF) \n",
      "does. TF stands for the counting part, while IDF factors in the discounting. A nave \n",
      "implementation will look like this:\n",
      ">>> import scipy as sp\n",
      ">>> def tfidf(term, doc, corpus):\n",
      "...     tf = doc.count(term) / len(doc)\n",
      "...     num_docs_with_term = len([d for d in corpus if term in d])\n",
      "...     idf = sp.log(len(corpus) / num_docs_with_term)\n",
      "...     return tf * idf\n",
      "You see that we did not simply count the terms, but also normalize the counts by the \n",
      "document length. This way, longer documents do not have an unfair advantage over \n",
      "shorter ones.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 84, 'page_label': '64'}\n",
      "Clustering  Finding Related Posts\n",
      "[  64 ]\n",
      "For the following documents, D, consisting of three already tokenized documents,  \n",
      "we can see how the terms are treated differently, although all appear equally often \n",
      "per document:\n",
      ">>> a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
      ">>> D = [a, abb, abc]\n",
      ">>> print(tfidf(\"a\", a, D))\n",
      "0.0\n",
      ">>> print(tfidf(\"a\", abb, D))\n",
      "0.0\n",
      ">>> print(tfidf(\"a\", abc, D))\n",
      "0.0\n",
      ">>> print(tfidf(\"b\", abb, D))\n",
      "0.270310072072\n",
      ">>> print(tfidf(\"a\", abc, D))\n",
      "0.0\n",
      ">>> print(tfidf(\"b\", abc, D))\n",
      "0.135155036036\n",
      ">>> print(tfidf(\"c\", abc, D))\n",
      "0.366204096223\n",
      "We see that a carries no meaning for any document since it is contained everywhere. \n",
      "The b term is more important for the document abb than for abc as it occurs there twice.\n",
      "In reality, there are more corner cases to handle than the preceding example does. \n",
      "Thanks to SciKit, we don't have to think of them as they are already nicely packaged \n",
      "in TfidfVectorizer, which is inherited from CountVectorizer. Sure enough, we \n",
      "don't want to miss our stemmer:\n",
      ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      ">>> class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "...     def build_analyzer(self):\n",
      "...         analyzer = super(TfidfVectorizer,\n",
      "                             self).build_analyzer()\n",
      "...         return lambda doc: (\n",
      "                english_stemmer.stem(w) for w in analyzer(doc))\n",
      ">>> vectorizer = StemmedTfidfVectorizer(min_df=1,\n",
      "                    stop_words='english', decode_error='ignore')\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 85, 'page_label': '65'}\n",
      "[ 65 ]\n",
      "The resulting document vectors will not contain counts any more. Instead they will \n",
      "contain the individual TF-IDF values per term.\n",
      "Our achievements and goals\n",
      "Our current text pre-processing phase includes the following steps:\n",
      "1. Firstly, tokenizing the text.\n",
      "2. This is followed by throwing away words that occur way too often to be of \n",
      "any help in detecting relevant posts.\n",
      "3. Throwing away words that occur way so seldom so that there is only little \n",
      "chance that they occur in future posts.\n",
      "4. Counting the remaining words.\n",
      "5. Finally, calculating TF-IDF values from the counts, considering the whole \n",
      "text corpus.\n",
      "Again, we can congratulate ourselves. With this process, we are able to convert a \n",
      "bunch of noisy text into a concise representation of feature values.\n",
      "But, as simple and powerful the bag of words approach with its extensions is, it has \n",
      "some drawbacks, which we should be aware of:\n",
      " It does not cover word relations: With the aforementioned vectorization \n",
      "approach, the text \"Car hits wall\" and \"Wall hits car\" will both have the  \n",
      "same feature vector.\n",
      " It does not capture negations correctly: For instance, the text \"I will eat \n",
      "ice cream\" and \"I will not eat ice cream\" will look very similar by means of \n",
      "their feature vectors although they contain quite the opposite meaning. This \n",
      "problem, however, can be easily changed by not only counting individual \n",
      "words, also called \"unigrams\", but instead also considering bigrams (pairs of \n",
      "words) or trigrams (three words in a row).\n",
      " It totally fails with misspelled words: Although it is clear to the human \n",
      "beings among us readers that \"database\" and \"databas\" convey the same \n",
      "meaning, our approach will treat them as totally different words.\n",
      "For brevity's sake, let's nevertheless stick with the current approach, which we can \n",
      "now use to efficiently build clusters from.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 86, 'page_label': '66'}\n",
      "Clustering  Finding Related Posts\n",
      "[  66 ]\n",
      "Clustering\n",
      "Finally, we have our vectors, which we believe capture the posts to a sufficient degree. \n",
      "Not surprisingly, there are many ways to group them together. Most clustering \n",
      "algorithms fall into one of the two methods: flat and hierarchical clustering.\n",
      "Flat clustering divides the posts into a set of clusters without relating the clusters to \n",
      "each other. The goal is simply to come up with a partitioning such that all posts in \n",
      "one cluster are most similar to each other while being dissimilar from the posts in all \n",
      "other clusters. Many flat clustering algorithms require the number of clusters to be \n",
      "specified up front.\n",
      "In hierarchical clustering, the number of clusters does not have to be specified. \n",
      "Instead, hierarchical clustering creates a hierarchy of clusters. While similar posts \n",
      "are grouped into one cluster, similar clusters are again grouped into one uber-cluster. \n",
      "This is done recursively, until only one cluster is left that contains everything. In \n",
      "this hierarchy, one can then choose the desired number of clusters after the fact. \n",
      "However, this comes at the cost of lower efficiency.\n",
      "SciKit provides a wide range of clustering approaches in the sklearn.cluster \n",
      "package. You can get a quick overview of advantages and drawbacks of each of  \n",
      "them at http://scikit-learn.org/dev/modules/clustering.html.\n",
      "In the following sections, we will use the flat clustering method K-means and play a \n",
      "bit with the desired number of clusters.\n",
      "K-means\n",
      "k-means is the most widely used flat clustering algorithm. After initializing it with \n",
      "the desired number of clusters, num_clusters, it maintains that number of so-called \n",
      "cluster centroids. Initially, it will pick any num_clusters posts and set the centroids \n",
      "to their feature vector. Then it will go through all other posts and assign them the \n",
      "nearest centroid as their current cluster. Following this, it will move each centroid \n",
      "into the middle of all the vectors of that particular class. This changes, of course, the \n",
      "cluster assignment. Some posts are now nearer to another cluster. So it will update \n",
      "the assignments for those changed posts. This is done as long as the centroids move \n",
      "considerably. After some iterations, the movements will fall below a threshold and \n",
      "we consider clustering to be converged.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 87, 'page_label': '67'}\n",
      "[ 67 ]\n",
      "Let's play this through with a toy example of posts containing only two words. Each \n",
      "point in the following chart represents one document:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 88, 'page_label': '68'}\n",
      "Clustering  Finding Related Posts\n",
      "[  68 ]\n",
      "After running one iteration of K-means, that is, taking any two vectors as starting \n",
      "points, assigning the labels to the rest and updating the cluster centers to now be the \n",
      "center point of all points in that cluster, we get the following clustering:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 89, 'page_label': '69'}\n",
      "[ 69 ]\n",
      "Because the cluster centers moved, we have to reassign the cluster labels and \n",
      "recalculate the cluster centers. After iteration 2, we get the following clustering:\n",
      "The arrows show the movements of the cluster centers. After five iterations in this \n",
      "example, the cluster centers don't move noticeably any more (SciKit's tolerance \n",
      "threshold is 0.0001 by default).\n",
      "After the clustering has settled, we just need to note down the cluster centers and \n",
      "their identity. Each new document that comes in, we then have to vectorize and \n",
      "compare against all cluster centers. The cluster center with the smallest distance to \n",
      "our new post vector belongs to the cluster we will assign to the new post.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 90, 'page_label': '70'}\n",
      "Clustering  Finding Related Posts\n",
      "[  70 ]\n",
      "Getting test data to evaluate our ideas on\n",
      "In order to test clustering, let's move away from the toy text examples and find a \n",
      "dataset that resembles the data we are expecting in the future so that we can test \n",
      "our approach. For our purpose, we need documents about technical topics that are \n",
      "already grouped together so that we can check whether our algorithm works as \n",
      "expected when we apply it later to the posts we hope to receive.\n",
      "One standard dataset in machine learning is the 20newsgroup dataset, which \n",
      "contains 18,826 posts from 20 different newsgroups. Among the groups' topics are \n",
      "technical ones such as comp.sys.mac.hardware or sci.crypt, as well as more \n",
      "politics- and religion-related ones such as talk.politics.guns or soc.religion.\n",
      "christian. We will restrict ourselves to the technical groups. If we assume each \n",
      "newsgroup as one cluster, we can nicely test whether our approach of finding related \n",
      "posts works.\n",
      "The dataset can be downloaded from http://people.csail.mit.edu/\n",
      "jrennie/20Newsgroups. Much more comfortable, however, is to download it \n",
      "from MLComp at http://mlcomp.org/datasets/379 (free registration required). \n",
      "SciKit already contains custom loaders for that dataset and rewards you with very \n",
      "convenient data loading options.\n",
      "The dataset comes in the form of a ZIP file dataset-379-20news-18828_WJQIG.zip, \n",
      "which we have to unzip to get the directory 379, which contains the datasets. We \n",
      "also have to notify SciKit about the path containing that data directory. It contains \n",
      "a metadata file and three directories test, train, and raw. The test and train \n",
      "directories split the whole dataset into 60 percent of training and 40 percent of testing \n",
      "posts. If you go this route, then you either need to set the environment variable \n",
      "MLCOMP_DATASETS_HOME or you specify the path directly with the mlcomp_root \n",
      "parameter when loading the dataset.\n",
      "http://mlcomp.org is a website for comparing machine learning \n",
      "programs on diverse datasets. It serves two purposes: finding the \n",
      "right dataset to tune your machine learning program, and exploring \n",
      "how other people use a particular dataset. For instance, you can see \n",
      "how well other people's algorithms performed on particular datasets \n",
      "and compare against them.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 91, 'page_label': '71'}\n",
      "[ 71 ]\n",
      "For convenience, the sklearn.datasets module also contains the \n",
      "fetch_20newsgroups function, which automatically downloads the  \n",
      "data behind the scenes:\n",
      ">>> import sklearn.datasets\n",
      ">>> all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n",
      ">>> print(len(all_data.filenames))\n",
      "18846\n",
      ">>> print(all_data.target_names)\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',  \n",
      "'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',  \n",
      "'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',  \n",
      "'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',  \n",
      "'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian',  \n",
      "'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc',  \n",
      "'talk.religion.misc']\n",
      "We can choose between training and test sets:\n",
      ">>> train_data = sklearn.datasets.fetch_20newsgroups(subset='train',  \n",
      "categories=groups)\n",
      ">>> print(len(train_data.filenames))\n",
      "11314\n",
      ">>> test_data = sklearn.datasets.fetch_20newsgroups(subset='test')\n",
      ">>> print(len(test_data.filenames))\n",
      "7532\n",
      "For simplicity's sake, we will restrict ourselves to only some newsgroups so  \n",
      "that the overall experimentation cycle is shorter. We can achieve this with the \n",
      "categories parameter:\n",
      ">>> groups = ['comp.graphics', 'comp.os.ms-windows.misc',  \n",
      "'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',  \n",
      "'comp.windows.x', 'sci.space']\n",
      ">>> train_data = sklearn.datasets.fetch_20newsgroups(subset='train',  \n",
      "categories=groups)\n",
      ">>> print(len(train_data.filenames))\n",
      "3529\n",
      ">>> test_data = sklearn.datasets.fetch_20newsgroups(subset='test',  \n",
      "categories=groups)\n",
      ">>> print(len(test_data.filenames))\n",
      "2349\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 92, 'page_label': '72'}\n",
      "Clustering  Finding Related Posts\n",
      "[  72 ]\n",
      "Clustering posts\n",
      "You would have already noticed one thingreal data is noisy. The newsgroup \n",
      "dataset is no exception. It even contains invalid characters that will result in \n",
      "UnicodeDecodeError.\n",
      "We have to tell the vectorizer to ignore them:\n",
      ">>> vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
      "...              stop_words='english', decode_error='ignore')\n",
      ">>> vectorized = vectorizer.fit_transform(train_data.data)\n",
      ">>> num_samples, num_features = vectorized.shape\n",
      ">>> print(\"#samples: %d, #features: %d\" % (num_samples,  \n",
      "num_features))\n",
      "#samples: 3529, #features: 4712\n",
      "We now have a pool of 3,529 posts and extracted for each of them a feature vector  \n",
      "of 4,712 dimensions. That is what K-means takes as input. We will fix the cluster size \n",
      "to 50 for this chapter and hope you are curious enough to try out different values as \n",
      "an exercise.\n",
      ">>> num_clusters = 50\n",
      ">>> from sklearn.cluster import KMeans\n",
      ">>> km = KMeans(n_clusters=num_clusters, init='random', n_init=1,\n",
      "verbose=1, random_state=3)\n",
      ">>> km.fit(vectorized)\n",
      "That's it. We provided a random state just so that you can get the same results. In \n",
      "real-world applications, you will not do this. After fitting, we can get the clustering \n",
      "information out of members of km. For every vectorized post that has been fit, there is \n",
      "a corresponding integer label in km.labels_:\n",
      ">>> print(km.labels_)\n",
      "[48 23 31 ...,  6  2 22]\n",
      ">>> print(km.labels_.shape)\n",
      "3529\n",
      "The cluster centers can be accessed via km.cluster_centers_.\n",
      "In the next section, we will see how we can assign a cluster to a newly arriving post \n",
      "using km.predict.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 93, 'page_label': '73'}\n",
      "[ 73 ]\n",
      "Solving our initial challenge\n",
      "We will now put everything together and demonstrate our system for the following \n",
      "new post that we assign to the new_post variable:\n",
      "\"Disk drive problems. Hi, I have a problem with my hard disk.\n",
      "After 1 year it is working only sporadically now.\n",
      "I tried to format it, but now it doesn't boot any more.\n",
      "Any ideas? Thanks.\"\n",
      "As you learned earlier, you will first have to vectorize this post before you predict  \n",
      "its label:\n",
      ">>> new_post_vec = vectorizer.transform([new_post])\n",
      ">>> new_post_label = km.predict(new_post_vec)[0]\n",
      "Now that we have the clustering, we do not need to compare new_post_vec to all \n",
      "post vectors. Instead, we can focus only on the posts of the same cluster. Let's fetch \n",
      "their indices in the original data set:\n",
      ">>> similar_indices = (km.labels_==new_post_label).nonzero()[0]\n",
      "The comparison in the bracket results in a Boolean array, and nonzero converts that \n",
      "array into a smaller array containing the indices of the True elements.\n",
      "Using similar_indices, we then simply have to build a list of posts together with \n",
      "their similarity scores:\n",
      ">>> similar = []\n",
      ">>> for i in similar_indices:\n",
      "...    dist = sp.linalg.norm((new_post_vec -  \n",
      "vectorized[i]).toarray())\n",
      "...    similar.append((dist, dataset.data[i]))\n",
      ">>> similar = sorted(similar)\n",
      ">>> print(len(similar))\n",
      "131\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 94, 'page_label': '74'}\n",
      "Clustering  Finding Related Posts\n",
      "[  74 ]\n",
      "We found 131 posts in the cluster of our post. To give the user a quick idea of what \n",
      "kind of similar posts are available, we can now present the most similar post (show_\n",
      "at_1), and two less similar but still related ones  all from the same cluster.\n",
      ">>> show_at_1 = similar[0]\n",
      ">>> show_at_2 = similar[int(len(similar)/10)]\n",
      ">>> show_at_3 = similar[int(len(similar)/2)]\n",
      "The following table shows the posts together with their similarity values:\n",
      "Position Similarity Excerpt from post\n",
      "1 1.038 BOOT PROBLEM with IDE controller\n",
      "Hi,\n",
      "I've got a Multi I/O card (IDE controller + serial/parallel interface) \n",
      "and two floppy drives (5 1/4, 3 1/2) and a Quantum ProDrive \n",
      "80AT connected to it. I was able to format the hard disk, but I \n",
      "could not boot from it. I can boot from drive A: (which disk drive \n",
      "does not matter) but if I remove the disk from drive A and press \n",
      "the reset switch, the LED of drive A: continues to glow, and the \n",
      "hard disk is not accessed at all. I guess this must be a problem of \n",
      "either the Multi I/o card or floppy disk drive settings (jumper \n",
      "configuration?) Does someone have any hint what could be the \n",
      "reason for it. []\n",
      "2 1.150 Booting from B drive\n",
      "I have a 5 1/4\" drive as drive A. How can I make the system boot \n",
      "from my 3 1/2\" B drive? (Optimally, the computer would be able \n",
      "to boot: from either A or B, checking them in order for a bootable \n",
      "disk. But: if I have to switch cables around and simply switch the \n",
      "drives so that: it can't boot 5 1/4\" disks, that's OK. Also, boot_b \n",
      "won't do the trick for me. []\n",
      " []\n",
      "3 1.280 IBM PS/1 vs TEAC FD\n",
      "Hello, I already tried our national news group without success. I \n",
      "tried to replace a friend s original IBM floppy disk in his PS/1-PC \n",
      "with a normal TEAC drive. I already identified the power supply \n",
      "on pins 3 (5V) and 6 (12V), shorted pin 6 (5.25\"/3.5\" switch) and \n",
      "inserted pullup resistors (2K2) on pins 8, 26, 28, 30, and 34. The \n",
      "computer doesn't complain about a missing FD, but the FD s light \n",
      "stays on all the time. The drive spins up o.k. when I insert a disk, \n",
      "but I can't access it. The TEAC works fine in a normal PC. Are \n",
      "there any points I missed? []\n",
      " []\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 95, 'page_label': '75'}\n",
      "[ 75 ]\n",
      "It is interesting how the posts reflect the similarity measurement score. The first post \n",
      "contains all the salient words from our new post. The second also revolves around \n",
      "booting problems, but is about floppy disks and not hard disks. Finally, the third \n",
      "is neither about hard disks, nor about booting problems. Still, of all the posts, we \n",
      "would say that they belong to the same domain as the new post.\n",
      "Another look at noise\n",
      "We should not expect a perfect clustering in the sense that posts from the same \n",
      "newsgroup (for example, comp.graphics) are also clustered together. An example \n",
      "will give us a quick impression of the noise that we have to expect. For the sake of \n",
      "simplicity, we will focus on one of the shorter posts:\n",
      ">>> post_group = zip(train_data.data, train_data.target)\n",
      ">>> all = [(len(post[0]), post[0], train_data.target_names[post[1]])  \n",
      "for post in post_group]\n",
      ">>> graphics = sorted([post for post in all if  \n",
      "post[2]=='comp.graphics'])\n",
      ">>> print(graphics[5])\n",
      "(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject:  \n",
      "test....(sorry)\\nOrganization: The University of Birmingham, United  \n",
      "Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk<snip>',  \n",
      "'comp.graphics')\n",
      "For this post, there is no real indication that it belongs to comp.graphics considering \n",
      "only the wording that is left after the preprocessing step:\n",
      ">>> noise_post = graphics[5][1]\n",
      ">>> analyzer = vectorizer.build_analyzer()\n",
      ">>> print(list(analyzer(noise_post)))\n",
      "['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test',  \n",
      "'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line',  \n",
      "'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n",
      "This is only after tokenization, lowercasing, and stop word removal. If we also \n",
      "subtract those words that will be later filtered out via min_df and max_df, which  \n",
      "will be done later in fit_transform, it gets even worse:\n",
      ">>> useful = set(analyzer(noise_post)).intersection \n",
      "(vectorizer.get_feature_names())\n",
      ">>> print(sorted(useful))\n",
      "['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test',  \n",
      "'uk', 'unit', 'univers']\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 96, 'page_label': '76'}\n",
      "Clustering  Finding Related Posts\n",
      "[  76 ]\n",
      "Even more, most of the words occur frequently in other posts as well, as we \n",
      "can check with the IDF scores. Remember that the higher TF-IDF, the more \n",
      "discriminative a term is for a given post. As IDF is a multiplicative factor here,  \n",
      "a low value of it signals that it is not of great value in general.\n",
      ">>> for term in sorted(useful):\n",
      "...     print('IDF(%s)=%.2f'%(term,  \n",
      "vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))\n",
      "IDF(ac)=3.51\n",
      "IDF(birmingham)=6.77\n",
      "IDF(host)=1.74\n",
      "IDF(kingdom)=6.68\n",
      "IDF(nntp)=1.77\n",
      "IDF(sorri)=4.14\n",
      "IDF(test)=3.83\n",
      "IDF(uk)=3.70\n",
      "IDF(unit)=4.42\n",
      "IDF(univers)=1.91\n",
      "So, the terms with the highest discriminative power, birmingham and kingdom, \n",
      "clearly are not that computer graphics related, the same is the case with the terms \n",
      "with lower IDF scores. Understandably, posts from different newsgroups will be \n",
      "clustered together.\n",
      "For our goal, however, this is no big deal, as we are only interested in cutting down \n",
      "the number of posts that we have to compare a new post to. After all, the particular \n",
      "newsgroup from where our training data came from is of no special interest.\n",
      "Tweaking the parameters\n",
      "So what about all the other parameters? Can we tweak them to get better results?\n",
      "Sure. We can, of course, tweak the number of clusters, or play with the vectorizer's \n",
      "max_features parameter (you should try that!). Also, we can play with different \n",
      "cluster center initializations. Then there are more exciting alternatives to K-means \n",
      "itself. There are, for example, clustering approaches that let you even use different \n",
      "similarity measurements, such as Cosine similarity, Pearson, or Jaccard. An exciting \n",
      "field for you to play.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 97, 'page_label': '77'}\n",
      "[ 77 ]\n",
      "But before you go there, you will have to define what you actually mean by \"better\". \n",
      "SciKit has a complete package dedicated only to this definition. The package is called \n",
      "sklearn.metrics and also contains a full range of different metrics to measure \n",
      "clustering quality. Maybe that should be the first place to go now. Right into the \n",
      "sources of the metrics package.\n",
      "Summary\n",
      "That was a tough ride from pre-processing over clustering to a solution that can \n",
      "convert noisy text into a meaningful concise vector representation, which we can \n",
      "cluster. If we look at the efforts we had to do to finally being able to cluster, it was \n",
      "more than half of the overall task. But on the way, we learned quite a bit on text \n",
      "processing and how simple counting can get you very far in the noisy real-world data.\n",
      "The ride has been made much smoother, though, because of SciKit and its powerful \n",
      "packages. And there is more to explore. In this chapter, we were scratching the \n",
      "surface of its capabilities. In the next chapters, we will see more of its power.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 98, 'page_label': '78'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 99, 'page_label': '79'}\n",
      "[ 79 ]\n",
      "Topic Modeling\n",
      "In the previous chapter, we grouped text documents using clustering. This is a very \n",
      "useful tool, but it is not always the best. Clustering results in each text belonging \n",
      "to exactly one cluster. This book is about machine learning and Python. Should it \n",
      "be grouped with other Python-related works or with machine-related works? In a \n",
      "physical bookstore, we will need a single place to stock the book. In an Internet store, \n",
      "however, the answer is this book is about both machine learning and Python and the book \n",
      "should be listed in both the sections in an online bookstore. This does not mean that \n",
      "the book will be listed in all the sections, of course. We will not list this book with \n",
      "other baking books.\n",
      "In this chapter, we will learn methods that do not cluster documents into completely \n",
      "separate groups but allow each document to refer to several topics. These topics will \n",
      "be identified automatically from a collection of text documents. These documents \n",
      "may be whole books or shorter pieces of text such as a blogpost, a news story, or  \n",
      "an e-mail.\n",
      "We would also like to be able to infer the fact that these documents may have topics \n",
      "that are central to them, while referring to other topics only in passing. This book \n",
      "mentions plotting every so often, but it is not a central topic as machine learning is. \n",
      "This means that documents have topics that are central to them and others that are \n",
      "more peripheral. The subfield of machine learning that deals with these problems is \n",
      "called topic modeling and is the subject of this chapter.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 100, 'page_label': '80'}\n",
      "Topic Modeling\n",
      "[  80 ]\n",
      "Latent Dirichlet allocation\n",
      "LDA and LDAunfortunately, there are two methods in machine learning with \n",
      "the initials LDA: latent Dirichlet allocation, which is a topic modeling method and \n",
      "linear discriminant analysis, which is a classification method. They are completely \n",
      "unrelated, except for the fact that the initials LDA can refer to either. In certain \n",
      "situations, this can be confusing. The scikit-learn tool has a submodule, sklearn.\n",
      "lda, which implements linear discriminant analysis. At the moment, scikit-learn \n",
      "does not implement latent Dirichlet allocation.\n",
      "The topic model we will look at is latent Dirichlet allocation (LDA). The mathematical \n",
      "ideas behind LDA are fairly complex, and we will not go into the details here.\n",
      "For those who are interested, and adventurous enough, Wikipedia will provide all \n",
      "the equations behind these algorithms: http://en.wikipedia.org/wiki/Latent_\n",
      "Dirichlet_allocation.\n",
      "However, we can understand the ideas behind LDA intuitively at a high-level. \n",
      "LDA belongs to a class of models that are called generative models as they have a \n",
      "sort of fable, which explains how the data was generated. This generative story is \n",
      "a simplification of reality, of course, to make machine learning easier. In the LDA \n",
      "fable, we first create topics by assigning probability weights to words. Each topic \n",
      "will assign different weights to different words. For example, a Python topic will \n",
      "assign high probability to the word \"variable\" and a low probability to the word \n",
      "\"inebriated\". When we wish to generate a new document, we first choose the topics it \n",
      "will use and then mix words from these topics.\n",
      "For example, let's say we have only three topics that books discuss:\n",
      " Machine learning\n",
      " Python\n",
      " Baking\n",
      "For each topic, we have a list of words associated with it. This book will be a \n",
      "mixture of the first two topics, perhaps 50 percent each. The mixture does not need \n",
      "to be equal, it can also be a 70/30 split. When we are generating the actual text, we \n",
      "generate word by word; first we decide which topic this word will come from. This is \n",
      "a random decision based on the topic weights. Once a topic is chosen, we generate a \n",
      "word from that topic's list of words. To be precise, we choose a word in English with \n",
      "the probability given by the topic.\n",
      "In this model, the order of words does not matter. This is a bag of words model as we \n",
      "have already seen in the previous chapter. It is a crude simplification of language, \n",
      "but it often works well enough, because just knowing which words were used in a \n",
      "document and their frequencies are enough to make machine learning decisions.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 101, 'page_label': '81'}\n",
      "[ 81 ]\n",
      "In the real world, we do not know what the topics are. Our task is to take a collection \n",
      "of text and to reverse engineer this fable in order to discover what topics are out \n",
      "there and simultaneously figure out which topics each document uses.\n",
      "Building a topic model\n",
      "Unfortunately, scikit-learn does not support latent Dirichlet allocation. Therefore, \n",
      "we are going to use the gensim package in Python. Gensim is developed by Radim \n",
      "ehek who is a machine learning researcher and consultant in the United Kingdom. \n",
      "We must start by installing it. We can achieve this by running the following command:\n",
      "pip install gensim\n",
      "As input data, we are going to use a collection of news reports from the Associated \n",
      "Press (AP). This is a standard dataset for text modeling research, which was used in \n",
      "some of the initial works on topic models. After downloading the data, we can load \n",
      "it by running the following code:\n",
      ">>> from gensim import corpora, models\n",
      ">>> corpus = corpora.BleiCorpus('./data/ap/ap.dat',  \n",
      "    './data/ap/vocab.txt')\n",
      "The corpus variable holds all of the text documents and has loaded them in a  \n",
      "format that makes for easy processing. We can now build a topic model using  \n",
      "this object as input:\n",
      ">>> model = models.ldamodel.LdaModel(\n",
      "              corpus,\n",
      "              num_topics=100,\n",
      "              id2word=corpus.id2word)\n",
      "This single constructor call will statistically infer which topics are present in the \n",
      "corpus. We can explore the resulting model in many ways. We can see the list  \n",
      "of topics a document refers to using the model[doc] syntax, as shown in the \n",
      "following example:\n",
      "  >>> doc = corpus.docbyoffset(0)\n",
      "  >>> topics = model[doc]\n",
      "  >>> print(topics)\n",
      "[(3, 0.023607255776894751),\n",
      " (13, 0.11679936618551275),\n",
      " (19, 0.075935855202707139),\n",
      "....\n",
      " (92, 0.10781541687001292)]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 102, 'page_label': '82'}\n",
      "Topic Modeling\n",
      "[  82 ]\n",
      "The result will almost surely look different on our computer! The learning algorithm \n",
      "uses some random numbers and every time you learn a new topic model on the \n",
      "same input data, the result is different. Some of the qualitative properties of the \n",
      "model will be stable across different runs if your data is well behaved. For example, \n",
      "if you are using the topics to compare documents, as we do here, then the similarities \n",
      "should be robust and change only slightly. On the other hand, the order of the \n",
      "different topics will be completely different.\n",
      "The format of the result is a list of pairs: (topic_index, topic_weight). We can \n",
      "see that only a few topics are used for each document (in the preceding example, \n",
      "there is no weight for topics 0, 1, and 2; the weight for those topics is 0). The topic \n",
      "model is a sparse model, as although there are many possible topics; for each \n",
      "document, only a few of them are used. This is not strictly true as all the topics \n",
      "have a nonzero probability in the LDA model, but some of them have such a small \n",
      "probability that we can round it to zero as a good approximation.\n",
      "We can explore this further by plotting a histogram of the number of topics that each \n",
      "document refers to:\n",
      ">>> num_topics_used = [len(model[doc]) for doc in corpus]\n",
      ">>> plt.hist(num_topics_used)\n",
      "You will get the following plot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 103, 'page_label': '83'}\n",
      "[ 83 ]\n",
      "Sparsity means that while you may have large matrices and vectors, \n",
      "in principle, most of the values are zero (or so small that we can round \n",
      "them to zero as a good approximation). Therefore, only a few things are \n",
      "relevant at any given time.\n",
      "Often problems that seem too big to solve are actually feasible because \n",
      "the data is sparse. For example, even though any web page can link to \n",
      "any other web page, the graph of links is actually very sparse as each \n",
      "web page will link to a very tiny fraction of all other web pages.\n",
      "In the preceding graph, we can see that about 150 documents have 5 topics, while the \n",
      "majority deals with around 10 to 12 of them. No document talks about more than 20 \n",
      "different topics.\n",
      "To a large extent, this is due to the value of the parameters that were used, namely, \n",
      "the alpha parameter. The exact meaning of alpha is a bit abstract, but bigger values \n",
      "for alpha will result in more topics per document.\n",
      "Alpha needs to be a value greater than zero, but is typically set to a lesser value, \n",
      "usually, less than one. The smaller the value of alpha, the fewer topics each \n",
      "document will be expected to discuss. By default, gensim will set alpha to 1/num_\n",
      "topics, but you can set it explicitly by passing it as an argument in the LdaModel \n",
      "constructor as follows:\n",
      ">>> model = models.ldamodel.LdaModel(\n",
      "              corpus,\n",
      "              num_topics=100,\n",
      "              id2word=corpus.id2word,\n",
      "              alpha=1)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 104, 'page_label': '84'}\n",
      "Topic Modeling\n",
      "[  84 ]\n",
      "In this case, this is a larger alpha than the default, which should lead to more topics \n",
      "per document. As we can see in the combined histogram given next, gensim behaves \n",
      "as we expected and assigns more topics to each document:\n",
      "Now, we can see in the preceding histogram that many documents touch upon \n",
      "20 to 25 different topics. If you set the value lower, you will observe the opposite \n",
      "(downloading the code from the online repository will allow you to play around \n",
      "with these values).\n",
      "What are these topics? Technically, as we discussed earlier, they are multinomial \n",
      "distributions over words, which means that they assign a probability to each word in \n",
      "the vocabulary. Words with high probability are more associated with that topic than \n",
      "words with lower probability.\n",
      "Our brains are not very good at reasoning with probability distributions, but we can \n",
      "readily make sense of a list of words. Therefore, it is typical to summarize topics by \n",
      "the list of the most highly weighted words.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 105, 'page_label': '85'}\n",
      "[ 85 ]\n",
      "In the following table, we display the first ten topics:\n",
      "Topic no. Topic\n",
      "1 dress military soviet president new state capt carlucci states leader stance \n",
      "government\n",
      "2 koch zambia lusaka oneparty orange kochs party i government mayor new \n",
      "political\n",
      "3 human turkey rights abuses royal thompson threats new state wrote garden \n",
      "president\n",
      "4 bill employees experiments levin taxation federal measure legislation senate \n",
      "president whistleblowers sponsor\n",
      "5 ohio july drought jesus disaster percent hartford mississippi crops northern \n",
      "valley virginia\n",
      "6 united percent billion year president world years states people i bush news\n",
      "7 b hughes affidavit states united ounces squarefoot care delaying charged \n",
      "unrealistic bush\n",
      "8 yeutter dukakis bush convention farm subsidies uruguay percent secretary \n",
      "general i told\n",
      "9 kashmir government people srinagar india dumps city two jammukashmir \n",
      "group moslem pakistan\n",
      "10 workers vietnamese irish wage immigrants percent bargaining last island \n",
      "police hutton I\n",
      "Although daunting at first glance, when reading through the list of words, we can \n",
      "clearly see that the topics are not just random words, but instead these are logical \n",
      "groups. We can also see that these topics refer to older news items, from when the \n",
      "Soviet Union still existed and Gorbachev was its Secretary General. We can also \n",
      "represent the topics as word clouds, making more likely words larger. For example, \n",
      "this is the visualization of a topic which deals with the Middle East and politics:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 106, 'page_label': '86'}\n",
      "Topic Modeling\n",
      "[  86 ]\n",
      "We can also see that some of the words should perhaps be removed (for example, the \n",
      "word \"I\") as they are not so informative, they are stop words. When building topic \n",
      "modeling, it can be useful to filter out stop words, as otherwise, you might end up \n",
      "with a topic consisting entirely of stop words. We may also wish to preprocess the \n",
      "text to stems in order to normalize plurals and verb forms. This process was covered \n",
      "in the previous chapter and you can refer to it for details. If you are interested, you \n",
      "can download the code from the companion website of the book and try all these \n",
      "variations to draw different pictures.\n",
      "Building a word cloud like the previous one can be done with \n",
      "several different pieces of software. For the graphics in this \n",
      "chapter, we used a Python-based tool called pytagcloud. This \n",
      "package requires a few dependencies to install and is not central \n",
      "to machine learning, so we won't consider it in the main text; \n",
      "however, we have all of the code available in the online code \n",
      "repository to generate the figures in this chapter.\n",
      "Comparing documents by topics\n",
      "Topics can be useful on their own to build the sort of small vignettes with words that \n",
      "are shown in the previous screenshot. These visualizations can be used to navigate a \n",
      "large collection of documents. For example, a website can display the different topics \n",
      "as different word clouds, allowing a user to click through to the documents. In fact, \n",
      "they have been used in just this way to analyze large collections of documents.\n",
      "However, topics are often just an intermediate tool to another end. Now that we \n",
      "have an estimate for each document of how much of that document comes from each \n",
      "topic, we can compare the documents in topic space. This simply means that instead \n",
      "of comparing word to word, we say that two documents are similar if they talk about \n",
      "the same topics.\n",
      "This can be very powerful as two text documents that share few words may actually \n",
      "refer to the same topic! They may just refer to it using different constructions (for \n",
      "example, one document may read \"the President of the United States\" while the other \n",
      "will use the name \"Barack Obama\").\n",
      "Topic models are good on their own to build visualizations \n",
      "and explore data. They are also very useful as an intermediate \n",
      "step in many other tasks.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 107, 'page_label': '87'}\n",
      "[ 87 ]\n",
      "At this point, we can redo the exercise we performed in the last chapter and look \n",
      "for the most similar post to an input query, by using the topics to define similarity. \n",
      "Whereas, earlier we compared two documents by comparing their word vectors \n",
      "directly, we can now compare two documents by comparing their topic vectors.\n",
      "For this, we are going to project the documents to the topic space. That is, we want \n",
      "to have a vector of topics that summarize the document. How to perform these \n",
      "types of dimensionality reduction in general is an important task in itself and we \n",
      "have a chapter entirely devoted to this task. For the moment, we just show how \n",
      "topic models can be used for exactly this purpose; once topics have been computed \n",
      "for each document, we can perform operations on its topic vector and forget about \n",
      "the original words. If the topics are meaningful, they will be potentially more \n",
      "informative than the raw words. Additionally, this may bring computational \n",
      "advantages, as it is much faster to compare 100 vectors of topic weights than vectors \n",
      "of the size of the vocabulary (which will contain thousands of terms).\n",
      "Using gensim, we have seen earlier how to compute the topics corresponding to all \n",
      "the documents in the corpus. We will now compute these for all the documents and \n",
      "store it in a NumPy arrays and compute all pairwise distances:\n",
      ">>> from gensim import matutils\n",
      ">>> topics = matutils.corpus2dense(model[corpus],  \n",
      "    num_terms=model.num_topics)\n",
      "Now, topics is a matrix of topics. We can use the pdist function in SciPy to \n",
      "compute all pairwise distances. That is, with a single function call, we compute  \n",
      "all the values of sum((topics[ti]  topics[tj])**2):\n",
      ">>> from scipy.spatial import distance\n",
      ">>> pairwise = distance.squareform(distance.pdist(topics))\n",
      "Now, we will employ one last little trick; we will set the diagonal elements of the \n",
      "distance matrix to a high value (it just needs to be larger than the other values in \n",
      "the matrix):\n",
      ">>> largest = pairwise.max()\n",
      ">>> for ti in range(len(topics)):\n",
      "...     pairwise[ti,ti] = largest+1\n",
      "And we are done! For each document, we can look up the closest element easily (this \n",
      "is a type of nearest neighbor classifier):\n",
      " >>> def closest_to(doc_id):\n",
      " ...    return pairwise[doc_id].argmin()\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 108, 'page_label': '88'}\n",
      "Topic Modeling\n",
      "[  88 ]\n",
      "Note that this will not work if we had not set the diagonal \n",
      "elements to a large value: the function will always return the \n",
      "same element as it is the one most similar to itself (except in \n",
      "the weird case where two elements had exactly the same topic \n",
      "distribution, which is very rare unless they are exactly the same).\n",
      "For example, here is one possible query document (it is the second document in  \n",
      "our collection):\n",
      "From: geb@cs.pitt.edu (Gordon Banks)\n",
      "Subject: Re: request for information on \"essential tremor\" and  \n",
      "Indrol?\n",
      "In article <1q1tbnINNnfn@life.ai.mit.edu> sundar@ai.mit.edu  \n",
      "writes:\n",
      "Essential tremor is a progressive hereditary tremor that gets  \n",
      "worse\n",
      "when the patient tries to use the effected member.  All limbs,  \n",
      "vocal\n",
      "cords, and head can be involved.  Inderal is a beta-blocker and\n",
      "is usually effective in diminishing the tremor.  Alcohol and  \n",
      "mysoline\n",
      "are also effective, but alcohol is too toxic to use as a  \n",
      "treatment.\n",
      "--\n",
      "------------------------------------------------------------------ \n",
      "----------\n",
      "Gordon Banks  N3JXP      | \"Skepticism is the chastity of the  \n",
      "intellect, and\n",
      "geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too  \n",
      "soon.\"\n",
      "  ---------------------------------------------------------------- \n",
      "------------\n",
      "If we ask for the most similar document to closest_to(1), we receive the following \n",
      "document as a result:\n",
      "From: geb@cs.pitt.edu (Gordon Banks)\n",
      "Subject: Re: High Prolactin\n",
      "In article <93088.112203JER4@psuvm.psu.edu> JER4@psuvm.psu.edu  \n",
      "(John E. Rodway) writes:\n",
      ">Any comments on the use of the drug Parlodel for high prolactin  \n",
      "in the blood?\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 109, 'page_label': '89'}\n",
      "[ 89 ]\n",
      ">\n",
      "It can suppress secretion of prolactin.  Is useful in cases of  \n",
      "galactorrhea.\n",
      "Some adenomas of the pituitary secret too much.\n",
      "--\n",
      "------------------------------------------------------------------ \n",
      "----------\n",
      "Gordon Banks  N3JXP      | \"Skepticism is the chastity of the  \n",
      "intellect, and\n",
      "geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too  \n",
      "soon.\"\n",
      "The system returns a post by the same author discussing medications.\n",
      "Modeling the whole of Wikipedia\n",
      "While the initial LDA implementations can be slow, which limited their use to small \n",
      "document collections, modern algorithms work well with very large collections of \n",
      "data. Following the documentation of gensim, we are going to build a topic model \n",
      "for the whole of the English-language Wikipedia. This takes hours, but can be done \n",
      "even with just a laptop! With a cluster of machines, we can make it go much faster, \n",
      "but we will look at that sort of processing environment in a later chapter.\n",
      "First, we download the whole Wikipedia dump from \n",
      "http://dumps.wikimedia.\n",
      "org. This is a large file (currently over 10 GB), so it may take a while, unless your \n",
      "Internet connection is very fast. Then, we will index it with a gensim tool:\n",
      "python -m gensim.scripts.make_wiki \\\n",
      "       enwiki-latest-pages-articles.xml.bz2 wiki_en_output\n",
      "Run the previous line on the command shell, not on the Python shell. After a few \n",
      "hours, the index will be saved in the same directory. At this point, we can build \n",
      "the final topic model. This process looks exactly like what we did for the small AP \n",
      "dataset. We first import a few packages:\n",
      ">>> import logging, gensim\n",
      "Now, we set up logging, using the standard Python logging module (which gensim \n",
      "uses to print out status messages). This step is not strictly necessary, but it is nice to \n",
      "have a little more output to know what is happening:\n",
      ">>> logging.basicConfig(\n",
      "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
      "    level=logging.INFO)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 110, 'page_label': '90'}\n",
      "Topic Modeling\n",
      "[  90 ]\n",
      "Now we load the preprocessed data:\n",
      ">>> id2word = gensim.corpora.Dictionary.load_from_text(\n",
      "              'wiki_en_output_wordids.txt')\n",
      ">>> mm = gensim.corpora.MmCorpus('wiki_en_output_tfidf.mm')\n",
      "Finally, we build the LDA model as we did earlier:\n",
      ">>> model = gensim.models.ldamodel.LdaModel(\n",
      "          corpus=mm,\n",
      "          id2word=id2word,\n",
      "          num_topics=100,\n",
      "          update_every=1,\n",
      "          chunksize=10000,\n",
      "          passes=1)\n",
      "This will again take a couple of hours. You will see the progress on your console, \n",
      "which can give you an indication of how long you still have to wait.\n",
      "Once it is done, we can save the topic model to a file, so we don't have to redo it:\n",
      "  >>> model.save('wiki_lda.pkl')\n",
      "If you exit your session and come back later, you can load the model again using the \n",
      "following command (after the appropriate imports, naturally):\n",
      "  >>> model = gensim.models.ldamodel.LdaModel.load('wiki_lda.pkl')\n",
      "The model object can be used to explore the collection of documents, and build the \n",
      "topics matrix as we did earlier.\n",
      "We can see that this is still a sparse model even if we have many more documents \n",
      "than we had earlier (over 4 million as we are writing this):\n",
      "  >>> lens = (topics > 0).sum(axis=0)\n",
      "  >>> print(np.mean(lens))\n",
      "  6.41\n",
      "  >>> print(np.mean(lens <= 10))\n",
      "  0.941\n",
      "So, the average document mentions 6.4 topics and 94 percent of them mention 10 or \n",
      "fewer topics.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 111, 'page_label': '91'}\n",
      "[ 91 ]\n",
      "We can ask what the most talked about topic in Wikipedia is. We will first compute \n",
      "the total weight for each topic (by summing up the weights from all the documents) \n",
      "and then retrieve the words corresponding to the most highly weighted topic. This is \n",
      "performed using the following code:\n",
      ">>> weights = topics.sum(axis=0)\n",
      ">>> words = model.show_topic(weights.argmax(), 64)\n",
      "Using the same tools as we did earlier to build up a visualization, we can see that \n",
      "the most talked about topic is related to music and is a very coherent topic. A full 18 \n",
      "percent of Wikipedia pages are partially related to this topic (5.5 percent of all the \n",
      "words in Wikipedia are assigned to this topic). Take a look at the following screenshot:\n",
      "These plots and numbers were obtained when the book was \n",
      "being written. As Wikipedia keeps changing, your results will \n",
      "be different. We expect that the trends will be similar, but the \n",
      "details may vary.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 112, 'page_label': '92'}\n",
      "Topic Modeling\n",
      "[  92 ]\n",
      "Alternatively, we can look at the least talked about topic:\n",
      "  >>> words = model.show_topic(weights.argmin(), 64)\n",
      "The least talked about topic is harder to interpret, but many of its top words refer \n",
      "to airports in eastern countries. Just 1.6 percent of documents touch upon it, and it \n",
      "represents just 0.1 percent of the words.\n",
      "Choosing the number of topics\n",
      "So far in the chapter, we have used a fixed number of topics for our analyses, namely \n",
      "100. This was a purely arbitrary number, we could have just as well used either 20 \n",
      "or 200 topics. Fortunately, for many uses, this number does not really matter. If you \n",
      "are going to only use the topics as an intermediate step, as we did previously when \n",
      "finding similar posts, the final behavior of the system is rarely very sensitive to the \n",
      "exact number of topics used in the model. This means that as long as you use enough \n",
      "topics, whether you use 100 topics or 200, the recommendations that result from the \n",
      "process will not be very different; 100 is often a good enough number (while 20 is too \n",
      "few for a general collection of text documents). The same is true of setting the alpha \n",
      "value. While playing around with it can change the topics, the final results are again \n",
      "robust against this change.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 113, 'page_label': '93'}\n",
      "[ 93 ]\n",
      "Topic modeling is often an end towards a goal. In that case, it \n",
      "is not always very important exactly which parameter values \n",
      "are used. A different number of topics or values for parameters \n",
      "such as alpha will result in systems whose end results are \n",
      "almost identical in their final results.\n",
      "On the other hand, if you are going to explore the topics directly, or build a \n",
      "visualization tool that exposes them, you should probably try a few values  \n",
      "and see which gives you the most useful or most appealing results.\n",
      "Alternatively, there are a few methods that will automatically determine the \n",
      "number of topics for you, depending on the dataset. One popular model is called \n",
      "the hierarchical Dirichlet process. Again, the full mathematical model behind it is \n",
      "complex and beyond the scope of this book. However, the fable we can tell is that \n",
      "instead of having the topics fixed first as in the LDA generative story, the topics \n",
      "themselves were generated along with the data, one at a time. Whenever the writer \n",
      "starts a new document, they have the option of using the topics that already exist or \n",
      "to create a completely new one. When more topics have already been created, the \n",
      "probability of creating a new one, instead of reusing what exists goes down, but the \n",
      "possibility always exists.\n",
      "This means that the more documents we have, the more topics we will end up with. This \n",
      "is one of those statements that is unintuitive at first but makes perfect sense upon \n",
      "reflection. We are grouping documents and the more examples we have, the more \n",
      "we can break them up. If we only have a few examples of news articles, then \"Sports\" \n",
      "will be a topic. However, as we have more, we start to break it up into the individual \n",
      "modalities: \"Hockey\", \"Soccer\", and so on. As we have even more data, we can start \n",
      "to tell nuances apart, articles about individual teams and even individual players. \n",
      "The same is true for people. In a group of many different backgrounds, with a few \n",
      "\"computer people\", you might put them together; in a slightly larger group, you will \n",
      "have separate gatherings for programmers and systems administrators; and in the \n",
      "real-world, we even have different gatherings for Python and Ruby programmers.\n",
      "The hierarchical Dirichlet process (HDP) is available in gensim. Using it is trivial. \n",
      "To adapt the code we wrote for LDA, we just need to replace the call to gensim.\n",
      "models.ldamodel.LdaModel with a call to the HdpModel constructor as follows:\n",
      "  >>> hdp = gensim.models.hdpmodel.HdpModel(mm, id2word)\n",
      "That's it (except that it takes a bit longer to computethere are no free lunches). \n",
      "Now, we can use this model in much the same way as we used the LDA model, \n",
      "except that we did not need to specify the number of topics.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 114, 'page_label': '94'}\n",
      "Topic Modeling\n",
      "[  94 ]\n",
      "Summary\n",
      "In this chapter, we discussed topic modeling. Topic modeling is more flexible than \n",
      "clustering as these methods allow each document to be partially present in more \n",
      "than one group. To explore these methods, we used a new package, gensim.\n",
      "Topic modeling was first developed and is easier to understand in the case of text, \n",
      "but in the computer vision chapter we will see how some of these techniques may \n",
      "be applied to images as well. Topic models are very important in modern computer \n",
      "vision research. In fact, unlike the previous chapters, this chapter was very close to the \n",
      "cutting edge of research in machine learning algorithms. The original LDA algorithm \n",
      "was published in a scientific journal in 2003, but the method that gensim uses to be \n",
      "able to handle Wikipedia was only developed in 2010 and the HDP algorithm is from \n",
      "2011. The research continues and you can find many variations and models with \n",
      "wonderful names such as the Indian buffet process (not to be confused with the Chinese \n",
      "restaurant process, which is a different model), or Pachinko allocation (Pachinko being a \n",
      "type of Japanese game, a cross between a slot-machine and pinball).\n",
      "We have now gone through some of the major machine learning modes: \n",
      "classification, clustering, and topic modeling.\n",
      "In the next chapter, we go back to classification, but this time, we will be exploring \n",
      "advanced algorithms and approaches.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 115, 'page_label': '95'}\n",
      "[ 95 ]\n",
      "Classification  Detecting \n",
      "Poor Answers\n",
      "Now that we are able to extract useful features from text, we can take on the \n",
      "challenge of building a classifier using real data. Let's come back to our imaginary \n",
      "website in Chapter 3, Clustering  Finding Related Posts, where users can submit \n",
      "questions and get them answered.\n",
      "A continuous challenge for owners of those Q&A sites is to maintain a decent level of \n",
      "quality in the posted content. Sites such as StackOverflow make considerable efforts \n",
      "to encourage users with diverse possibilities to score content and offer badges and \n",
      "bonus points in order to encourage the users to spend more energy on carving out \n",
      "the question or crafting a possible answer.\n",
      "One particular successful incentive is the ability for the asker to flag one answer  \n",
      "to their question as the accepted answer (again there are incentives for the asker  \n",
      "to flag answers as such). This will result in more score points for the author of  \n",
      "the flagged answer.\n",
      "Would it not be very useful to the user to immediately see how good his answer is \n",
      "while he is typing it in? That means, the website would continuously evaluate his \n",
      "work-in-progress answer and provide feedback as to whether the answer shows \n",
      "some signs of a poor one. This will encourage the user to put more effort into writing \n",
      "the answer (providing a code example? including an image?), and thus improve the \n",
      "overall system.\n",
      "Let's build such a mechanism in this chapter.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 116, 'page_label': '96'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 96 ]\n",
      "Sketching our roadmap\n",
      "As we will build a system using real data that is very noisy, this chapter is not for the \n",
      "fainthearted, as we will not arrive at the golden solution of a classifier that achieves \n",
      "100 percent accuracy; often, even humans disagree whether an answer was good \n",
      "or not (just look at some of the StackOverflow comments). Quite the contrary, we \n",
      "will find out that some problems like this one are so hard that we have to adjust \n",
      "our initial goals on the way. But on the way, we will start with the nearest neighbor \n",
      "approach, find out why it is not very good for the task, switch over to logistic \n",
      "regression, and arrive at a solution that will achieve good enough prediction quality, \n",
      "but on a smaller part of the answers. Finally, we will spend some time looking at \n",
      "how to extract the winner to deploy it on the target system.\n",
      "Learning to classify classy answers\n",
      "In classification, we want to find the corresponding classes , sometimes also called \n",
      "labels, for given data instances. To be able to achieve this, we need to answer  \n",
      "two questions:\n",
      " How should we represent the data instances?\n",
      " Which model or structure should our classifier possess?\n",
      "Tuning the instance\n",
      "In its simplest form, in our case, the data instance is the text of the answer and the \n",
      "label would be a binary value indicating whether the asker accepted this text as an \n",
      "answer or not. Raw text, however, is a very inconvenient representation to process \n",
      "for most machine learning algorithms. They want numbers. And it will be our task to \n",
      "extract useful features from the raw text, which the machine learning algorithm can \n",
      "then use to learn the right label for it.\n",
      "Tuning the classifier\n",
      "Once we have found or collected enough (text, label) pairs, we can train a classifier. \n",
      "For the underlying structure of the classifier, we have a wide range of possibilities, \n",
      "each of them having advantages and drawbacks. Just to name some of the more \n",
      "prominent choices, there are logistic regression, decision trees, SVMs, and Nave \n",
      "Bayes. In this chapter, we will contrast the instance-based method from the last \n",
      "chapter, nearest neighbor, with model-based logistic regression.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 117, 'page_label': '97'}\n",
      "Chapter 5\n",
      "[  97 ]\n",
      "Fetching the data\n",
      "Luckily for us, the team behind StackOverflow provides most of the data behind the \n",
      "StackExchange universe to which StackOverflow belongs under a cc-wiki license. \n",
      "At the time of writing this book, the latest data dump can be found at https://\n",
      "archive.org/details/stackexchange. It contains data dumps of all Q&A sites of \n",
      "the StackExchange family. For StackOverflow, you will find multiple files, of which \n",
      "we only need the stackoverflow.com-Posts.7z file, which is 5.2 GB.\n",
      "After downloading and extracting it, we have around 26 GB of data in the format of \n",
      "XML, containing all questions and answers as individual row tags within the root \n",
      "tag posts:\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<posts>\n",
      "...\n",
      "  <row Id=\"4572748\" PostTypeId=\"2\" ParentId=\"4568987\"  \n",
      "CreationDate=\"2011-01-01T00:01:03.387\" Score=\"4\" ViewCount=\"\"  \n",
      "Body=\"&lt;p&gt;IANAL, but &lt;a  \n",
      "href=&quot;http://support.apple.com/kb/HT2931&quot;  \n",
      "rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; indicates to me that you  \n",
      "cannot use the loops in your  \n",
      "application:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;   \n",
      "&lt;p&gt;...however, individual audio loops may&#xA;  not be  \n",
      "commercially or otherwise&#xA;  distributed on a standalone basis,  \n",
      "nor&#xA;  may they be repackaged in whole or in&#xA;  part as audio  \n",
      "samples, sound effects&#xA;  or music beds.&quot;&lt;/p&gt;&#xA;   \n",
      "&#xA;  &lt;p&gt;So don't worry, you can make&#xA;  commercial music  \n",
      "with GarageBand, you&#xA;  just can't distribute the loops as&#xA;   \n",
      "loops.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;\" OwnerUserId=\"203568\"  \n",
      "LastActivityDate=\"2011-01-01T00:01:03.387\" CommentCount=\"1\" />\n",
      "\n",
      "</posts>\n",
      "Name Type Description\n",
      "Id Integer This is a unique identifier.\n",
      "PostTypeId Integer This describes the category of the post. The values \n",
      "interesting to us are the following:\n",
      " Question\n",
      " Answer\n",
      "Other values will be ignored.\n",
      "ParentId Integer This is a unique identifier of the question to which \n",
      "this answer belongs (missing for questions).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 118, 'page_label': '98'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 98 ]\n",
      "Name Type Description\n",
      "CreationDate DateTime This is the date of submission.\n",
      "Score Integer This is the score of the post.\n",
      "ViewCount Integer \n",
      "or empty\n",
      "This is the number of user views for this post.\n",
      "Body String This is the complete post as encoded HTML text.\n",
      "OwnerUserId Id This is a unique identifier of the poster. If 1, then it \n",
      "is a wiki question.\n",
      "Title String This is the title of the question (missing for \n",
      "answers).\n",
      "AcceptedAnswerId Id This is the ID for the accepted answer (missing for \n",
      "answers).\n",
      "CommentCount Integer This is the number of comments for the post.\n",
      "Slimming the data down to chewable chunks\n",
      "To speed up our experimentation phase, we should not try to evaluate our \n",
      "classification ideas on the huge XML file. Instead, we should think of how we could \n",
      "trim it down so that we still keep a representable snapshot of it while being able to \n",
      "quickly test our ideas. If we filter the XML for row tags that have a creation date of, \n",
      "for example, 2012, we still end up with over 6 million posts (2,323,184 questions and \n",
      "4,055,999 answers), which should be enough to pick our training data from for now. \n",
      "We also do not want to operate on the XML format as it will slow us down, too. \n",
      "The simpler the format, the better. That's why we parse the remaining XML using \n",
      "Python's cElementTree and write it out to a tab-separated file.\n",
      "Preselection and processing of attributes\n",
      "To cut down the data even more, we can certainly drop attributes that we think will \n",
      "not help the classifier in distinguishing between good and not-so-good answers. But \n",
      "we have to be cautious here. Although some features are not directly impacting the \n",
      "classification, they are still necessary to keep.\n",
      "The PostTypeId attribute, for example, is necessary to distinguish between questions \n",
      "and answers. It will not be picked to serve as a feature, but we will need it to filter \n",
      "the data.\n",
      "CreationDate could be interesting to determine the time span between posting the \n",
      "question and posting the individual answers, so we keep it. The Score is of course \n",
      "important as an indicator for the community's evaluation.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 119, 'page_label': '99'}\n",
      "Chapter 5\n",
      "[  99 ]\n",
      "ViewCount, in contrast, is most likely of no use for our task. Even if it would help the \n",
      "classifier to distinguish between good and bad, we would not have this information \n",
      "at the time when an answer is being submitted. Drop it!\n",
      "The Body attribute obviously contains the most important information. As it is \n",
      "encoded HTML, we will have to decode to plain text.\n",
      "OwnerUserId is only useful if we take user-dependent features in to account, which \n",
      "we won't. Although we drop it here, we encourage you to use it to build a better \n",
      "classifier (maybe in connection with stackoverflow.com-Users.7z).\n",
      "The Title attribute is also ignored here, although it could add some more \n",
      "information about the question.\n",
      "CommentCount is also ignored. Similar to ViewCount, it could help the classifier  \n",
      "with posts that are out there for a while (more comments = more ambiguous post?). \n",
      "It will, however, not help the classifier at the time an answer is posted.\n",
      "AcceptedAnswerId is similar to Score in that it is an indicator of a post's quality. \n",
      "As we will access this per answer, instead of keeping this attribute, we will create \n",
      "the new attribute IsAccepted, which is 0 or 1 for answers and ignored for questions \n",
      "(ParentId=-1).\n",
      "We end up with the following format:\n",
      "Id <TAB> ParentId <TAB> IsAccepted <TAB> TimeToAnswer <TAB> Score  \n",
      "<TAB> Text\n",
      "For the concrete parsing details, please refer to so_xml_to_tsv.py and choose_\n",
      "instance.py. Suffice to say that in order to speed up processing, we will split the \n",
      "data into two files: in meta.json, we store a dictionary mapping a post's Id value to \n",
      "its other data except Text in JSON format so that we can read it in the proper format. \n",
      "For example, the score of a post would reside at meta[Id]['Score']. In data.tsv, we \n",
      "store the Id and Text values, which we can easily read with the following method:\n",
      "    def fetch_posts():\n",
      "        for line in open(\"data.tsv\", \"r\"):\n",
      "            post_id, text = line.split(\"\\t\")\n",
      "            yield int(post_id), text.strip()\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 120, 'page_label': '100'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 100 ]\n",
      "Defining what is a good answer\n",
      "Before we can train a classifier to distinguish between good and bad answers, we \n",
      "have to create the training data. So far, we only have a bunch of data. What we still \n",
      "have to do is define labels.\n",
      "We could, of course, simply use the IsAccepted attribute as a label. After all, that \n",
      "marks the answer that answered the question.  However, that is only the opinion  \n",
      "of the asker. Naturally, the asker wants to have a quick answer and accepts the first \n",
      "best answer. If over time more answers are submitted, some of them will tend to \n",
      "be better than the already accepted one. The asker, however, seldom gets back to \n",
      "the question and changes his mind. So we end up with many questions that have \n",
      "accepted answers that are not scored highest.\n",
      "At the other extreme, we could simply always take the best and worst scored answer \n",
      "per question as positive and negative examples. However, what do we do with \n",
      "questions that have only good answers, say, one with two and the other with four \n",
      "points? Should we really take an answer with, for example, two points as a negative \n",
      "example just because it happened to be the one with the lower score?\n",
      "We should settle somewhere between these extremes. If we take all answers that \n",
      "are scored higher than zero as positive and all answers with zero or less points as \n",
      "negative, we end up with quite reasonable labels:\n",
      ">>> all_answers = [q for q,v in meta.items() if v['ParentId']!=-1]\n",
      ">>> Y = np.asarray([meta[answerId]['Score']>0 for answerId in  \n",
      "all_answers])\n",
      "Creating our first classifier\n",
      "Let's start with the simple and beautiful nearest neighbor method from the previous \n",
      "chapter. Although it is not as advanced as other methods, it is very powerful: as it \n",
      "is not model-based, it can learn nearly any data. But this beauty comes with a clear \n",
      "disadvantage, which we will find out very soon.\n",
      "Starting with kNN\n",
      "This time, we won't implement it ourselves, but rather take it from the sklearn \n",
      "toolkit. There, the classifier resides in sklearn.neighbors. Let's start with a simple \n",
      "2-Nearest Neighbor classifier:\n",
      ">>> from sklearn import neighbors\n",
      ">>> knn = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
      ">>> print(knn)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 121, 'page_label': '101'}\n",
      "Chapter 5\n",
      "[  101 ]\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30,  \n",
      "metric='minkowski', n_neighbors=2, p=2, weights='uniform')\n",
      "It provides the same interface as all other estimators in sklearn: we train it using \n",
      "fit(), after which we can predict the class of new data instances using predict():\n",
      ">>> knn.fit([[1],[2],[3],[4],[5],[6]], [0,0,0,1,1,1])\n",
      ">>> knn.predict(1.5)\n",
      "array([0])\n",
      ">>> knn.predict(37)\n",
      "array([1])\n",
      ">>> knn.predict(3)\n",
      "array([0])\n",
      "To get the class probabilities, we can use predict_proba(). In this case of having \n",
      "two classes, 0 and 1, it will return an array of two elements:\n",
      ">>> knn.predict_proba(1.5)\n",
      "array([[ 1.,  0.]])\n",
      ">>> knn.predict_proba(37)\n",
      "array([[ 0.,  1.]])\n",
      ">>> knn.predict_proba(3.5)\n",
      "array([[ 0.5,  0.5]])\n",
      "Engineering the features\n",
      "So, what kind of features can we provide to our classifier? What do we think will \n",
      "have the most discriminative power?\n",
      "TimeToAnswer is already there in our meta dictionary, but it probably won't provide \n",
      "much value on its own. Then there is only Text, but in its raw form, we cannot pass \n",
      "it to the classifier, as the features must be in numerical form. We will have to do the \n",
      "dirty (and fun!) work of extracting features from it.\n",
      "What we could do is check the number of HTML links in the answer as a proxy for \n",
      "quality. Our hypothesis would be that more hyperlinks in an answer indicate better \n",
      "answers and thus a higher likelihood of being up-voted. Of course, we want to only \n",
      "count links in normal text and not code examples:\n",
      "import re\n",
      "code_match = re.compile('<pre>(.*?)</pre>',\n",
      "                        re.MULTILINE | re.DOTALL)\n",
      "link_match = re.compile('<a href=\"http://.*?\".*?>(.*?)</a>',\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 122, 'page_label': '102'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 102 ]\n",
      "                        re.MULTILINE | re.DOTALL)\n",
      "tag_match = re.compile('<[^>]*>', \n",
      "                        re.MULTILINE | re.DOTALL)\n",
      "def extract_features_from_body(s):\n",
      "    link_count_in_code = 0\n",
      "    # count links in code to later subtract them \n",
      "    for match_str in code_match.findall(s):\n",
      "        link_count_in_code += len(link_match.findall(match_str))\n",
      "    \n",
      "    return len(link_match.findall(s))  link_count_in_code\n",
      "For production systems, we would not want to parse HTML \n",
      "content with regular expressions. Instead, we should rely on \n",
      "excellent libraries such as BeautifulSoup, which does a marvelous \n",
      "job of robustly handling all the weird things that typically occur in \n",
      "everyday HTML.\n",
      "With this in place, we can generate one feature per answer. But before we train \n",
      "the classifier, let's first have a look at what we will train it with. We can get a first \n",
      "impression with the frequency distribution of our new feature. This can be done by \n",
      "plotting the percentage of how often each value occurs in the data. Have a look at the \n",
      "following plot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 123, 'page_label': '103'}\n",
      "Chapter 5\n",
      "[  103 ]\n",
      "With the majority of posts having no link at all, we know now that this feature will \n",
      "not make a good classifier alone. Let's nevertheless try it out to get a first estimation \n",
      "of where we are.\n",
      "Training the classifier\n",
      "We have to pass the feature array together with the previously defined labels Y to the \n",
      "kNN learner to obtain a classifier:\n",
      "X = np.asarray([extract_features_from_body(text) for post_id, text in  \n",
      "                fetch_posts() if post_id in all_answers])\n",
      "knn = neighbors.KNeighborsClassifier()\n",
      "knn.fit(X, Y)\n",
      "Using the standard parameters, we just fitted a 5NN (meaning NN with k=5) to \n",
      "our data. Why 5NN? Well, at the current state of our knowledge about the data, we \n",
      "really have no clue what the right k should be. Once we have more insight, we will \n",
      "have a better idea of how to set k.\n",
      "Measuring the classifier's performance\n",
      "We have to be clear about what we want to measure. The nave but easiest way is to \n",
      "simply calculate the average prediction quality over the test set. This will result in a \n",
      "value between 0 for predicting everything wrongly and 1 for perfect prediction. The \n",
      "accuracy can be obtained through knn.score().\n",
      "But as we learned in the previous chapter, we will not do it just once, but apply \n",
      "cross-validation here using the readymade KFold class from sklearn.cross_\n",
      "validation. Finally, we will then average the scores on the test set of each fold  \n",
      "and see how much it varies using standard deviation:\n",
      "from sklearn.cross_validation import KFold\n",
      "scores = []\n",
      "cv = KFold(n=len(X), k=10, indices=True)\n",
      "for train, test in cv:\n",
      "    X_train, y_train = X[train], Y[train]\n",
      "    X_test, y_test = X[test], Y[test]\n",
      "    clf = neighbors.KNeighborsClassifier()\n",
      "    clf.fit(X, Y)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 124, 'page_label': '104'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 104 ]\n",
      "    scores.append(clf.score(X_test, y_test))\n",
      "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"\\\n",
      "      %(np.mean(scores), np.std(scores)))\n",
      "Here is the output:\n",
      "Mean(scores)=0.50250    Stddev(scores)=0.055591\n",
      "Now that is far from being usable. With only 55 percent accuracy, it is not much \n",
      "better than tossing a coin. Apparently, the number of links in a post is not a very \n",
      "good indicator for the quality of a post. So, we can say that this feature does not  \n",
      "have much discriminative powerat least not for kNN with k=5.\n",
      "Designing more features\n",
      "In addition to using the number of hyperlinks as a proxy for a post's quality, the \n",
      "number of code lines is possibly another good one, too. At least it is a good indicator \n",
      "that the post's author is interested in answering the question. We can find the code \n",
      "embedded in the <pre></pre> tag. And once we have it extracted, we should count \n",
      "the number of words in the post while ignoring code lines:\n",
      "def extract_features_from_body(s):\n",
      "    num_code_lines = 0\n",
      "    link_count_in_code = 0\n",
      "    code_free_s = s\n",
      "    # remove source code and count how many lines\n",
      "    for match_str in code_match.findall(s):\n",
      "        num_code_lines += match_str.count('\\n')\n",
      "        code_free_s = code_match.sub(\"\", code_free_s)\n",
      "        # Sometimes source code contains links, \n",
      "        # which we don't want to count\n",
      "        link_count_in_code += len(link_match.findall(match_str))\n",
      "    links = link_match.findall(s)\n",
      "    link_count = len(links)\n",
      "    link_count -= link_count_in_code\n",
      "    html_free_s = re.sub(\" +\", \" \",\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 125, 'page_label': '105'}\n",
      "Chapter 5\n",
      "[  105 ]\n",
      "          tag_match.sub('',  code_free_s)).replace(\"\\n\", \"\")\n",
      "    link_free_s = html_free_s\n",
      "    # remove links from text before counting words\n",
      "    for link in links:\n",
      "        if link.lower().startswith(\"http://\"):\n",
      "            link_free_s = link_free_s.replace(link,'')\n",
      "    num_text_tokens = html_free_s.count(\" \")\n",
      "    return num_text_tokens, num_code_lines, link_count\n",
      "Looking at them, we notice that at least the number of words in a post shows  \n",
      "higher variability:\n",
      "Training on the bigger feature space improves accuracy quite a bit:\n",
      "Mean(scores)=0.59800    Stddev(scores)=0.02600\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 126, 'page_label': '106'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 106 ]\n",
      "But still, this would mean that we would classify roughly 4 out of 10 wrong. At least \n",
      "we are going in the right direction. More features lead to higher accuracy, which \n",
      "leads us to adding more features. Therefore, let's extend the feature space by even \n",
      "more features:\n",
      " AvgSentLen: This measures the average number of words in a sentence. \n",
      "Maybe there is a pattern that particularly good posts don't overload the \n",
      "reader's brain with overly long sentences?\n",
      " AvgWordLen: Similar to AvgSentLen, this feature measures the average \n",
      "number of characters in the words of a post.\n",
      " NumAllCaps: This measures the number of words that are written in \n",
      "uppercase, which is considered bad style.\n",
      " NumExclams: This measures the number of exclamation marks.\n",
      "The following charts show the value distributions for average sentence and word \n",
      "lengths and number of uppercase words and exclamation marks:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 127, 'page_label': '107'}\n",
      "Chapter 5\n",
      "[  107 ]\n",
      "With these four additional features, we now have seven features representing the \n",
      "individual posts. Let's see how we progress:\n",
      "Mean(scores)=0.61400    Stddev(scores)= 0.02154\n",
      "Now, that's interesting. We added four more features and don't get anything in \n",
      "return. How can that be?\n",
      "To understand this, we have to remind ourselves how kNN works. Our 5NN \n",
      "classifier determines the class of a new post by calculating the seven aforementioned \n",
      "features, LinkCount, NumTextTokens, NumCodeLines, AvgSentLen, AvgWordLen, \n",
      "NumAllCaps, and NumExclams, and then finds the five nearest other posts. The new \n",
      "post's class is then the majority of the classes of those nearest posts. The nearest \n",
      "posts are determined by calculating the Euclidean distance (as we did not specify \n",
      "it, the classifier was initialized with the default p=2, which is the parameter in the \n",
      "Minkowski distance). That means that all seven features are treated similarly. kNN \n",
      "does not really learn that, for instance, NumTextTokens is good to have but much less \n",
      "important than NumLinks. Let's consider the following two posts A and B that only \n",
      "differ in the following features and how they compare to a new post:\n",
      "Post NumLinks NumTextTokens\n",
      "A 2 20\n",
      "B 0 25\n",
      "new 1 23\n",
      "Although we would think that links provide more value than mere text, post B \n",
      "would be considered more similar to the new post than post A.\n",
      "Clearly, kNN has a hard time in correctly using the available data.\n",
      "Deciding how to improve\n",
      "To improve on this, we basically have the following options:\n",
      " Add more data: Maybe it is just not enough data for the learning algorithm \n",
      "and we should simply add more training data?\n",
      " Play with the model complexity: Maybe the model is not complex enough? \n",
      "Or maybe it is already too complex? In this case, we could decrease k so \n",
      "that it would take less nearest neighbors into account and thus be better in \n",
      "predicting non-smooth data. Or we could increase it to achieve the opposite.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 128, 'page_label': '108'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 108 ]\n",
      " Modify the feature space: Maybe we do not have the right set of features? \n",
      "We could, for example, change the scale of our current features or design \n",
      "even more new features. Or should we rather remove some of our current \n",
      "features in case some features are aliasing others?\n",
      " Change the model: Maybe kNN is in general not a good fit for our use case \n",
      "such that it will never be capable of achieving good prediction performance, \n",
      "no matter how complex we allow it to be and how sophisticated the feature \n",
      "space will become?\n",
      "In real life, at this point, people often try to improve the current performance by \n",
      "randomly picking one of the these options and trying them out in no particular \n",
      "order, hoping to find the golden configuration by chance. We could do the same \n",
      "here, but it will surely take longer than making informed decisions. Let's take the \n",
      "informed route, for which we need to introduce the bias-variance tradeoff.\n",
      "Bias-variance and their tradeoff\n",
      "In Chapter 1, Getting Started with Python Machine Learning, we tried to fit polynomials \n",
      "of different complexities controlled by the dimensionality parameter d to fit the \n",
      "data. We realized that a two-dimensional polynomial, a straight line, does not fit the \n",
      "example data very well, because the data was not of linear nature. No matter how \n",
      "elaborate our fitting procedure would have been, our two-dimensional model would \n",
      "see everything as a straight line. We say that it is too biased for the data at hand. It is \n",
      "under-fitting.\n",
      "We played a bit with the dimensions and found out that the 100-dimensional \n",
      "polynomial is actually fitting very well to the data on which it was trained (we did \n",
      "not know about train-test splits at that time). However, we quickly found out that it \n",
      "was fitting too well. We realized that it was over-fitting so badly, that with different \n",
      "samples of the data points, we would have gotten totally different 100-dimensional \n",
      "polynomials. We say that the model has a too high variance for the given data, or \n",
      "that it is over-fitting.\n",
      "These are the extremes between which most of our machine learning problems \n",
      "reside. Ideally, we want to have both, low bias and low variance. But, we are in  \n",
      "a bad world, and have to tradeoff between them. If we improve on one, we will \n",
      "likely get worse on the other.\n",
      "Fixing high bias\n",
      "Let's now assume we suffer from high bias. In that case, adding more training data \n",
      "clearly does not help. Also, removing features surely will not help, as our model \n",
      "would have already been overly simplistic.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 129, 'page_label': '109'}\n",
      "Chapter 5\n",
      "[  109 ]\n",
      "The only possibilities we have in this case are to get more features, make the model \n",
      "more complex, or change the model.\n",
      "Fixing high variance\n",
      "If, on the contrary, we suffer from high variance, that means that our model is too \n",
      "complex for the data. In this case, we can only try to get more data or decrease the \n",
      "complexity. This would mean to increase k so that more neighbors would be taken \n",
      "into account or to remove some of the features.\n",
      "High bias or low bias\n",
      "To find out what our problem actually is, we have to simply plot the train and test \n",
      "errors over the data size.\n",
      "High bias is typically revealed by the test error decreasing a bit at the beginning, but \n",
      "then settling at a very high value with the train error approaching with a growing \n",
      "dataset size. High variance is recognized by a big gap between both curves.\n",
      "Plotting the errors for different dataset sizes for 5NN shows a big gap between train \n",
      "and test errors, hinting at a high variance problem:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 130, 'page_label': '110'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 110 ]\n",
      "Looking at the graph, we immediately see that adding more training data will  \n",
      "not help, as the dashed line corresponding to the test error seems to stay above 0.4. \n",
      "The only option we have is to decrease the complexity, either by increasing k or by \n",
      "reducing the feature space.\n",
      "Reducing the feature space does not help here. We can easily confirm this by plotting \n",
      "the graph for a simplified feature space of only LinkCount and NumTextTokens:\n",
      "We get similar graphs for other smaller feature sets. No matter what subset of \n",
      "features we take, the graph would look similar.\n",
      "At least reducing the model complexity by increasing k shows some positive impact:\n",
      "k mean(scores) stddev(scores)\n",
      "40 0.62800 0.03750\n",
      "10 0.62000 0.04111\n",
      "5 0.61400 0.02154\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 131, 'page_label': '111'}\n",
      "Chapter 5\n",
      "[  111 ]\n",
      "But it is not enough, and also comes at a price of lower classification runtime \n",
      "performance. Take, for instance, k=40, where we have a very low test error. To \n",
      "classify a new post, we would need to find the 40 nearest other posts to decide \n",
      "whether the new post is a good one or not:\n",
      "Clearly, it seems to be an issue with using nearest neighbor for our scenario. And it \n",
      "has another real disadvantage. Over time, we will get more and more posts into our \n",
      "system. As the nearest neighbor method is an instance-based approach, we will have \n",
      "to store all posts in our system. The more we get, the slower the prediction will be. \n",
      "This is different with model-based approaches, where one tries to derive a model \n",
      "from the data.\n",
      "There we are, with enough reasons now to abandon the nearest neighbor approach \n",
      "to look for better places in the classification world. Of course, we will never know \n",
      "whether there is the one golden feature we just did not happen to think of. But for \n",
      "now, let's move on to another classification method that is known to work great in \n",
      "text-based classification scenarios.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 132, 'page_label': '112'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 112 ]\n",
      "Using logistic regression\n",
      "Contrary to its name, logistic regression is a classification method. It is a very \n",
      "powerful one when it comes to text-based classification; it achieves this by first  \n",
      "doing a regression on a logistic function, hence the name.\n",
      "A bit of math with a small example\n",
      "To get an initial understanding of the way logistic regression works, let's first take \n",
      "a look at the following example where we have artificial feature values X plotted \n",
      "with the corresponding classes, 0 or 1. As we can see, the data is noisy such that \n",
      "classes overlap in the feature value range between 1 and 6. Therefore, it is better to \n",
      "not directly model the discrete classes, but rather the probability that a feature value \n",
      "belongs to class 1, P(X). Once we possess such a model, we could then predict class 1 \n",
      "if P(X)>0.5, and class 0 otherwise.\n",
      "Mathematically, it is always difficult to model something that has a finite range, \n",
      "as is the case here with our discrete labels 0 and 1. We can, however, tweak the \n",
      "probabilities a bit so that they always stay between 0 and 1. And for that, we will \n",
      "need the odds ratio and the logarithm of it.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 133, 'page_label': '113'}\n",
      "Chapter 5\n",
      "[  113 ]\n",
      "Let's say a feature has the probability of 0.9 that it belongs to class 1, P(y=1) = 0.9. The \n",
      "odds ratio is then P(y=1)/P(y=0) = 0.9/0.1 = 9. We could say that the chance is 9:1 that \n",
      "this feature maps to class 1. If P(y=0.5), we would consequently have a 1:1 chance \n",
      "that the instance is of class 1. The odds ratio is bounded by 0, but goes to infinity \n",
      "(the left graph in the following set of graphs). If we now take the logarithm of it, we \n",
      "can map all probabilities between 0 and 1 to the full range from negative to positive \n",
      "infinity (the right graph in the following set of graphs). The nice thing is that we still \n",
      "maintain the relationship that higher probability leads to a higher log of odds, just \n",
      "not limited to 0 and 1 anymore.\n",
      "This means that we can now fit linear combinations of our features (OK, we only \n",
      "have one and a constant, but that will change soon) to the \n",
      " values. In a \n",
      "sense, we replace the linear from Chapter 1, Getting Started with Python Machine \n",
      "Learning, 0 1i iy c c x= +  with 0 1\n",
      "1\n",
      "i\n",
      "i\n",
      "plog c cxp\n",
      "  = +  \n",
      " (replacing y with log(odds)).\n",
      "We can solve this for pi, so that we have ( )0 1\n",
      "1\n",
      "1\n",
      "ii c c xp\n",
      "e\n",
      " +=\n",
      "+\n",
      ".\n",
      "We simply have to find the right coefficients, such that the formula gives the lowest \n",
      "errors for all our (xi, pi) pairs in our data set, but that will be done by scikit-learn.\n",
      "After fitting, the formula will give the probability for every new data point x that \n",
      "belongs to class 1:\n",
      ">>> from sklearn.linear_model import LogisticRegression\n",
      ">>> clf = LogisticRegression()\n",
      ">>> print(clf)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 134, 'page_label': '114'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 114 ]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False,  \n",
      "fit_intercept=True, intercept_scaling=1, penalty=l2, tol=0.0001)\n",
      ">>> clf.fit(X, y)\n",
      ">>> print(np.exp(clf.intercept_), np.exp(clf.coef_.ravel()))\n",
      "[ 0.09437188] [ 1.80094112]\n",
      ">>> def lr_model(clf, X):\n",
      "...     return 1 / (1 + np.exp(-(clf.intercept_ + clf.coef_*X)))\n",
      ">>> print(\"P(x=-1)=%.2f\\tP(x=7)=%.2f\"%(lr_model(clf, -1),  \n",
      "lr_model(clf, 7)))\n",
      "P(x=-1)=0.05    P(x=7)=0.85\n",
      "You might have noticed that scikit-learn exposes the first coefficient through the \n",
      "special field intercept_.\n",
      "If we plot the fitted model, we see that it makes perfect sense given the data:\n",
      "Applying logistic regression to our post classification problem\n",
      "Admittedly, the example in the previous section was created to show the beauty of \n",
      "logistic regression. How does it perform on the real, noisy data?\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 135, 'page_label': '115'}\n",
      "Chapter 5\n",
      "[  115 ]\n",
      "Comparing it to the best nearest neighbor classifier (k=40) as a baseline, we see that it \n",
      "performs a bit better, but also won't change the situation a whole lot.\n",
      "Method mean(scores) stddev(scores)\n",
      "LogReg C=0.1 0.64650 0.03139\n",
      "LogReg C=1.00  0.64650 0.03155\n",
      "LogReg C=10.00 0.64550 0.03102\n",
      "LogReg C=0.01 0.63850 0.01950\n",
      "40NN 0.62800 0.03750\n",
      "We have shown the accuracy for different values of the regularization parameter \n",
      "C. With it, we can control the model complexity, similar to the parameter k for the \n",
      "nearest neighbor method. Smaller values for C result in more penalization of the \n",
      "model complexity.\n",
      "A quick look at the bias-variance chart for one of our best candidates, C=0.1, shows \n",
      "that our model has high biastest and train error curves approach closely but stay \n",
      "at unacceptable high values. This indicates that logistic regression with the current \n",
      "feature space is under-fitting and cannot learn a model that captures the data correctly:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 136, 'page_label': '116'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 116 ]\n",
      "So what now? We switched the model and tuned it as much as we could with our \n",
      "current state of knowledge, but we still have no acceptable classifier.\n",
      "More and more it seems that either the data is too noisy for this task or that our set of \n",
      "features is still not appropriate to discriminate the classes well enough.\n",
      "Looking behind accuracy  precision and \n",
      "recall\n",
      "Let's step back and think again about what we are trying to achieve here. Actually, \n",
      "we do not need a classifier that perfectly predicts good and bad answers as we \n",
      "measured it until now using accuracy. If we can tune the classifier to be particularly \n",
      "good at predicting one class, we could adapt the feedback to the user accordingly. If \n",
      "we, for example, had a classifier that was always right when it predicted an answer \n",
      "to be bad, we would give no feedback until the classifier detected the answer to be \n",
      "bad. On the contrary, if the classifier exceeded in predicting answers to be good, we \n",
      "could show helpful comments to the user at the beginning and remove them when \n",
      "the classifier said that the answer is a good one.\n",
      "To find out in which situation we are here, we have to understand how to measure \n",
      "precision and recall. And to understand that, we have to look into the four distinct \n",
      "classification results as they are described in the following table:\n",
      "Classified as\n",
      "Positive Negative\n",
      "In reality \n",
      "it is\n",
      "Positive True positive (TP) False negative (FN)\n",
      "Negative False positive (FP) True negative (TN)\n",
      "For instance, if the classifier predicts an instance to be positive and the instance \n",
      "indeed is positive in reality, this is a true positive instance. If on the other hand the \n",
      "classifier misclassified that instance, saying that it is negative while in reality it was \n",
      "positive, that instance is said to be a false negative.\n",
      "What we want is to have a high success rate when we are predicting a post as either \n",
      "good or bad, but not necessarily both. That is, we want as much true positives as \n",
      "possible. This is what precision captures:\n",
      "TPPrecision TP FP= +\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 137, 'page_label': '117'}\n",
      "Chapter 5\n",
      "[  117 ]\n",
      "If instead our goal would have been to detect as much good or bad answers as \n",
      "possible, we would be more interested in recall:\n",
      "TPRecall TP FN= +\n",
      "In terms of the following graphic, precision is the fraction of the intersection of the \n",
      "right circle while recall is the fraction of the intersection of the left circle:\n",
      "So, how can we now optimize for precision? Up to now, we have always used 0.5  \n",
      "as the threshold to decide whether an answer is good or not. What we can do now  \n",
      "is count the number of TP, FP, and FN while varying that threshold between 0 and 1. \n",
      "With those counts, we can then plot precision over recall.\n",
      "The handy function precision_recall_curve() from the metrics module does all \n",
      "the calculations for us:\n",
      ">>> from sklearn.metrics import precision_recall_curve\n",
      ">>> precision, recall, thresholds = precision_recall_curve(y_test,  \n",
      "    clf.predict(X_test))\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 138, 'page_label': '118'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 118 ]\n",
      "Predicting one class with acceptable performance does not always mean that \n",
      "the classifier is also acceptable predicting the other class. This can be seen in the \n",
      "following two plots, where we plot the precision/recall curves for classifying bad \n",
      "(the left graph) and good (the right graph) answers:\n",
      "In the graphs, we have also included a much better description of \n",
      "a classifier's performance, the area under curve (AUC). It can be \n",
      "understood as the average precision of the classifier and is a great \n",
      "way of comparing different classifiers.\n",
      "We see that we can basically forget predicting bad answers (the left plot). Precision \n",
      "drops to a very low recall and stays at an unacceptably low 60 percent.\n",
      "Predicting good answers, however, shows that we can get above 80 percent precision \n",
      "at a recall of almost 40 percent. Let's find out what threshold we need for that. As we \n",
      "trained many classifiers on different folds (remember, we iterated over KFold() a \n",
      "couple of pages back), we need to retrieve the classifier that was neither too bad nor \n",
      "too good in order to get a realistic view. Let's call it the medium clone:\n",
      ">>> medium = np.argsort(scores)[int(len(scores) / 2)]\n",
      ">>> thresholds = np.hstack(([0],thresholds[medium]))\n",
      ">>> idx80 = precisions>=0.8\n",
      ">>> print(\"P=%.2f R=%.2f thresh=%.2f\" % (precision[idx80][0],  \n",
      "                                      recall[idx80][0], threshold[idx80]\n",
      "[0]))\n",
      "P=0.80 R=0.37 thresh=0.59\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 139, 'page_label': '119'}\n",
      "Chapter 5\n",
      "[  119 ]\n",
      "Setting the threshold at 0.59, we see that we can still achieve a precision of 80 \n",
      "percent detecting good answers when we accept a low recall of 37 percent. That \n",
      "means that we would detect only one in three good answers as such. But from that \n",
      "third of good answers that we manage to detect, we would be reasonably sure that \n",
      "they are indeed good. For the rest, we could then politely display additional hints on \n",
      "how to improve answers in general.\n",
      "To apply this threshold in the prediction process, we have to use predict_proba(), \n",
      "which returns per class probabilities, instead of predict(), which returns the  \n",
      "class itself:\n",
      ">>> thresh80 = threshold[idx80][0]\n",
      ">>> probs_for_good = clf.predict_proba(answer_features)[:,1]\n",
      ">>> answer_class = probs_for_good>thresh80\n",
      "We can confirm that we are in the desired precision/recall range using \n",
      "classification_report:\n",
      ">>> from sklearn.metrics import classification_report\n",
      ">>> print(classification_report(y_test, clf.predict_proba [:,1]>0.63,  \n",
      "    target_names=['not accepted', 'accepted']))\n",
      "            precision    recall  f1-score   support\n",
      "not accepted         0.59      0.85      0.70       101\n",
      "accepted             0.73      0.40      0.52        99\n",
      "avg / total          0.66      0.63      0.61       200\n",
      "Note that using the threshold will not guarantee that we are \n",
      "always above the precision and recall values that we determined \n",
      "above together with its threshold.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 140, 'page_label': '120'}\n",
      "Classification  Detecting Poor Answers\n",
      "[ 120 ]\n",
      "Slimming the classifier\n",
      "It is always worth looking at the actual contributions of the individual features. \n",
      "For logistic regression, we can directly take the learned coefficients (clf.coef_) \n",
      "to get an impression of the features' impact. The higher the coefficient of a feature, \n",
      "the more the feature plays a role in determining whether the post is good or \n",
      "not. Consequently, negative coefficients tell us that the higher values for the \n",
      "corresponding features indicate a stronger signal for the post to be classified as bad.\n",
      "We see that LinkCount, AvgWordLen, NumAllCaps, and NumExclams have the biggest \n",
      "impact on the overall classification decision, while NumImages (a feature that we \n",
      "sneaked in just for demonstration purposes a second ago) and AvgSentLen play a \n",
      "rather minor role. While the feature importance overall makes sense intuitively, it is \n",
      "surprising that NumImages is basically ignored. Normally, answers containing images \n",
      "are always rated high. In reality, however, answers very rarely have images. So, \n",
      "although in principal it is a very powerful feature, it is too sparse to be of any value. \n",
      "We could easily drop that feature and retain the same classification performance.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 141, 'page_label': '121'}\n",
      "Chapter 5\n",
      "[  121 ]\n",
      "Ship it!\n",
      "Let's assume we want to integrate this classifier into our site. What we definitely do \n",
      "not want is training the classifier each time we start the classification service. Instead, \n",
      "we can simply serialize the classifier after training and then deserialize on that site:\n",
      ">>> import pickle\n",
      ">>> pickle.dump(clf, open(\"logreg.dat\", \"w\"))\n",
      ">>> clf = pickle.load(open(\"logreg.dat\", \"r\"))\n",
      "Congratulations, the classifier is now ready to be used as if it had just been trained.\n",
      "Summary\n",
      "We made it! For a very noisy dataset, we built a classifier that suits a part of our goal. \n",
      "Of course, we had to be pragmatic and adapt our initial goal to what was achievable. \n",
      "But on the way we learned about strengths and weaknesses of nearest neighbor \n",
      "and logistic regression. We learned how to extract features such as LinkCount, \n",
      "NumTextTokens, NumCodeLines, AvgSentLen, AvgWordLen, NumAllCaps, NumExclams, \n",
      "and NumImages, and how to analyze their impact on the classifier's performance.\n",
      "But what is even more valuable is that we learned an informed way of how to debug \n",
      "bad performing classifiers. That will help us in the future to come up with usable \n",
      "systems much faster.\n",
      "After having looked into nearest neighbor and logistic regression, in the next  \n",
      "chapter, we will get familiar with yet another simple yet powerful classification \n",
      "algorithm: Nave Bayes. Along the way, we will also learn some more convenient \n",
      "tools from scikit-learn.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 142, 'page_label': '122'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 143, 'page_label': '123'}\n",
      "[ 123 ]\n",
      "Classification II  Sentiment \n",
      "Analysis\n",
      "For companies, it is vital to closely monitor the public reception of key events, such \n",
      "as product launches or press releases. With its real-time access and easy accessibility \n",
      "of user-generated content on Twitter, it is now possible to do sentiment classification \n",
      "of tweets. Sometimes also called opinion mining, it is an active field of research, in \n",
      "which several companies are already selling such services. As this shows that there \n",
      "obviously exists a market, we have motivation to use our classification muscles built \n",
      "in the last chapter, to build our own home-grown sentiment classifier.\n",
      "Sketching our roadmap\n",
      "Sentiment analysis of tweets is particularly hard, because of Twitter's size limitation \n",
      "of 140 characters. This leads to a special syntax, creative abbreviations, and seldom \n",
      "well-formed sentences. The typical approach of analyzing sentences, aggregating \n",
      "their sentiment information per paragraph, and then calculating the overall \n",
      "sentiment of a document does not work here.\n",
      "Clearly, we will not try to build a state-of-the-art sentiment classifier. Instead, we \n",
      "want to:\n",
      " Use this scenario as a vehicle to introduce yet another classification \n",
      "algorithm, Nave Bayes\n",
      " Explain how Part Of Speech (POS) tagging works and how it can help us\n",
      " Show some more tricks from the scikit-learn toolbox that come in handy from \n",
      "time to time\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 144, 'page_label': '124'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 124 ]\n",
      "Fetching the Twitter data\n",
      "Naturally, we need tweets and their corresponding labels that tell whether a tweet is \n",
      "containing a positive, negative, or neutral sentiment. In this chapter, we will use the \n",
      "corpus from Niek Sanders, who has done an awesome job of manually labeling more \n",
      "than 5,000 tweets and has granted us permission to use it in this chapter.\n",
      "To comply with Twitter's terms of services, we will not provide any data from \n",
      "Twitter nor show any real tweets in this chapter. Instead, we can use Sander's  \n",
      "hand-labeled data, which contains the tweet IDs and their hand-labeled sentiment, \n",
      "and use his script, install.py, to fetch the corresponding Twitter data. As the script \n",
      "is playing nice with Twitter's servers, it will take quite some time to download all the \n",
      "data for more than 5,000 tweets. So it is a good idea to start it right away.\n",
      "The data comes with four sentiment labels:\n",
      ">>> X, Y = load_sanders_data()\n",
      ">>> classes = np.unique(Y)\n",
      ">>> for c in classes: print(\"#%s: %i\" % (c, sum(Y==c)))\n",
      "#irrelevant: 490\n",
      "#negative: 487\n",
      "#neutral: 1952\n",
      "#positive: 433\n",
      "Inside load_sanders_data(), we are treating irrelevant and neutral labels together \n",
      "as neutral and drop ping all non-English tweets, resulting in 3,362 tweets.\n",
      "In case you get different counts here, it is because, in the meantime, tweets might \n",
      "have been deleted or set to be private. In that case, you might also get slightly \n",
      "different numbers and graphs than the ones shown in the upcoming sections.\n",
      "Introducing the Nave Bayes classifier\n",
      "Nave Bayes is probably one of the most elegant machine learning algorithms out \n",
      "there that is of practical use. And despite its name, it is not that nave when you look \n",
      "at its classification performance. It proves to be quite robust to irrelevant features, \n",
      "which it kindly ignores. It learns fast and predicts equally so. It does not require lots \n",
      "of storage. So, why is it then called nave?\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 145, 'page_label': '125'}\n",
      "Chapter 6\n",
      "[  125 ]\n",
      "The Nave was added to account for one assumption that is required for Nave Bayes \n",
      "to work optimally. The assumption is that the features do not impact each other. \n",
      "This, however, is rarely the case for real-world applications. Nevertheless, it still \n",
      "returns very good accuracy in practice even when the independence assumption \n",
      "does not hold.\n",
      "Getting to know the Bayes' theorem\n",
      "At its core, Nave Bayes classification is nothing more than keeping track of which \n",
      "feature gives evidence to which class. The way the features are designed determines \n",
      "the model that is used to learn. The so-called Bernoulli model only cares about \n",
      "Boolean features: whether a word occurs only once or multiple times in a tweet does \n",
      "not matter. In contrast, the Multinomial model uses word counts as features. For \n",
      "the sake of simplicity, we will use the Bernoulli model to explain how to use Nave \n",
      "Bayes for sentiment analysis. We will then use the Multinomial model later on to set \n",
      "up and tune our real-world classifiers.\n",
      "Let's assume the following meanings for the variables that we will use to explain \n",
      "Nave Bayes:\n",
      "Variable Meaning\n",
      "This is the class of a tweet (positive or negative)\n",
      "The word \"awesome\" occurs at least once in the tweet\n",
      "The word \"crazy\" occurs at least once in the tweet\n",
      "During training, we learned the Nave Bayes model, which is the probability for \n",
      "a class \n",
      "  when we already know features \n",
      "  and \n",
      " . This probability is written as \n",
      ".\n",
      "Since we cannot estimate \n",
      "  directly, we apply a trick, which was found out \n",
      "by Bayes:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 146, 'page_label': '126'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 126 ]\n",
      "If we substitute \n",
      "  with the probability of both words \"awesome\" and \"crazy\", and \n",
      "think of \n",
      "  as being our class \n",
      " , we arrive at the relationship that helps us to later \n",
      "retrieve the probability for the data instance belonging to the specified class:\n",
      "This allows us to express \n",
      "  by means of the other probabilities:\n",
      "We could also describe this as follows:\n",
      "The prior and the evidence are easily determined:\n",
      " \n",
      "  is the prior probability of class \n",
      "  without knowing about the data. We \n",
      "can estimate this quantity by simply calculating the fraction of all training \n",
      "data instances belonging to that particular class.\n",
      " \n",
      "  is the evidence or the probability of features \n",
      "  and \n",
      " .\n",
      "The tricky part is the calculation of the likelihood \n",
      " . It is the value \n",
      "describing how likely it is to see feature values \n",
      "  and \n",
      "  if we know that the  \n",
      "class of the data instance is \n",
      " . To estimate this, we need to do some thinking.\n",
      "Being nave\n",
      "From probability theory, we also know the following relationship:\n",
      "This alone, however, does not help much, since we treat one difficult problem \n",
      "(estimating \n",
      ") with another one (estimating \n",
      " ).\n",
      "However, if we navely assume that \n",
      "  and \n",
      "  are independent from each other, \n",
      " simplifies to \n",
      "  and we can write it as follows:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 147, 'page_label': '127'}\n",
      "Chapter 6\n",
      "[  127 ]\n",
      "Putting everything together, we get the quite manageable formula:\n",
      "The interesting thing is that although it is not theoretically correct to simply tweak \n",
      "our assumptions when we are in the mood to do so, in this case, it proves to work \n",
      "astonishingly well in real-world applications.\n",
      "Using Nave Bayes to classify\n",
      "Given a new tweet, the only part left is to simply calculate the probabilities:\n",
      "Then choose the class \n",
      "  having higher probability.\n",
      "As for both classes the denominator, \n",
      " , is the same, we can simply ignore it \n",
      "without changing the winner class.\n",
      "Note, however, that we don't calculate any real probabilities any more. Instead, we are \n",
      "estimating which class is more likely, given the evidence. This is another reason why \n",
      "Nave Bayes is so robust: It is not so much interested in the real probabilities, but only \n",
      "in the information, which class is more likely. In short, we can write:\n",
      "This is simply telling that we are calculating the part after argmax for all classes of \n",
      "  \n",
      "(pos and neg in our case) and returning the class that results in the highest value.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 148, 'page_label': '128'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 128 ]\n",
      "But, for the following example, let's stick to real probabilities and do some \n",
      "calculations to see how Nave Bayes works. For the sake of simplicity, we will \n",
      "assume that Twitter allows only for the two aforementioned words, \"awesome\"  \n",
      "and \"crazy\", and that we had already manually classified a handful of tweets:\n",
      "Tweet Class\n",
      "awesome Positive tweet\n",
      "awesome Positive tweet\n",
      "awesome crazy Positive tweet\n",
      "crazy Positive tweet\n",
      "crazy Negative tweet\n",
      "crazy Negative tweet\n",
      "In this example, we have the tweet \"crazy\" both in a positive and negative tweet to \n",
      "emulate some ambiguities you will often find in the real world (for example, \"being \n",
      "soccer crazy\" versus \"a crazy idiot\").\n",
      "In this case, we have six total tweets, out of which four are positive and two negative, \n",
      "which results in the following priors:\n",
      "This means, without knowing anything about the tweet itself, it would be wise to \n",
      "assume the tweet to be positive.\n",
      "A still missing piece is the calculation of \n",
      "  and \n",
      " , which are the \n",
      "probabilities for the two features \n",
      "  and \n",
      "  conditioned in class \n",
      " .\n",
      "This is calculated as the number of tweets, in which we have seen the concrete \n",
      "feature divided by the number of tweets that have been labeled with the class of \n",
      " . \n",
      "Let's say we want to know the probability of seeing \"awesome\" occurring in a tweet, \n",
      "knowing that its class is positive, we will have:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 149, 'page_label': '129'}\n",
      "Chapter 6\n",
      "[  129 ]\n",
      "Because out of the four positive tweets three contained the word \"awesome\". \n",
      "Obviously, the probability for not having \"awesome\" in a positive tweet is its inverse:\n",
      "Similarly, for the rest (omitting the case that a word is not occurring in a tweet):\n",
      "For the sake of completeness, we will also compute the evidence so that we can see \n",
      "real probabilities in the following example tweets. For two concrete values of \n",
      "  and \n",
      ", we can calculate the evidence as follows:\n",
      "This leads to the following values:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 150, 'page_label': '130'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 130 ]\n",
      "Now we have all the data to classify new tweets. The only work left is to parse the \n",
      "tweet and featurize it:\n",
      "Tweet\n",
      " Class probabilities Classification\n",
      "\"awesome\" 1 0\n",
      " Positive\n",
      "\"crazy\" 0 1\n",
      " Negative\n",
      "\"awesome \n",
      "crazy\"\n",
      "1 1\n",
      " Positive\n",
      "So far, so good. The classification of trivial tweets seems to assign correct labels to \n",
      "the tweets. The question remains, however, how we should treat words that did not \n",
      "occur in our training corpus. After all, with the preceding formula, new words will \n",
      "always be assigned a probability of zero.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 151, 'page_label': '131'}\n",
      "Chapter 6\n",
      "[  131 ]\n",
      "Accounting for unseen words and other \n",
      "oddities\n",
      "When we calculated the probabilities earlier, we actually cheated ourselves. We were \n",
      "not calculating the real probabilities, but only rough approximations by means of \n",
      "the fractions. We assumed that the training corpus will tell us the whole truth about \n",
      "the real probabilities. It did not. A corpus of only six tweets obviously cannot give us \n",
      "all the information about every tweet that has ever been written. For example, there \n",
      "certainly are tweets containing the word \"text\" in them. It is only that we have never \n",
      "seen them. Apparently, our approximation is very rough and we should account for \n",
      "that. This is often done in practice with the so-called add-one smoothing.\n",
      "Add-one smoothing is sometimes also referred to as additive \n",
      "smoothing or Laplace smoothing. Note that Laplace smoothing \n",
      "has nothing to do with Laplacian smoothing, which is related to the \n",
      "smoothing of polygon meshes. If we do not smooth by 1 but by an \n",
      "adjustable parameter alpha<0, it is called Lidstone smoothing.It is a very simple technique that adds one to all feature occurrences. It has the \n",
      "underlying assumption that even if we have not seen a given word in the whole \n",
      "corpus, there is still a chance that it is just that our sample of tweets happened to not \n",
      "include that word. So, with add-one smoothing we pretend that we have seen every \n",
      "occurrence once more than we actually did. That means that instead of calculating \n",
      ", we now do \n",
      " .\n",
      "Why do we add 2 in the denominator? Because we have two features: the occurrence \n",
      "of \"awesome\" and \"crazy\". Since we add 1 for each feature, we have to make sure that \n",
      "the end result is again a probability. And indeed, we get 1 as the total probability:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 152, 'page_label': '132'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 132 ]\n",
      "Accounting for arithmetic underflows\n",
      "There is yet another road block. In reality, we work with probabilities much smaller \n",
      "than the ones we have dealt with in the toy example. Typically, we also have many \n",
      "more than only two features, which we multiply with each other. This will quickly \n",
      "lead to the point where the accuracy provided by NumPy does not suffice any more:\n",
      ">>> import numpy as np\n",
      ">>> np.set_printoptions(precision=20) # tell numpy to print out more  \n",
      "digits (default is 8)\n",
      ">>> np.array([2.48E-324])\n",
      "array([ 4.94065645841246544177e-324])\n",
      ">>> np.array([2.47E-324])\n",
      "array([ 0.])\n",
      "So, how probable is it that we will ever hit a number like 2.47E-324? To answer this, \n",
      "we just need to imagine a likelihood for the conditional probabilities of 0.0001, and \n",
      "then multiply 65 of them together (meaning that we have 65 low probable feature \n",
      "values) and you've been hit by the arithmetic underflow:\n",
      ">>> x = 0.00001\n",
      ">>> x**64 # still fine\n",
      "1e-320\n",
      ">>> x**65 # ouch\n",
      "0.0\n",
      "A float in Python is typically implemented using double in C. To find out whether \n",
      "this is the case for your platform you can check it as follows:\n",
      ">>> import sys\n",
      ">>> sys.float_info\n",
      "sys.float_info(max=1.7976931348623157e+308, max_exp=1024,  \n",
      "max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021,  \n",
      "min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16,  \n",
      "radix=2, rounds=1)\n",
      "To mitigate this, one could switch to math libraries such as mpmath (http://code.\n",
      "google.com/p/mpmath/) that allow for arbitrary accuracy. However, they are not \n",
      "fast enough to work as a NumPy replacement.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 153, 'page_label': '133'}\n",
      "Chapter 6\n",
      "[  133 ]\n",
      "Fortunately, there is a better way to take care of this, and it has to do with a nice \n",
      "relationship that we might still remember from school:\n",
      "If we apply it to our case, we get the following:\n",
      "As the probabilities are in the interval between 0 and 1, the log of the probabilities \n",
      "lies in the interval - and 0. Don't be bothered with that. Higher numbers are still a \n",
      "stronger indicator for the correct classit is only that they are negative now.\n",
      "There is one caveat though: we actually don't have the log in the formula's nominator \n",
      "(the part above the fraction). We only have the product of the probabilities. In our \n",
      "case, luckily, we are not interested in the actual value of the probabilities. We simply \n",
      "want to know which class has the highest posterior probability. We are lucky, \n",
      "because if we find that \n",
      ", then we will always \n",
      "also have \n",
      " .\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 154, 'page_label': '134'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 134 ]\n",
      "A quick look at the preceding graph shows that the curve is monotonically increasing, \n",
      "that is, it never goes down, when we go from left to right. So let's stick this into the \n",
      "aforementioned formula:\n",
      "This will finally retrieve the formula for two features that will give us the best class \n",
      "also for the real-world data that we will see in practice:\n",
      "Of course, we will not be very successful with only two features, so, let's rewrite it to \n",
      "allow for an arbitrary number of features:\n",
      "There we are, ready to use our first classifier from the scikit-learn toolkit.\n",
      "As mentioned earlier, we just learned the Bernoulli model of Nave Bayes. Instead \n",
      "of having Boolean features, we can also use the number of word occurrences, also \n",
      "known as the Multinomial model. As this provides more information, and often \n",
      "also results in better performance, we will use this for our real-world data. Note, \n",
      "however, that the underlying formulas change a bit. However, no worries, as the \n",
      "general idea how Nave Bayes works, is still the same.\n",
      "Creating our first classifier and tuning it\n",
      "The Nave Bayes classifiers  resides in the sklearn.naive_bayes package. There are \n",
      "different kinds of Nave Bayes classifiers:\n",
      " GaussianNB: This classifier assumes the features to be normally distributed \n",
      "(Gaussian). One use case for it could be the classification of sex given the \n",
      "height and width of a person. In our case, we are given tweet texts from \n",
      "which we extract word counts. These are clearly not Gaussian distributed.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 155, 'page_label': '135'}\n",
      "Chapter 6\n",
      "[  135 ]\n",
      " MultinomialNB: This classifier assumes the features to be occurrence counts, \n",
      "which is our case going forward, since we will be using word counts in  \n",
      "the tweets as features. In practice, this classifier also works well with  \n",
      "TF-IDF vectors.\n",
      " BernoulliNB: This classifier is similar to MultinomialNB, but more suited \n",
      "when using binary word occurrences and not word counts.\n",
      "As we will mainly look at the word occurrences, for our purpose the MultinomialNB \n",
      "classifier is best suited.\n",
      "Solving an easy problem first\n",
      "As we have seen, when we looked at our tweet data, the tweets are not only positive \n",
      "or negative. The majority of tweets actually do not contain any sentiment, but are \n",
      "neutral or irrelevant, containing, for instance, raw information (for example, \"New \n",
      "book: Building Machine Learning  http://link\"). This leads to four classes. To not \n",
      "complicate the task too much, let's only focus on the positive and negative tweets  \n",
      "for now.\n",
      ">>> # first create a Boolean list having true for tweets\n",
      ">>> # that are either positive or negative\n",
      ">>> pos_neg_idx = np.logical_or(Y==\"positive\", Y==\"negative\")\n",
      ">>> # now use that index to filter the data and the labels\n",
      ">>> X = X[pos_neg_idx]\n",
      ">>> Y = Y[pos_neg_idx]\n",
      ">>> # finally convert the labels themselves into Boolean\n",
      ">>> Y = Y==\"positive\"\n",
      "Now, we have in X the raw tweet texts and in Y the binary classification, 0 for \n",
      "negative and 1 for positive tweets.\n",
      "We just said that we will use word occurrence counts as features. We will \n",
      "not use them in their raw form, though. Instead, we will use our power horse \n",
      "TfidfVectorizer to convert the raw tweet text into TF-IDF feature values, which \n",
      "we then use together with the labels to train our first classifier. For convenience, we \n",
      "will use the Pipeline class, which allows us to hook the vectorizer and the classifier \n",
      "together and provides the same interface:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 156, 'page_label': '136'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 136 ]\n",
      "from sklearn.pipeline import Pipeline\n",
      "def create_ngram_model():\n",
      "    tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3),  \n",
      "                                   analyzer=\"word\", binary=False)\n",
      "    clf = MultinomialNB()\n",
      "    return Pipeline([('vect', tfidf_ngrams), ('clf', clf)])\n",
      "The Pipeline instance returned by create_ngram_model() can now be used to fit \n",
      "and predict as if we had a normal classifier.\n",
      "Since we do not have that much data, we should do cross-validation. This time, \n",
      "however, we will not use KFold, which partitions the data in consecutive folds, but \n",
      "instead, we use ShuffleSplit. It shuffles the data for us, but does not prevent the \n",
      "same data instance to be in multiple folds. For each fold, then, we keep track of the \n",
      "area under the Precision-Recall curve and for accuracy.\n",
      "To keep our experimentation agile, let's wrap everything together in a train_\n",
      "model()function, which takes a function as a parameter that creates the classifier.\n",
      "from sklearn.metrics import precision_recall_curve, auc\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "def train_model(clf_factory, X, Y):\n",
      "    # setting random_state to get deterministic behavior\n",
      "    cv = ShuffleSplit(n=len(X), n_iter=10, test_size=0.3,  \n",
      "    random_state=0)\n",
      "    scores = []\n",
      "    pr_scores = []\n",
      "    for train, test in cv:\n",
      "        X_train, y_train = X[train], Y[train]\n",
      "        X_test, y_test = X[test], Y[test]\n",
      "        clf = clf_factory()\n",
      "        clf.fit(X_train, y_train)\n",
      "        train_score = clf.score(X_train, y_train)\n",
      "        test_score = clf.score(X_test, y_test)\n",
      "        scores.append(test_score)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 157, 'page_label': '137'}\n",
      "Chapter 6\n",
      "[  137 ]\n",
      "        proba = clf.predict_proba(X_test)\n",
      "        precision, recall, pr_thresholds =  \n",
      "        precision_recall_curve(y_test, proba[:,1])\n",
      "        pr_scores.append(auc(recall, precision))\n",
      "        summary = (np.mean(scores), np.std(scores),\n",
      "np.mean(pr_scores), np.std(pr_scores))\n",
      "        print(\"%.3f\\t%.3f\\t%.3f\\t%.3f\" % summary)\n",
      "Putting everything together, we can train our first model:\n",
      ">>> X, Y = load_sanders_data()\n",
      ">>> pos_neg_idx = np.logical_or(Y==\"positive\", Y==\"negative\")\n",
      ">>> X = X[pos_neg_idx]\n",
      ">>> Y = Y[pos_neg_idx]\n",
      ">>> Y = Y==\"positive\"\n",
      ">>> train_model(create_ngram_model, X, Y)\n",
      "0.788   0.024   0.882   0.036\n",
      "With our first try using Nave Bayes on vectorized TF-IDF trigram features, we get \n",
      "an accuracy of 78.8 percent and an average P/R AUC of 88.2 percent. Looking at the \n",
      "P/R chart of the median (the train/test split that is performing most similar to the \n",
      "average), it shows a much more encouraging behavior than the plots we have seen in \n",
      "the previous chapter.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 158, 'page_label': '138'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 138 ]\n",
      "For a start, the results are quite encouraging. They get even more impressive when \n",
      "we realize that 100 percent accuracy is probably never achievable in a sentiment \n",
      "classification task. For some tweets, even humans often do not really agree on the \n",
      "same classification label.\n",
      "Using all classes\n",
      "Once again, we simplified our task a bit, since we used only positive or negative \n",
      "tweets. That means, we assumed a perfect classifier that upfront classified whether \n",
      "the tweet contains a sentiment and forwarded that to our Nave Bayes classifier.\n",
      "So, how well do we perform if we also classify whether a tweet contains any \n",
      "sentiment at all? To find that out, let's first write a convenience function that returns \n",
      "a modified class array providing a list of sentiments that we would like to interpret \n",
      "as positive:\n",
      "def tweak_labels(Y, pos_sent_list):\n",
      "    pos = Y==pos_sent_list[0]\n",
      "    for sent_label in pos_sent_list[1:]:\n",
      "        pos |= Y==sent_label\n",
      "    Y = np.zeros(Y.shape[0])\n",
      "    Y[pos] = 1\n",
      "    Y = Y.astype(int)\n",
      "return Y\n",
      "Note that we are talking about two different positives now. The sentiment of a tweet \n",
      "can be positive, which is to be distinguished from the class of the training data. If, for \n",
      "example, we want to find out how good we can separate tweets having sentiment \n",
      "from neutral ones, we could do:\n",
      ">>> Y = tweak_labels(Y, [\"positive\", \"negative\"])\n",
      "In Y we have now 1 (positive class) for all tweets that are either positive or negative \n",
      "and 0 (negative class) for neutral and irrelevant ones.\n",
      ">>> train_model(create_ngram_model, X, Y, plot=True)\n",
      "0.750   0.012   0.659   0.023\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 159, 'page_label': '139'}\n",
      "Chapter 6\n",
      "[  139 ]\n",
      "Have a look at the following plot:\n",
      "As expected, P/R AUC drops considerably, being only 66 percent now. The accuracy is \n",
      "still high, but that is only due to the fact that we have a highly imbalanced dataset. Out \n",
      "of 3,362 total tweets, only 920 are either positive or negative, which is about 27 percent. \n",
      "This means, if we create a classifier that always classifies a tweet as not containing \n",
      "any sentiment, we will already have an accuracy of 73 percent. This is another case to \n",
      "always look at precision and recall if the training and test data is unbalanced.\n",
      "So, how will the Nave Bayes classifier perform on classifying positive tweets versus \n",
      "the rest and negative tweets versus the rest? One word: bad.\n",
      "== Pos vs. rest ==\n",
      "0.873   0.009   0.305   0.026\n",
      "== Neg vs. rest ==\n",
      "0.861   0.006   0.497   0.026\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 160, 'page_label': '140'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 140 ]\n",
      "Pretty unusable if you ask me. Looking at the P/R curves in the following plots,  \n",
      "we will also find no usable precision/recall trade-off, as we were able to do in the \n",
      "last chapter:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 161, 'page_label': '141'}\n",
      "Chapter 6\n",
      "[  141 ]\n",
      "Tuning the classifier's parameters\n",
      "Certainly, we have not explored the current setup enough and should  \n",
      "investigate more. There are roughly two areas, where we can play with the  \n",
      "knobs: TfidfVectorizer and MultinomialNB. As we have no real intuition  \n",
      "in which area we should explore, let's try to distribute the parameters' values.\n",
      "We will see the TfidfVectorizer parameter first:\n",
      " Using different settings for NGrams:\n",
      "  unigrams (1,1)\n",
      "  unigrams and bigrams (1,2)\n",
      "  unigrams, bigrams, and trigrams (1,3)\n",
      " Playing with min_df: 1 or 2\n",
      " Exploring the impact of IDF within TF-IDF using use_idf and smooth_idf: \n",
      "False or True\n",
      " Whether to remove stop words or not, by setting stop_words to english  \n",
      "or None\n",
      " Whether to use the logarithm of the word counts (sublinear_tf)\n",
      " Whether to track word counts or simply track whether words occur or not, \n",
      "by setting binary to True or False\n",
      "Now we will see the MultinomialNB classifier:\n",
      " Which smoothing method to use by setting alpha:\n",
      "  Add-one or Laplace smoothing: 1\n",
      "  Lidstone smoothing: 0.01, 0.05, 0.1, or 0.5\n",
      "  No smoothing: 0\n",
      "A simple approach could be to train a classifier for all those reasonable exploration \n",
      "values, while keeping the other parameters constant and check the classifier's results. \n",
      "As we do not know whether those parameters affect each other, doing it right will \n",
      "require that we train a classifier for every possible combination of all parameter \n",
      "values. Obviously, this is too tedious for us.\n",
      "Because this kind of parameter exploration occurs frequently in machine learning \n",
      "tasks, scikit-learn has a dedicated class for it, called GridSearchCV. It takes an \n",
      "estimator (instance with a classifier-like interface), which will be the Pipeline \n",
      "instance in our case, and a dictionary of parameters with their potential values.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 162, 'page_label': '142'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 142 ]\n",
      "GridSearchCV expects the dictionary's keys to obey a certain format so that it is able \n",
      "to set the parameters of the correct estimator. The format is as follows:\n",
      "<estimator>__<subestimator>__...__<param_name>\n",
      "For example, if we want to specify the desired values to explore for the min_df \n",
      "parameter of TfidfVectorizer (named vect in the Pipeline description), we \n",
      "would have to say:\n",
      "param_grid={\"vect__ngram_range\"=[(1, 1), (1, 2), (1, 3)]}\n",
      "This will tell GridSearchCV to try out unigrams to trigrams as parameter values for \n",
      "the ngram_range parameter of TfidfVectorizer.\n",
      "Then, it trains the estimator with all possible parameter value combinations. \n",
      "Here, we make sure that it trains on random samples of the training data using \n",
      "ShuffleSplit, which generates an iterator of random train/test splits. Finally, it \n",
      "provides the best estimator in the form of the member variable, best_estimator_.\n",
      "As we want to compare the returned best classifier with our current best one, we \n",
      "need to evaluate it in the same way. Therefore, we can pass the ShuffleSplit \n",
      "instance using the cv parameter (therefore, CV in GridSearchCV).\n",
      "The last missing piece is to define how GridSearchCV should determine the best \n",
      "estimator. This can be done by providing the desired score function to (surprise!) \n",
      "the score_func parameter. We can either write one ourselves, or pick one from the \n",
      "sklearn.metrics package. We should certainly not take metric.accuracy because \n",
      "of our class imbalance (we have a lot less tweets containing sentiment than neutral \n",
      "ones). Instead, we want to have good precision and recall on both classes, tweets \n",
      "with sentiment and tweets without positive or negative opinions. One metric that \n",
      "combines both precision and recall is the so-called F-measure, which is implemented \n",
      "as metrics.f1_score:\n",
      "After putting everything together, we get the following code:\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import f1_score\n",
      "def grid_search_model(clf_factory, X, Y):\n",
      "    cv = ShuffleSplit(\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 163, 'page_label': '143'}\n",
      "Chapter 6\n",
      "[  143 ]\n",
      "        n=len(X), n_iter=10, test_size=0.3,random_state=0)\n",
      "    param_grid = dict(vect__ngram_range=[(1, 1), (1, 2), (1, 3)],\n",
      "        vect__min_df=[1, 2],\n",
      "        vect__stop_words=[None, \"english\"],\n",
      "        vect__smooth_idf=[False, True],\n",
      "        vect__use_idf=[False, True],\n",
      "        vect__sublinear_tf=[False, True],\n",
      "        vect__binary=[False, True],\n",
      "        clf__alpha=[0, 0.01, 0.05, 0.1, 0.5, 1],\n",
      "        )\n",
      "    grid_search = GridSearchCV(clf_factory(),\n",
      "        param_grid=param_grid,\n",
      "        cv=cv,\n",
      "        score_func=f1_score,\n",
      "        verbose=10)\n",
      "    grid_search.fit(X, Y) \n",
      "    return grid_search.best_estimator_\n",
      "We have to be patient while executing this:\n",
      "clf = grid_search_model(create_ngram_model, X, Y)\n",
      "print(clf)\n",
      "Since we have just requested a parameter, sweep over \n",
      "  \n",
      "parameter combinations, each being trained on 10 folds:\n",
      "... waiting some hours  ...\n",
      "Pipeline(clf=MultinomialNB(\n",
      "alpha=0.01, class_weight=None, fit_prior=True),\n",
      "clf__alpha=0.01, \n",
      "clf__class_weight=None, \n",
      "clf__fit_prior=True,\n",
      "vect=TfidfVectorizer(\n",
      "analyzer=word, binary=False,\n",
      "   charset=utf-8, charset_error=strict, \n",
      "dtype=<type 'long'>,input=content,\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 164, 'page_label': '144'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 144 ]\n",
      "lowercase=True, max_df=1.0,\n",
      "max_features=None, max_n=None,\n",
      "min_df=1, min_n=None, ngram_range=(1, 2),\n",
      "norm=l2, preprocessor=None, smooth_idf=False,\n",
      "stop_words=None,strip_accents=None, \n",
      "sublinear_tf=True,token_pattern=(?u)\\b\\w\\w+\\b,\n",
      "token_processor=None, tokenizer=None, \n",
      "use_idf=False, vocabulary=None),\n",
      "vect__analyzer=word, vect__binary=False, \n",
      "vect__charset=utf-8,\n",
      "vect__charset_error=strict, \n",
      "vect__dtype=<type 'long'>,\n",
      "vect__input=content, vect__lowercase=True, \n",
      "vect__max_df=1.0,vect__max_features=None, \n",
      "vect__max_n=None, vect__min_df=1,\n",
      "vect__min_n=None, vect__ngram_range=(1, 2), \n",
      "vect__norm=l2, vect__preprocessor=None, \n",
      "vect__smooth_idf=False, vect__stop_words=None, \n",
      "vect__strip_accents=None, vect__sublinear_tf=True,\n",
      "vect__token_pattern=(?u)\\b\\w\\w+\\b,\n",
      "vect__token_processor=None, vect__tokenizer=None,\n",
      "vect__use_idf=False, vect__vocabulary=None)\n",
      "0.795  0.007  0.702  0.028\n",
      "The best estimator indeed improves the P/R AUC by nearly 3.3 percent to now 70.2, \n",
      "with the settings shown in the previous code.\n",
      "Also, the devastating results for positive tweets against the rest and negative tweets \n",
      "against the rest improve if we configure the vectorizer and classifier with those \n",
      "parameters we have just found out:\n",
      "== Pos vs. rest ==\n",
      "0.889   0.010   0.509   0.041\n",
      "== Neg vs. rest ==\n",
      "0.886   0.007   0.615   0.035\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 165, 'page_label': '145'}\n",
      "Chapter 6\n",
      "[  145 ]\n",
      "Have a look at the following plots:\n",
      "Indeed, the P/R curves look much better (note that the plots are from the medium of \n",
      "the fold classifiers, thus, slightly diverging AUC values). Nevertheless, we probably \n",
      "still wouldn't use those classifiers. Time for something completely different\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 166, 'page_label': '146'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 146 ]\n",
      "Cleaning tweets\n",
      "New constraints lead to new forms. Twitter is no exception in this regard. Because \n",
      "the text has to fit into 140 characters, people naturally develop new language \n",
      "shortcuts to say the same in less characters. So far, we have ignored all the diverse \n",
      "emoticons and abbreviations. Let's see how much we can improve by taking that into \n",
      "account. For this endeavor, we will have to provide our own preprocessor() to \n",
      "TfidfVectorizer.\n",
      "First, we define a range of frequent emoticons and their replacements in a dictionary. \n",
      "Although we can find more distinct replacements, we go with obvious positive or \n",
      "negative words to help the classifier:\n",
      "emo_repl = {\n",
      "    # positive emoticons\n",
      "    \"&lt;3\": \" good \",\n",
      "    \":d\": \" good \", # :D in lower case\n",
      "    \":dd\": \" good \", # :DD in lower case\n",
      "    \"8)\": \" good \",\n",
      "    \":-)\": \" good \",\n",
      "    \":)\": \" good \",\n",
      "    \";)\": \" good \",\n",
      "    \"(-:\": \" good \",\n",
      "    \"(:\": \" good \",\n",
      "    # negative emoticons:\n",
      "    \":/\": \" bad \",\n",
      "    \":&gt;\": \" sad \",\n",
      "    \":')\": \" sad \",\n",
      "    \":-(\": \" bad \",\n",
      "    \":(\": \" bad \",\n",
      "    \":S\": \" bad \",\n",
      "    \":-S\": \" bad \",\n",
      "    }\n",
      "# make sure that e.g. :dd is replaced before :d\n",
      "emo_repl_order = [k for (k_len,k) in reversed(sorted([(len(k),k) for k in \n",
      "emo_repl.keys()]))]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 167, 'page_label': '147'}\n",
      "Chapter 6\n",
      "[  147 ]\n",
      "Then, we define abbreviations as regular expressions together with their expansions \n",
      "(\\b marks the word boundary):\n",
      "re_repl = {\n",
      "r\"\\br\\b\": \"are\",\n",
      "r\"\\bu\\b\": \"you\",\n",
      "r\"\\bhaha\\b\": \"ha\",\n",
      "r\"\\bhahaha\\b\": \"ha\",\n",
      "r\"\\bdon't\\b\": \"do not\",\n",
      "r\"\\bdoesn't\\b\": \"does not\",\n",
      "r\"\\bdidn't\\b\": \"did not\",\n",
      "r\"\\bhasn't\\b\": \"has not\",\n",
      "r\"\\bhaven't\\b\": \"have not\",\n",
      "r\"\\bhadn't\\b\": \"had not\",\n",
      "r\"\\bwon't\\b\": \"will not\",\n",
      "r\"\\bwouldn't\\b\": \"would not\",\n",
      "r\"\\bcan't\\b\": \"can not\",\n",
      "r\"\\bcannot\\b\": \"can not\",\n",
      "    }\n",
      "def create_ngram_model(params=None):\n",
      "    def preprocessor(tweet):\n",
      "        tweet = tweet.lower()\n",
      "        for k in emo_repl_order:\n",
      "            tweet = tweet.replace(k, emo_repl[k])\n",
      "        for r, repl in re_repl.items():\n",
      "            tweet = re.sub(r, repl, tweet)\n",
      "        return tweet\n",
      "    tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor,\n",
      "analyzer=\"word\")\n",
      "    # ...\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 168, 'page_label': '148'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 148 ]\n",
      "Certainly, there are many more abbreviations that can be used here. But already with \n",
      "this limited set, we get an improvement for sentiment versus not sentiment of half a \n",
      "point, being now 70.7 percent:\n",
      "== Pos vs. neg ==\n",
      "0.808   0.024   0.885   0.029\n",
      "== Pos/neg vs. irrelevant/neutral ==\n",
      "0.793   0.010   0.685   0.024\n",
      "== Pos vs. rest ==\n",
      "0.890   0.011   0.517   0.041\n",
      "== Neg vs. rest ==\n",
      "0.886   0.006   0.624   0.033\n",
      "Taking the word types into account\n",
      "So far, our hope was that simply using the words independent of each other with \n",
      "the bag-of-words approach would suffice. Just from our intuition, however, neutral \n",
      "tweets probably contain a higher fraction of nouns, while positive or negative tweets \n",
      "are more colorful, requiring more adjectives and verbs. What if we use this linguistic \n",
      "information of the tweets as well? If we could find out how many words in a tweet \n",
      "were nouns, verbs, adjectives, and so on, the classifier could probably take that into \n",
      "account as well.\n",
      "Determining the word types\n",
      "This is what part-of-speech tagging, or POS tagging, is all about. A POS tagger parses \n",
      "a full sentence with the goal to arrange it into a dependence tree, where each node \n",
      "corresponds to a word and the parent-child relationship determines which word it \n",
      "depends on. With this tree, it can then make more informed decisions, for example, \n",
      "whether the word \"book\" is a noun (\"This is a good book.\") or a verb (\"Could you \n",
      "please book the flight?\").\n",
      "You might have already guessed that NLTK will play its role in this area as well. \n",
      "And indeed, it comes readily packaged with all sorts of parsers and taggers. The  \n",
      "POS tagger we will use, nltk.pos_tag(), is actually a full blown classifier trained \n",
      "using manually annotated sentences from the Penn Treebank Project (http://www.\n",
      "cis.upenn.edu/~treebank). It takes as input a list of word tokens and outputs a  \n",
      "list of tuples, where each element contains the part of the original sentence and its \n",
      "part-of-speech tag.\n",
      ">>> import nltk\n",
      ">>> nltk.pos_tag(nltk.word_tokenize(\"This is a good book.\"))\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 169, 'page_label': '149'}\n",
      "Chapter 6\n",
      "[  149 ]\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('book',  \n",
      "'NN'), ('.', '.')]\n",
      ">>> nltk.pos_tag(nltk.word_tokenize(\"Could you please book the  \n",
      "flight?\"))\n",
      "[('Could', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('book', 'NN'),  \n",
      "('the', 'DT'), ('flight', 'NN'), ('?', '.')]\n",
      "The POS tag abbreviations are taken from the Penn Treebank (adapted from  \n",
      "http://www.anc.org/OANC/penn.html):\n",
      "POS tag Description Example\n",
      "CC coordinating conjunction or\n",
      "CD cardinal number 2, second\n",
      "DT determiner the\n",
      "EX existential there there are\n",
      "FW foreign word kindergarten\n",
      "IN preposition/subordinating conjunction on, of, like\n",
      "JJ adjective cool\n",
      "JJR adjective, comparative cooler\n",
      "JJS adjective, superlative coolest\n",
      "LS list marker 1)\n",
      "MD modal could, will\n",
      "NN noun, singular or mass book\n",
      "NNS noun plural books\n",
      "NNP proper noun, singular Sean\n",
      "NNPS proper noun, plural Vikings\n",
      "PDT predeterminer both the boys\n",
      "POS possessive ending friend's\n",
      "PRP personal pronoun I, he, it\n",
      "PRP$ possessive pronoun my, his\n",
      "RB adverb however, usually, naturally, here, \n",
      "good\n",
      "RBR adverb, comparative better\n",
      "RBS adverb, superlative best\n",
      "RP particle give up\n",
      "TO to to go, to him\n",
      "UH interjection uhhuhhuhh\n",
      "VB verb, base form take\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 170, 'page_label': '150'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 150 ]\n",
      "POS tag Description Example\n",
      "VBD verb, past tense took\n",
      "VBG verb, gerund/present participle taking\n",
      "VBN verb, past participle taken\n",
      "VBP verb, sing. present, non-3d take\n",
      "VBZ verb, 3rd person sing. present takes\n",
      "WDT wh-determiner which\n",
      "WP wh-pronoun who, what\n",
      "WP$ possessive wh-pronoun whose\n",
      "WRB wh-abverb where, when\n",
      "With these tags, it is pretty easy to filter the desired tags from the output of pos_tag(). \n",
      "We simply have to count all words whose tags start with NN for nouns, VB for verbs,  \n",
      "JJ for adjectives, and RB for adverbs.\n",
      "Successfully cheating using SentiWordNet\n",
      "While linguistic information, as mentioned in the preceding section, will most  \n",
      "likely help us, there is something better we can do to harvest it: SentiWordNet \n",
      "(http://sentiwordnet.isti.cnr.it). Simply put, it is a 13 MB file that assigns \n",
      "most of the English words a positive and negative value. More complicated put,  \n",
      "for every synonym set, it records both the positive and negative sentiment values. \n",
      "Some examples are as follows:\n",
      "POS ID PosScore NegScore SynsetTerms Description\n",
      "a 00311354 0.25 0.125 studious#1 Marked by care and \n",
      "effort; \"made a studious \n",
      "attempt to fix the \n",
      "television set\"\n",
      "a 00311663 0 0.5 careless#1 Marked by lack of \n",
      "attention or consideration \n",
      "or forethought or \n",
      "thoroughness; not \n",
      "careful\n",
      "n 03563710 0 0 implant#1 A prosthesis placed \n",
      "permanently in tissue\n",
      "v 00362128 0 0 kink#2 \n",
      "curve#5 \n",
      "curl#1\n",
      "Form a curl, curve, or \n",
      "kink; \"the cigar smoke \n",
      "curled up at the ceiling\"\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 171, 'page_label': '151'}\n",
      "Chapter 6\n",
      "[  151 ]\n",
      "With the information in the POS column, we will be able to distinguish between the \n",
      "noun \"book\" and the verb \"book\". PosScore and NegScore together will help us to \n",
      "determine the neutrality of the word, which is 1-PosScore-NegScore. SynsetTerms \n",
      "lists all words in the set that are synonyms. We can safely ignore the ID and \n",
      "Description columns for our purposes.\n",
      "The synset terms have a number appended, because some occur multiple times in \n",
      "different synsets. For example, \"fantasize\" conveys two quite different meanings, \n",
      "which also leads to different scores:\n",
      "POS ID PosScore NegScore SynsetTerms Description\n",
      "v 01636859 0.375 0 fantasize#2 \n",
      "fantasise#2\n",
      "Portray in the mind; \"he \n",
      "is fantasizing the ideal \n",
      "wife\"\n",
      "v 01637368 0 0.125 fantasy#1 \n",
      "fantasize#1 \n",
      "fantasise#1\n",
      "Indulge in fantasies; \"he is \n",
      "fantasizing when he says \n",
      "he plans to start his own \n",
      "company\"\n",
      "To find out which of the synsets to take, we will need to really understand the \n",
      "meaning of the tweets, which is beyond the scope of this chapter. The field of \n",
      "research that is focusing on this challenge is called word-sense-disambiguation.  \n",
      "For our task, we take the easy route and simply average the scores over all the \n",
      "synsets, in which a term is found. For \"fantasize\", PosScore will be 0.1875 and \n",
      "NegScore will be 0.0625.\n",
      "The following function, load_sent_word_net(), does all that for us and returns  \n",
      "a dictionary where the keys are strings of the form word type/word, for example,  \n",
      "n/implant, and the values are the positive and negative scores:\n",
      "import csv, collections\n",
      "def load_sent_word_net():\n",
      "   # making our life easier by using a dictionary that\n",
      "   # automatically creates an empty list whenever we access\n",
      "   # a not yet existing key\n",
      "   sent_scores = collections.defaultdict(list)\n",
      "   with open(os.path.join(DATA_DIR, SentiWordNet_3.0.0_20130122.txt\"), \n",
      "\"r\") as csvfile:\n",
      "      reader = csv.reader(csvfile, delimiter='\\t',\n",
      "quotechar='\"')\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 172, 'page_label': '152'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 152 ]\n",
      "      for line in reader:\n",
      "         if line[0].startswith(\"#\"):\n",
      "            continue\n",
      "         if len(line)==1:\n",
      "            continue\n",
      "         POS, ID, PosScore, NegScore, SynsetTerms, Gloss = line\n",
      "         if len(POS)==0 or len(ID)==0:\n",
      "            continue\n",
      "         for term in SynsetTerms.split(\" \"):\n",
      "            # drop number at the end of every term\n",
      "            term = term.split(\"#\")[0] \n",
      "            term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n",
      "            key = \"%s/%s\"%(POS, term.split(\"#\")[0])\n",
      "            sent_scores[key].append((float(PosScore), \n",
      "float(NegScore)))\n",
      "    for key, value in sent_scores.items():\n",
      "        sent_scores[key] = np.mean(value, axis=0)\n",
      "    return sent_scores\n",
      "Our first estimator\n",
      "Now, we have everything in place to create our own first vectorizer. The most \n",
      "convenient way to do it is to inherit it from BaseEstimator. It requires us to  \n",
      "implement the following three methods:\n",
      " get_feature_names(): This returns a list of strings of the features that we \n",
      "will return in transform().\n",
      " fit(document, y=None): As we are not implementing a classifier, we can \n",
      "ignore this one and simply return self.\n",
      " transform(documents): This returns numpy.array(), containing an array of \n",
      "shape (len(documents), len(get_feature_names)). This means, for every \n",
      "document in documents, it has to return a value for every feature name in \n",
      "get_feature_names().\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 173, 'page_label': '153'}\n",
      "Chapter 6\n",
      "[  153 ]\n",
      "Here is the implementation:\n",
      "sent_word_net = load_sent_word_net()\n",
      "class LinguisticVectorizer(BaseEstimator):\n",
      "    def get_feature_names(self):\n",
      "        return np.array(['sent_neut', 'sent_pos', 'sent_neg',\n",
      "         'nouns', 'adjectives', 'verbs', 'adverbs',\n",
      "         'allcaps', 'exclamation', 'question', 'hashtag',  \n",
      "         'mentioning'])\n",
      "    # we don't fit here but need to return the reference\n",
      "    # so that it can be used like fit(d).transform(d)\n",
      "    def fit(self, documents, y=None):\n",
      "        return self\n",
      "    def _get_sentiments(self, d):\n",
      "        sent = tuple(d.split())\n",
      "        tagged = nltk.pos_tag(sent)\n",
      "        pos_vals = []\n",
      "        neg_vals = []\n",
      "        nouns = 0.\n",
      "        adjectives = 0.\n",
      "        verbs = 0.\n",
      "        adverbs = 0.\n",
      "        for w,t in tagged:\n",
      "            p, n = 0,0\n",
      "            sent_pos_type = None\n",
      "            if t.startswith(\"NN\"):\n",
      "                sent_pos_type = \"n\"\n",
      "                nouns += 1\n",
      "            elif t.startswith(\"JJ\"):\n",
      "                sent_pos_type = \"a\"\n",
      "                adjectives += 1\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 174, 'page_label': '154'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 154 ]\n",
      "            elif t.startswith(\"VB\"):\n",
      "                sent_pos_type = \"v\"\n",
      "                verbs += 1\n",
      "            elif t.startswith(\"RB\"):\n",
      "                sent_pos_type = \"r\"\n",
      "                adverbs += 1\n",
      "            if sent_pos_type is not None:\n",
      "                sent_word = \"%s/%s\" % (sent_pos_type, w)\n",
      "                if sent_word in sent_word_net:\n",
      "                    p,n = sent_word_net[sent_word]\n",
      "            pos_vals.append(p)\n",
      "            neg_vals.append(n)\n",
      "        l = len(sent)\n",
      "        avg_pos_val = np.mean(pos_vals)\n",
      "        avg_neg_val = np.mean(neg_vals)\n",
      "        return [1-avg_pos_val-avg_neg_val, avg_pos_val, avg_neg_val,  \n",
      "        nouns/l, adjectives/l, verbs/l, adverbs/l]\n",
      "    def transform(self, documents):\n",
      "        obj_val, pos_val, neg_val, nouns, adjectives, \\\n",
      "verbs, adverbs = np.array([self._get_sentiments(d) \\\n",
      "for d in documents]).T\n",
      "        allcaps = []\n",
      "        exclamation = []\n",
      "        question = []\n",
      "        hashtag = []\n",
      "        mentioning = []\n",
      "        for d in documents:\n",
      "            allcaps.append(np.sum([t.isupper() \\\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 175, 'page_label': '155'}\n",
      "Chapter 6\n",
      "[  155 ]\n",
      "                for t in d.split() if len(t)>2]))\n",
      "            exclamation.append(d.count(\"!\"))\n",
      "            question.append(d.count(\"?\"))\n",
      "            hashtag.append(d.count(\"#\"))\n",
      "            mentioning.append(d.count(\"@\"))\n",
      "        result = np.array([obj_val, pos_val, neg_val, nouns, adjectives, \n",
      "verbs, adverbs, allcaps, exclamation, question, \n",
      "hashtag, mentioning]).T\n",
      "        return result\n",
      "Putting everything together\n",
      "Nevertheless, using these linguistic features in isolation without the words \n",
      "themselves will not take us very far. Therefore, we have to combine the \n",
      "TfidfVectorizer parameter with the linguistic features. This can be done with \n",
      "scikit-learn's FeatureUnion class. It is initialized in the same manner as Pipeline; \n",
      "however, instead of evaluating the estimators in a sequence each passing the output \n",
      "of the previous one to the next one, FeatureUnion does it in parallel and joins the \n",
      "output vectors afterwards.\n",
      "def create_union_model(params=None):\n",
      "    def preprocessor(tweet):\n",
      "        tweet = tweet.lower()\n",
      "        for k in emo_repl_order:\n",
      "            tweet = tweet.replace(k, emo_repl[k])\n",
      "        for r, repl in re_repl.items():\n",
      "            tweet = re.sub(r, repl, tweet)\n",
      "        return tweet.replace(\"-\", \" \").replace(\"_\", \" \")\n",
      "    tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor,  \n",
      "    analyzer=\"word\")\n",
      "    ling_stats = LinguisticVectorizer()\n",
      "    all_features = FeatureUnion([('ling', ling_stats), ('tfidf',  \n",
      "    tfidf_ngrams)])\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 176, 'page_label': '156'}\n",
      "Classification II  Sentiment Analysis\n",
      "[ 156 ]\n",
      "    clf = MultinomialNB()\n",
      "    pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
      "    if params:\n",
      "        pipeline.set_params(**params)\n",
      "    return pipeline\n",
      "Training and testing on the combined featurizers, gives another 0.4 percent \n",
      "improvement on average P/R AUC for positive versus negative:\n",
      "== Pos vs. neg ==\n",
      "0.810   0.023   0.890   0.025\n",
      "== Pos/neg vs. irrelevant/neutral ==\n",
      "0.791   0.007   0.691   0.022\n",
      "== Pos vs. rest ==\n",
      "0.890   0.011   0.529   0.035\n",
      "== Neg vs. rest ==\n",
      "0.883   0.007   0.617   0.033\n",
      "time spent: 214.12578797340393\n",
      "With these results, we probably do not want to use the positive versus rest and \n",
      "negative versus rest classifiers, but instead use first the classifier determining \n",
      "whether the tweet contains sentiment at all (pos/neg versus irrelevant/neutral)  \n",
      "and then, in case it does, use the positive versus negative classifier to determine  \n",
      "the actual sentiment.\n",
      "Summary\n",
      "Congratulations for sticking with us until the end! Together we have learned how \n",
      "Nave Bayes works and why it is not that nave at all. Especially, for training sets, \n",
      "where we don't have enough data to learn all the niches in the class probability \n",
      "space, Nave Bayes does a great job of generalizing. We learned how to apply it to \n",
      "tweets and that cleaning the rough tweets' texts helps a lot. Finally, we realized that \n",
      "a bit of \"cheating\" (only after we have done our fair share of work) is okay. Especially \n",
      "when it gives another improvement of the classifier's performance, as we have \n",
      "experienced with the use of SentiWordNet.\n",
      "In the next chapter, we will look at regressions.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 177, 'page_label': '157'}\n",
      "[ 157 ]\n",
      "Regression\n",
      "You probably learned about regression in your high school mathematics class. The \n",
      "specific method you learned was probably what is called ordinary least squares \n",
      "(OLS) regression. This 200-year-old technique is computationally fast and can be \n",
      "used for many real-world problems. This chapter will start by reviewing it and \n",
      "showing you how it is available in scikit-learn.\n",
      "For some problems, however, this method is insufficient. This is particularly true \n",
      "when we have many features, and it completely fails when we have more features \n",
      "than datapoints. For those cases, we need more advanced methods. These methods \n",
      "are very modern, with major developments happening in the last decade. They go by \n",
      "names such as Lasso, Ridge, or ElasticNets. We will go into these in detail. They are \n",
      "also available in scikit-learn.\n",
      "Predicting house prices with regression\n",
      "Let's start with a simple problem, predicting house prices in Boston; a problem for \n",
      "which we can use a publicly available dataset. We are given several demographic \n",
      "and geographical attributes, such as the crime rate or the pupil-teacher ratio in the \n",
      "neighborhood. The goal is to predict the median value of a house in a particular area. \n",
      "As usual, we have some training data, where the answer is known to us.\n",
      "This is one of the built-in datasets that scikit-learn comes with, so it is very easy to \n",
      "load the data into memory:\n",
      ">>> from sklearn.datasets import load_boston\n",
      ">>> boston = load_boston()\n",
      "The boston object contains several attributes; in particular, boston.data contains \n",
      "the input data and boston.target contains the price of houses.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 178, 'page_label': '158'}\n",
      "Regression\n",
      "[  158 ]\n",
      "We will start with a simple one-dimensional regression, trying to regress the price  \n",
      "on a single attribute, the average number of rooms per dwelling in the neighborhood, \n",
      "which is stored at position 5 (you can consult boston.DESCR and boston.feature_\n",
      "names for detailed information on the data):\n",
      ">>> from matplotlib import pyplot as plt\n",
      ">>> plt.scatter(boston.data[:,5], boston.target, color='r')\n",
      "The boston.target attribute contains the average house price (our target variable). \n",
      "We can use the standard least squares regression you probably first saw in high-school. \n",
      "Our first attempt looks like this:\n",
      ">>> from sklearn.linear_model import LinearRegression\n",
      ">>> lr = LinearRegression()\n",
      "We import LinearRegression from the sklearn.linear_model module and \n",
      "construct a LinearRegression object. This object will behave analogously to  \n",
      "the classifier objects from scikit-learn that we used earlier.\n",
      ">>> x = boston.data[:,5]\n",
      ">>> y = boston.target\n",
      ">>> x = np.transpose(np.atleast_2d(x))\n",
      ">>> lr.fit(x, y)\n",
      ">>> y_predicted = lr.predict(x)\n",
      "The only nonobvious line in this code block is the call to np.atleast_2d, which \n",
      "converts x from a one-dimensional to a two-dimensional array. This conversion is \n",
      "necessary as the fit method expects a two-dimensional array as its first argument. \n",
      "Finally, for the dimensions to work out correctly, we need to transpose this array.\n",
      "Note that we are calling methods named fit and predict on the LinearRegression \n",
      "object, just as we did with classifier objects, even though we are now performing \n",
      "regression. This regularity in the API is one of the nicer features of scikit-learn.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 179, 'page_label': '159'}\n",
      "Chapter 7\n",
      "[  159 ]\n",
      "The preceding graph shows all the points (as dots) and our fit (the solid line). We can \n",
      "see that visually it looks good, except for a few outliers.\n",
      "Ideally, though, we would like to measure how good of a fit this is quantitatively. \n",
      "This will be critical in order to be able to compare alternative methods. To do so, we \n",
      "can measure how close our prediction is to the true values. For this task, we can use \n",
      "the mean_squared_error function from the sklearn.metrics module:\n",
      ">>> from sklearn.metrics import mean_squared_error\n",
      "This function takes two arguments, the true value and the predictions, as follows:\n",
      ">>> mse = mean_squared_error(y, lr.predict(x))\n",
      ">>> print(\"Mean squared error (of training data): {:.3}\".format(mse))\n",
      "Mean squared error (of training data): 58.4\n",
      "This value can sometimes be hard to interpret, and it's better to take the square root, \n",
      "to obtain the root mean square error (RMSE):\n",
      ">>> rmse = np.sqrt(mse)\n",
      ">>> print(\"RMSE (of training data): {:.3}\".format(rmse))\n",
      "RMSE (of training data): 6.6\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 180, 'page_label': '160'}\n",
      "Regression\n",
      "[  160 ]\n",
      "One advantage of using RMSE is that we can quickly obtain a very rough estimate of \n",
      "the error by multiplying it by two. In our case, we can expect the estimated price to \n",
      "be different from the real price by, at most, 13 thousand dollars.\n",
      "Root mean squared error and prediction\n",
      "Root mean squared error corresponds approximately to an estimate \n",
      "of the standard deviation. Since most data is at most two standard \n",
      "deviations from the mean, we can double our RMSE to obtain a rough \n",
      "confident interval. This is only completely valid if the errors are \n",
      "normally distributed, but it is often roughly correct even if they are not.\n",
      "A number such as 6.6 is still hard to immediately intuit. Is this a good prediction? \n",
      "One possible way to answer this question is to compare it with the most simple \n",
      "baseline, the constant model. If we knew nothing of the input, the best we could  \n",
      "do is predict that the output will always be the average value of y. We can then \n",
      "compare the mean-squared error of this model with the mean-squared error of the \n",
      "null model. This idea is formalized in the coefficient of determination, which is \n",
      "defined as follows:\n",
      "In this formula, yi represents the value of the element with index i, while iy  is the \n",
      "estimate for the same element obtained by the regression model. Finally, y  is the \n",
      "mean value of y, which represents the null model that always returns the same value. \n",
      "This is roughly the same as first computing the ratio of the mean squared error with \n",
      "the variance of the output and, finally, considering one minus this ratio. This way, \n",
      "a perfect model obtains a score of one, while the null model obtains a score of zero. \n",
      "Note that it is possible to obtain a negative score, which means that the model is so \n",
      "poor that one is better off using the mean as a prediction.\n",
      "The coefficient of determination can be obtained using r2_score of the sklearn.\n",
      "metrics module:\n",
      ">>> from sklearn.metrics import r2_score\n",
      ">>> r2 = r2_score(y, lr.predict(x))\n",
      ">>> print(\"R2 (on training data): {:.2}\".format(r2))\n",
      "R2 (on training data): 0.31\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 181, 'page_label': '161'}\n",
      "Chapter 7\n",
      "[  161 ]\n",
      "This measure is also called the R score. If you are using linear regression and \n",
      "evaluating the error on the training data, then it does correspond to the square  \n",
      "of the correlation coefficient, R. However, this measure is more general, and as  \n",
      "we discussed, may even return a negative value.\n",
      "An alternative way to compute the coefficient of determination is to use the score \n",
      "method of the LinearRegression object:\n",
      ">>> r2 = lr.score(x,y)\n",
      "Multidimensional regression\n",
      "So far, we have only used a single variable for prediction, the number of \n",
      "rooms per dwelling. We will now use all the data we have to fit a model, using \n",
      "multidimensional regression. We now try to predict a single output (the average \n",
      "house price) based on multiple inputs.\n",
      "The code looks very much like before. In fact, it's even simpler as we can now pass \n",
      "the value of boston.data directly to the fit method:\n",
      ">>> x = boston.data\n",
      ">>> y = boston.target\n",
      ">>> lr.fit(x, y)\n",
      "Using all the input variables, the root mean squared error is only 4.7, which \n",
      "corresponds to a coefficient of determination of 0.74. This is better than what we  \n",
      "had before, which indicates that the extra variables did help. We can no longer easily \n",
      "display the regression line as we did, because we have a 14-dimensional regression \n",
      "hyperplane instead of a single line.\n",
      "We can, however, plot the prediction versus the actual value. The code is as follows:\n",
      ">>> p = lr.predict(x)\n",
      ">>> plt.scatter(p, y)\n",
      ">>> plt.xlabel('Predicted price')\n",
      ">>> plt.ylabel('Actual price')\n",
      ">>> plt.plot([y.min(), y.max()], [[y.min()], [y.max()]])\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 182, 'page_label': '162'}\n",
      "Regression\n",
      "[  162 ]\n",
      "The last line plots a diagonal line that corresponds to perfect agreement. This aids \n",
      "with visualization. The results are shown in the following plot, where the solid line \n",
      "shows the diagonal (where all the points would lie if there was perfect agreement \n",
      "between the prediction and the underlying value):\n",
      "Cross-validation for regression\n",
      "If you remember when we first introduced classification, we stressed the importance \n",
      "of cross-validation for checking the quality of our predictions. In regression, this is \n",
      "not always done. In fact, we discussed only the training error in this chapter so far. \n",
      "This is a mistake if you want to confidently infer the generalization ability. Since \n",
      "ordinary least squares is a very simple model, this is often not a very serious mistake. \n",
      "In other words, the amount of overfitting is slight. However, we should still test this \n",
      "empirically, which we can easily do with scikit-learn.\n",
      "We will use the Kfold class to build a 5 fold cross-validation loop and test the \n",
      "generalization ability of linear regression:\n",
      ">>> from sklearn.cross_validation import Kfold\n",
      ">>> kf = KFold(len(x), n_folds=5)\n",
      ">>> p = np.zeros_like(y)\n",
      ">>> for train,test in kf:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 183, 'page_label': '163'}\n",
      "Chapter 7\n",
      "[  163 ]\n",
      "...    lr.fit(x[train], y[train])\n",
      "...    p[test] = lr.predict(x[test])\n",
      ">>> rmse_cv = np.sqrt(mean_squared_error(p, y))\n",
      ">>> print('RMSE on 5-fold CV: {:.2}'.format(rmse_cv))\n",
      "RMSE on 5-fold CV: 5.6\n",
      "With cross-validation, we obtain a more conservative estimate (that is, the error is \n",
      "larger): 5.6. As in the case of classification, the cross-validation estimate is a better \n",
      "estimate of how well we could generalize to predict on unseen data.\n",
      "Ordinary least squares is fast at learning time and returns a simple model, which \n",
      "is fast at prediction time. For these reasons, it should often be the first model that \n",
      "you try in a regression problem. However, we are now going to see more advanced \n",
      "methods and why they are sometimes preferable.\n",
      "Penalized or regularized regression\n",
      "This section introduces penalized regression, also called regularized regression, an \n",
      "important class of regression models.\n",
      "In ordinary regression, the returned fit is the best fit on the training data. This  \n",
      "can lead to over-fitting. Penalizing means that we add a penalty for over-confidence \n",
      "in the parameter values. Thus, we accept a slightly worse fit in order to have a \n",
      "simpler model.\n",
      "Another way to think about it is to consider that the default is that there is no \n",
      "relationship between the input variables and the output prediction. When we have \n",
      "data, we change this opinion, but adding a penalty means that we require more data \n",
      "to convince us that this is a strong relationship.\n",
      "Penalized regression is about tradeoffs\n",
      "Penalized regression is another example of the bias-variance tradeoff. \n",
      "When using a penalty, we get a worse fit in the training data, as we \n",
      "are adding bias. On the other hand, we reduce the variance and tend \n",
      "to avoid over-fitting. Therefore, the overall result might generalize \n",
      "better to unseen (test) data.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 184, 'page_label': '164'}\n",
      "Regression\n",
      "[  164 ]\n",
      "L1 and L2 penalties\n",
      "We now explore these ideas in detail. Readers who do not care about some of the \n",
      "mathematical aspects should feel free to skip directly to the next section on how to \n",
      "use regularized regression in scikit-learn.\n",
      "The problem, in general, is that we are given a matrix X of training data (rows are \n",
      "observations and each column is a different feature), and a vector y of output values. \n",
      "The goal is to obtain a vector of weights, which we will call b*. The ordinary least \n",
      "squares regression is given by the following formula:\n",
      "That is, we find vector b that minimizes the squared distance to the target y. In these \n",
      "equations, we ignore the issue of setting an intercept by assuming that the training \n",
      "data has been preprocessed so that the mean of y is zero.\n",
      "Adding a penalty or a regularization means that we do not simply consider the best \n",
      "fit on the training data, but also how vector \n",
      "  is composed. There are two types of \n",
      "penalties that are typically used for regression: L1 and L2 penalties. An L1 penalty \n",
      "means that we penalize the regression by the sum of the absolute values of the \n",
      "coefficients, while an L2 penalty penalizes by the sum of squares.\n",
      "When we add an L1 penalty, instead of the preceding equation, we instead optimize \n",
      "the following:\n",
      "Here, we are trying to simultaneously make the error small, but also make the values \n",
      "of the coefficients small (in absolute terms). Using an L2 penalty, means that we use \n",
      "the following formula:\n",
      "The difference is rather subtle: we now penalize by the square of the coefficient \n",
      "rather than their absolute value. However, the difference in the results is dramatic.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 185, 'page_label': '165'}\n",
      "Chapter 7\n",
      "[  165 ]\n",
      "Ridge, Lasso, and ElasticNets\n",
      "These penalized models often go by rather interesting names. The \n",
      "L1 penalized model is often called the Lasso, while an L2 penalized \n",
      "one is known as Ridge Regression. When using both, we call this \n",
      "an ElasticNet model.\n",
      "Both the Lasso and the Ridge result in smaller coefficients than unpenalized \n",
      "regression (smaller in absolute value, ignoring the sign). However, the Lasso has the \n",
      "additional property that it results in many coefficients being set to exactly zero! This \n",
      "means that the final model does not even use some of its input features, the model \n",
      "is sparse. This is often a very desirable property as the model performs both feature \n",
      "selection and regression in a single step.\n",
      "You will notice that whenever we add a penalty, we also add a weight , which \n",
      "governs how much penalization we want. When  is close to zero, we are very close to \n",
      "unpenalized regression (in fact, if you set  to zero, you will simply perform OLS), and \n",
      "when  is large, we have a model that is very different from the unpenalized one.\n",
      "The Ridge model is older as the Lasso is hard to compute with pen and paper. \n",
      "However, with modern computers, we can use the Lasso as easily as Ridge, or even \n",
      "combine them to form ElasticNets. An ElasticNet has two penalties, one for the \n",
      "absolute value and the other for the squares and it solves the following equation:\n",
      "This formula is a combination of the two previous ones, with two parameters, 1 and \n",
      "2. Later in this chapter, we will discuss how to choose a good value for parameters.\n",
      "Using Lasso or ElasticNet in scikit-learn\n",
      "Let's adapt the preceding example to use ElasticNets. Using scikit-learn, it is very \n",
      "easy to swap in the ElasticNet regressor for the least squares one that we had before:\n",
      ">>> from sklearn.linear_model import ElasticNet, Lasso\n",
      ">>> en = ElasticNet(alpha=0.5)\n",
      "Now, we use en, whereas earlier we had used lr. This is the only change that is \n",
      "needed. The results are exactly what we would have expected. The training error \n",
      "increases to 5.0 (it was 4.6 before), but the cross-validation error decreases to 5.4 (it \n",
      "was 5.6 before). We trade a larger error on the training data in order to gain better \n",
      "generalization. We could have tried an L1 penalty using the Lasso class or L2 using \n",
      "the Ridge class with the same code.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 186, 'page_label': '166'}\n",
      "Regression\n",
      "[  166 ]\n",
      "Visualizing the Lasso path\n",
      "Using scikit-learn, we can easily visualize what happens as the value of the \n",
      "regularization parameter (alpha) changes. We will again use the Boston data,  \n",
      "but now we will use the Lasso regression object:\n",
      ">>> las = Lasso(normalize=1)\n",
      ">>> alphas = np.logspace(-5, 2, 1000)\n",
      ">>> alphas, coefs, _= las.path(x, y, alphas=alphas)\n",
      "For each value in alphas, the path method on the Lasso object returns the \n",
      "coefficients that solve the lasso problem with that parameter value. Because  \n",
      "the result changes smoothly with alpha, this can be computed very efficiently.\n",
      "A typical way to visualize this path is to plot the value of the coefficients as alpha \n",
      "decreases. You can do so as follows:\n",
      ">>> fig,ax = plt.subplots()\n",
      ">>> ax.plot(alphas, coefs.T)\n",
      ">>> # Set log scale\n",
      ">>> ax.set_xscale('log')\n",
      ">>> # Make alpha decrease from left to right\n",
      ">>> ax.set_xlim(alphas.max(), alphas.min())\n",
      "This results in the following plot (we left out the trivial code that adds axis labels and \n",
      "the title):\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 187, 'page_label': '167'}\n",
      "Chapter 7\n",
      "[  167 ]\n",
      "In this plot, the x axis shows decreasing amounts of regularization from left to \n",
      "right (alpha is decreasing). Each line shows how a different coefficient varies as \n",
      "alpha changes. The plot shows that when using very strong regularization (left \n",
      "side, very high alpha), the best solution is to have all values be exactly zero. As the \n",
      "regularization becomes weaker, one by one, the values of the different coefficients \n",
      "first shoot up, then stabilize. At some point, they all plateau as we are probably \n",
      "already close to the unpenalized solution.\n",
      "P-greater-than-N scenarios\n",
      "The title of this section is a bit of inside jargon, which you will learn now.  \n",
      "Starting in the 1990s, first in the biomedical domain, and then on the Web, problems \n",
      "started to appear where P was greater than N. What this means is that the number \n",
      "of features, P, was greater than the number of examples, N (these letters were the \n",
      "conventional statistical shorthand for these concepts). These became known as P \n",
      "greater than N problems.\n",
      "For example, if your input is a set of written documents, a simple way to approach it \n",
      "is to consider each possible word in the dictionary as a feature and regress on those \n",
      "(we will later work on one such problem ourselves). In the English language, you \n",
      "have over 20,000 words (this is if you perform some stemming and only consider \n",
      "common words; it is more than ten times that if you skip this preprocessing step). \n",
      "If you only have a few hundred or a few thousand examples, you will have more \n",
      "features than examples.\n",
      "In this case, as the number of features is greater than the number of examples, it is \n",
      "possible to have a perfect fit on the training data. This is a mathematical fact, which is \n",
      "independent of your data. You are, in effect, solving a system of linear equations with \n",
      "fewer equations than variables. You can find a set of regression coefficients with zero \n",
      "training error (in fact, you can find more than one perfect solution, infinitely many).\n",
      "However, and this is a major problem, zero training error does not mean that your \n",
      "solution will generalize well. In fact, it may generalize very poorly. Whereas earlier \n",
      "regularization could give you a little extra boost, it is now absolutely required for  \n",
      "a meaningful result.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 188, 'page_label': '168'}\n",
      "Regression\n",
      "[  168 ]\n",
      "An example based on text documents\n",
      "We will now turn to an example that comes from a study performed at Carnegie \n",
      "Mellon University by Prof. Noah Smith's research group. The study was based \n",
      "on mining the so-called 10-K reports that companies file with the Securities and \n",
      "Exchange Commission (SEC) in the United States. This filing is mandated by law for \n",
      "all publicly traded companies. The goal of their study was to predict, based on this \n",
      "piece of public information, what the future volatility of the company's stock will be. \n",
      "In the training data, we are actually using historical data for which we already know \n",
      "what happened.\n",
      "There are 16,087 examples available. The features, which have already been \n",
      "preprocessed for us, correspond to different words, 150,360 in total. Thus, we have \n",
      "many more features than examples, almost ten times as much. In the introduction, it \n",
      "was stated that ordinary least regression fails in these cases and we will now see why \n",
      "by attempting to blindly apply it.\n",
      "The dataset is available in SVMLight format from multiple sources, including the \n",
      "book's companion website. This is a format that scikit-learn can read. SVMLight is, \n",
      "as the name says, a support vector machine implementation, which is also available \n",
      "through scikit-learn; right now, we are only interested in the file format:\n",
      ">>> from sklearn.datasets import load_svmlight_file\n",
      ">>> data,target = load_svmlight_file('E2006.train')\n",
      "In the preceding code, data is a sparse matrix (that is, most of its entries are zeros \n",
      "and, therefore, only the nonzero entries are saved in memory), while the target  \n",
      "is a simple one-dimensional vector. We can start by looking at some attributes  \n",
      "of the target:\n",
      ">>> print('Min target value: {}'.format(target.min()))\n",
      "Min target value: -7.89957807347\n",
      ">>> print('Max target value: {}'.format(target.max()))\n",
      "Max target value: -0.51940952694\n",
      ">>> print('Mean target value: {}'.format(target.mean()))\n",
      "Mean target value: -3.51405313669\n",
      ">>> print('Std. dev. target: {}'.format(target.std()))\n",
      "Std. dev. target: 0.632278353911\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 189, 'page_label': '169'}\n",
      "Chapter 7\n",
      "[  169 ]\n",
      "So, we can see that the data lies between -7.9 and -0.5. Now that we have a feel for \n",
      "the data, we can check what happens when we use OLS to predict. Note that we  \n",
      "can use exactly the same classes and methods as we did earlier:\n",
      ">>> from sklearn.linear_model import LinearRegression\n",
      ">>> lr = LinearRegression()\n",
      ">>> lr.fit(data,target)\n",
      ">>> pred = lr.predict(data)\n",
      ">>> rmse_train = np.sqrt(mean_squared_error(target, pred))\n",
      ">>> print('RMSE on training: {:.2}'.format(rmse_train))\n",
      "RMSE on training: 0.0025\n",
      ">>> print('R2 on training: {:.2}'.format(r2_score(target, pred)))\n",
      "R2 on training: 1.0\n",
      "The root mean squared error is not exactly zero because of rounding errors, but \n",
      "it is very close. The coefficient of determination is 1.0. That is, the linear model is \n",
      "reporting a perfect prediction on its training data.\n",
      "When we use cross-validation (the code is very similar to what we used earlier in the \n",
      "Boston example), we get something very different: RMSE of 0.75, which corresponds \n",
      "to a negative coefficient of determination of -0.42. This means that if we always \n",
      "\"predict\" the mean value of -3.5, we do better than when using the regression model!\n",
      "Training and generalization error\n",
      "When the number of features is greater than the number of examples, \n",
      "you always get zero training errors with OLS, except perhaps for \n",
      "issues due to rounding off. However, this is rarely a sign that your \n",
      "model will do well in terms of generalization. In fact, you may get zero \n",
      "training error and have a completely useless model.\n",
      "The natural solution is to use regularization to counteract the overfitting. We can \n",
      "try the same cross-validation loop with an ElasticNet learner, having set the penalty \n",
      "parameter to 0.1:\n",
      ">>> from sklearn.linear_model import ElasticNet\n",
      ">>> met = ElasticNet(alpha=0.1)\n",
      ">>> kf = KFold(len(target), n_folds=5)\n",
      ">>> pred = np.zeros_like(target)\n",
      ">>> for train, test in kf:\n",
      "...    met.fit(data[train], target[train])\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 190, 'page_label': '170'}\n",
      "Regression\n",
      "[  170 ]\n",
      "...    pred[test] = met.predict(data[test])\n",
      ">>> # Compute RMSE\n",
      ">>> rmse = np.sqrt(mean_squared_error(target, pred))\n",
      ">>> print('[EN 0.1] RMSE on testing (5 fold): {:.2}'.format(rmse))\n",
      "[EN 0.1] RMSE on testing (5 fold): 0.4\n",
      ">>> # Compute Coefficient of determination\n",
      ">>> r2 = r2_score(target, pred)\n",
      ">>> print('[EN 0.1] R2 on testing (5 fold): {:.2}'.format(r2))\n",
      "[EN 0.1] R2 on testing (5 fold): 0.61\n",
      "Now, we get 0.4 RMSE and an R2 of 0.61, much better than just predicting the \n",
      "mean. There is one problem with this solution, though, which is the choice of alpha. \n",
      "When using the default value (1.0), the result is very different (and worse).\n",
      "In this case, we cheated as the author had previously tried a few values to see which \n",
      "ones would give a good result. This is not effective and can lead to over estimates of \n",
      "confidence (we are looking at the test data to decide which parameter values to use \n",
      "and which we should never use). The next section explains how to do it properly and \n",
      "how this is supported by scikit-learn.\n",
      "Setting hyperparameters in a principled way\n",
      "In the preceding example, we set the penalty parameter to 0.1. We could just as well \n",
      "have set it to 0.7 or 23.9. Naturally, the results vary each time. If we pick an overly \n",
      "large value, we get underfitting. In the extreme case, the learning system will just \n",
      "return every coefficient equal to zero. If we pick a value that is too small, we are very \n",
      "close to OLS, which overfits and generalizes poorly (as we saw earlier).\n",
      "How do we choose a good value? This is a general problem in machine  \n",
      "learning: setting parameters for our learning methods. A generic solution is to use \n",
      "cross-validation. We pick a set of possible values, and then use cross-validation to \n",
      "choose which one is best. This performs more computation (five times more if we  \n",
      "use five folds), but is always applicable and unbiased.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 191, 'page_label': '171'}\n",
      "Chapter 7\n",
      "[  171 ]\n",
      "We must be careful, though. In order to obtain an estimate of generalization, we have \n",
      "to use two-levels of cross-validation: one level is to estimate the generalization, while \n",
      "the second level is to get good parameters. That is, we split the data in, for example, \n",
      "five folds. We start by holding out the first fold and will learn on the other four. Now, \n",
      "we split these again into 5 folds in order to choose the parameters. Once we have set \n",
      "our parameters, we test on the first fold. Now, we repeat this four other times:\n",
      "The preceding figure shows how you break up a single training fold into subfolds. \n",
      "We would need to repeat it for all the other folds. In this case, we are looking at five \n",
      "outer folds and five inner folds, but there is no reason to use the same number of \n",
      "outer and inner folds, you can use any number you want as long as you keep the \n",
      "folds separate.\n",
      "This leads to a lot of computation, but it is necessary in order to do things correctly. \n",
      "The problem is that if you use a piece of data to make any decisions about your \n",
      "model (including which parameters to set), you have contaminated it and you can \n",
      "no longer use it to test the generalization ability of your model. This is a subtle point \n",
      "and it may not be immediately obvious. In fact, it is still the case that many users of \n",
      "machine learning get this wrong and overestimate how well their systems are doing, \n",
      "because they do not perform cross-validation correctly!\n",
      "Fortunately, scikit-learn makes it very easy to do the right thing; it provides  \n",
      "classes named LassoCV, RidgeCV, and ElasticNetCV, all of which encapsulate  \n",
      "an inner cross-validation loop to optimize for the necessary parameter. The code  \n",
      "is almost exactly like the previous one, except that we do not need to specify any \n",
      "value for alpha:\n",
      ">>> from sklearn.linear_model import ElasticNetCV\n",
      ">>> met = ElasticNetCV()\n",
      ">>> kf = KFold(len(target), n_folds=5)\n",
      ">>> p = np.zeros_like(target)\n",
      ">>> for train,test in kf:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 192, 'page_label': '172'}\n",
      "Regression\n",
      "[  172 ]\n",
      "...    met.fit(data[train],target[train])\n",
      "...    p[test] = met.predict(data[test])\n",
      ">>> r2_cv = r2_score(target, p)\n",
      ">>> print(\"R2 ElasticNetCV: {:.2}\".format(r2_cv))\n",
      "R2 ElasticNetCV: 0.65\n",
      "This results in a lot of computation, so you may want to get some coffee while \n",
      "you are waiting (depending on how fast your computer is). You might get better \n",
      "performance by taking advantage of multiple processors. This is a built-in feature of \n",
      "scikit-learn, which can be accessed quite trivially by using the n_jobs parameter to \n",
      "the ElasticNetCV constructor. To use four CPUs, make use of the following code:\n",
      ">>> met = ElasticNetCV(n_jobs=4)\n",
      "Set the n_jobs parameter to -1 to use all the available CPUs:\n",
      ">>> met = ElasticNetCV(n_jobs=-1)\n",
      "You may have wondered why, if ElasticNets have two penalties, the L1 and the \n",
      "L2 penalty, we only need to set a single value for alpha. In fact, the two values are \n",
      "specified by separately specifying alpha and the l1_ratio variable (that is spelled  \n",
      "ell-1-underscore-ratio). Then, 1 and 2 are set as follows (where  stands for l1_ratio):\n",
      "In an intuitive sense, alpha sets the overall amount of regularization while l1_ratio \n",
      "sets the tradeoff between the different types of regularization, L1 and L2.\n",
      "We can request that the ElasticNetCV object tests different values of l1_ratio, as is \n",
      "shown in the following code:\n",
      ">>> l1_ratio=[.01, .05, .25, .5, .75, .95, .99]\n",
      ">>> met = ElasticNetCV(\n",
      "                l1_ratio=l1_ratio,\n",
      "                n_jobs=-1)\n",
      "This set of l1_ratio values is recommended in the documentation. It will test \n",
      "models that are almost like Ridge (when l1_ratio is 0.01 or 0.05) as well as models \n",
      "that are almost like Lasso (when l1_ratio is 0.95 or 0.99). Thus, we explore a full \n",
      "range of different options.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 193, 'page_label': '173'}\n",
      "Chapter 7\n",
      "[  173 ]\n",
      "Because of its flexibility and the ability to use multiple CPUs, ElasticNetCV is \n",
      "an excellent default solution for regression problems when you don't have any \n",
      "particular reason to prefer one type of model over the rest.\n",
      "Putting all this together, we can now visualize the prediction versus real fit on this \n",
      "large dataset:\n",
      ">>> l1_ratio = [.01, .05, .25, .5, .75, .95, .99]\n",
      ">>> met = ElasticNetCV(\n",
      "                l1_ratio=l1_ratio,\n",
      "                n_jobs=-1)\n",
      ">>> p = np.zeros_like(target)\n",
      ">>> for train,test in kf:\n",
      "...     met.fit(data[train],target[train])\n",
      "...    p[test] = met.predict(data[test])\n",
      ">>> plt.scatter(p, y)\n",
      ">>> # Add diagonal line for reference\n",
      ">>> # (represents perfect agreement)\n",
      ">>> plt.plot([p.min(), p.max()], [p.min(), p.max()])\n",
      "This results in the following plot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 194, 'page_label': '174'}\n",
      "Regression\n",
      "[  174 ]\n",
      "We can see that the predictions do not match very well on the bottom end of the value \n",
      "range. This is perhaps because there are so many fewer elements on this end of the \n",
      "target range (which also implies that this affects only a small minority of datapoints).\n",
      "One last note: the approach of using an inner cross-validation loop to set a parameter \n",
      "is also available in scikit-learn using a grid search. In fact, we already used it in the \n",
      "previous chapter.\n",
      "Summary\n",
      "In this chapter, we started with the oldest trick in the book, ordinary least squares \n",
      "regression. Although centuries old, it is still often the best solution for regression. \n",
      "However, we also saw more modern approaches that avoid overfitting and can give \n",
      "us better results especially when we have a large number of features. We used Ridge, \n",
      "Lasso, and ElasticNets; these are the state-of-the-art methods for regression.\n",
      "We saw, once again, the danger of relying on training error to estimate \n",
      "generalization: it can be an overly optimistic estimate to the point where our model \n",
      "has zero training error, but we know that it is completely useless. When thinking \n",
      "through these issues, we were led into two-level cross-validation, an important point \n",
      "that many in the field still have not completely internalized.\n",
      "Throughout this chapter, we were able to rely on scikit-learn to support all the \n",
      "operations we wanted to perform, including an easy way to achieve correct \n",
      "cross-validation. ElasticNets with an inner cross-validation loop for parameter \n",
      "optimization (as implemented in scikit-learn by ElasticNetCV) should probably \n",
      "become your default method for regression.\n",
      "One reason to use an alternative is when you are interested in a sparse solution. In \n",
      "this case, a pure Lasso solution is more appropriate as it will set many coefficients \n",
      "to zero. It will also allow you to discover from the data a small number of variables, \n",
      "which are important to the output. Knowing the identity of these may be interesting \n",
      "in and of itself, in addition to having a good regression model.\n",
      "In the next chapter, we will look at recommendations, another machine learning \n",
      "problem. Our first approach will be to use regression to predict consumer product \n",
      "ratings. We will then see alternative models to generate recommendations.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 195, 'page_label': '175'}\n",
      "[ 175 ]\n",
      "Recommendations\n",
      "Recommendations have become one of the staples of online services and commerce. \n",
      "This type of automated system can provide each user with a personalized list of \n",
      "suggestions (be it a list of products to purchase, features to use, or new connections). \n",
      "In this chapter, we will see the basic ways in which automated recommendation \n",
      "generation systems work. The field of recommendation based on consumer inputs is \n",
      "often called collaborative filtering, as the users collaborate through the system to find \n",
      "the best items for each other.\n",
      "In the first part of this chapter, we will see how we can use past product ratings from \n",
      "consumers to predict new ratings. We start with a few ideas that are helpful and then \n",
      "combine all of them. When combining, we use regression to learn the best way in \n",
      "they can be combined. This will also allow us to explore a generic concept in machine \n",
      "learning: ensemble learning.\n",
      "In the second part of this chapter, we will take a look at a different way of learning \n",
      "recommendations: basket analysis. Unlike the case in which we have numeric ratings, \n",
      "in the basket analysis setting, all we have is information about the shopping baskets, \n",
      "that is, what items were bought together. The goal is to learn about recommendations. \n",
      "You have probably already seen features of the form \"people who bought X also \n",
      "bought Y\" in online shopping. We will develop a similar feature of our own.\n",
      "Rating predictions and recommendations\n",
      "If you have used any online shopping system in the last 10 years, you have  \n",
      "probably seen these recommendations. Some are like Amazon's \"costumers who \n",
      "bought X also bought Y\". These will be dealt with later in the chapter in the Basket \n",
      "analysis section. Other recommendations are based on predicting the rating of a \n",
      "product, such as a movie.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 196, 'page_label': '176'}\n",
      "Recommendations\n",
      "[  176 ]\n",
      "The problem of learning recommendations based on past product ratings was made \n",
      "famous by the Netflix Prize, a million-dollar machine learning public challenge by \n",
      "Netflix. Netflix (well-known in the USA and UK and in a process of international \n",
      "expansion) is a movie rental company. Traditionally, you would receive DVDs in \n",
      "the mail; more recently, Netflix has focused on the online streaming of movies and \n",
      "TV shows. From the start, one of the distinguishing features of the service was that it \n",
      "gives users the option to rate the films they have seen. Netflix then uses these ratings \n",
      "to recommend other films to its customers. In this machine learning problem, you \n",
      "not only have the information about which films the user saw but also about how the \n",
      "user rated them.\n",
      "In 2006, Netflix made a large number of customer ratings of films in its database \n",
      "available for a public challenge. The goal was to improve on their in-house algorithm \n",
      "for rating prediction. Whoever would be able to beat it by 10 percent or more would \n",
      "win 1 million dollars. In 2009, an international team named BellKor's Pragmatic \n",
      "Chaos was able to beat this mark and take the prize. They did so just 20 minutes \n",
      "before another team, The Ensemble, and passed the 10 percent mark as wellan \n",
      "exciting photo finish for a competition that lasted several years.\n",
      "Machine learning in the real world\n",
      "Much has been written about the Netflix Prize, and you may learn a \n",
      "lot by reading up on it. The techniques that won were a mixture of \n",
      "advanced machine learning and a lot of work put into preprocessing \n",
      "the data. For example, some users like to rate everything very highly, \n",
      "while others are always more negative; if you do not account for this in \n",
      "preprocessing, your model will suffer. Other normalizations were also \n",
      "necessary for a good result: how old is the film and how many ratings \n",
      "did it receive. Good algorithms are a good thing, but you always need \n",
      "to \"get your hands dirty\" and tune your methods to the properties of the \n",
      "data you have in front of you. Preprocessing and normalizing the data \n",
      "is often the most time-consuming part of the machine learning process. \n",
      "However, this is also the place where one can have the biggest impact \n",
      "on the final performance of the system.\n",
      "The first thing to note about the Netflix Prize is how hard it was. Roughly \n",
      "speaking, the internal system that Netflix used was about 10 percent better than no \n",
      "recommendations (that is, assigning each movie just the average value for all users). \n",
      "The goal was to obtain just another 10 percent improvement on this. In total, the \n",
      "winning system was roughly just 20 percent better than no personalization. Yet, it \n",
      "took a tremendous amount of time and effort to achieve this goal. And even though \n",
      "20 percent does not seem like much, the result is a system that is useful in practice.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 197, 'page_label': '177'}\n",
      "Chapter 8\n",
      "[  177 ]\n",
      "Unfortunately, for legal reasons, this dataset is no longer available. Although the \n",
      "data was anonymous, there were concerns that it might be possible to discover who \n",
      "the clients were and reveal private details of movie rentals. However, we can use an \n",
      "academic dataset with similar characteristics. This data comes from GroupLens, a \n",
      "research laboratory at the University of Minnesota.\n",
      "How can we solve a Netflix style ratings prediction question? We will see two \n",
      "different approaches, neighborhood approaches and regression approaches.  \n",
      "We will also see how to combine these to obtain a single prediction.\n",
      "Splitting into training and testing\n",
      "At a high level, splitting the dataset into training and testing data in order to obtain \n",
      "a principled estimate of the system's performance is performed as in previous \n",
      "chapters: we take a certain fraction of our data points (we will use 10 percent) and \n",
      "reserve them for testing; the rest will be used for training. However, because the data \n",
      "is structured differently in this context, the code is different. The first step is to load \n",
      "the data from disk, for which we use the following function:\n",
      "def load():\n",
      "    import numpy as np\n",
      "    from scipy import sparse\n",
      "    data = np.loadtxt('data/ml-100k/u.data')\n",
      "    ij = data[:, :2]\n",
      "    ij -= 1  # original data is in 1-based system\n",
      "    values = data[:, 2]\n",
      "    reviews = sparse.csc_matrix((values, ij.T)).astype(float)\n",
      "    return reviews.toarray()\n",
      "Note that zero entries in this matrix represent missing ratings.\n",
      ">>> reviews = load()\n",
      ">>> U,M = np.where(reviews)\n",
      "We now use the standard random module to choose indices to test:\n",
      ">>> import random\n",
      ">>> test_idxs = np.array(random.sample(range(len(U)), len(U)//10))\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 198, 'page_label': '178'}\n",
      "Recommendations\n",
      "[  178 ]\n",
      "Now, we build the train matrix, which is like reviews, but with the testing entries \n",
      "set to zero:\n",
      ">>> train = reviews.copy()\n",
      ">>> train[U[test_idxs], M[test_idxs]] = 0\n",
      "Finally, the test matrix contains just the testing values:\n",
      ">>> test = np.zeros_like(reviews)\n",
      ">>> test[U[test_idxs], M[test_idxs]] = reviews[U[test_idxs],  \n",
      "                                               M[test_idxs]]\n",
      "From now on, we will work on taking the training data, and try to predict all the \n",
      "missing entries in the dataset. That is, we will write code that assigns each (user, \n",
      "movie) pair a recommendation.\n",
      "Normalizing the training data\n",
      "As we discussed, it is best to normalize the data to remove obvious movie or  \n",
      "user-specific effects. We will just use one very simple type of normalization,  \n",
      "which we used before: conversion to z-scores.\n",
      "Unfortunately, we cannot simply use scikit-learn's normalization objects as we have \n",
      "to deal with the missing values in our data (that is, not all movies were rated by all \n",
      "users). Thus, we want to normalize by the mean and standard deviation of the values \n",
      "that are, in fact, present.\n",
      "We will write our own class, which ignores missing values. This class will follow the \n",
      "scikit-learn preprocessing API:\n",
      "class NormalizePositive(object):\n",
      "We want to choose the axis of normalization. By default, we normalize along the first \n",
      "axis, but sometimes it will be useful to normalize along the second one. This follows \n",
      "the convention of many other NumPy-related functions:\n",
      "    def __init__(self, axis=0):\n",
      "        self.axis = axis\n",
      "The most important method is the fit method. In our implementation, we compute \n",
      "the mean and standard deviation of the values that are not zero. Recall that zeros \n",
      "indicate \"missing values\":\n",
      "    def fit(self, features, y=None):\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 199, 'page_label': '179'}\n",
      "Chapter 8\n",
      "[  179 ]\n",
      "If the axis is 1, we operate on the transposed array as follows:\n",
      "      if self.axis == 1:\n",
      "          features = features.T\n",
      "      #  count features that are greater than zero in axis 0:\n",
      "      binary = (features > 0)\n",
      "      count0 = binary.sum(axis=0)\n",
      "      # to avoid division by zero, set zero counts to one:\n",
      "      count0[count0 == 0] = 1.\n",
      "      # computing the mean is easy:\n",
      "      self.mean = features.sum(axis=0)/count0\n",
      "      # only consider differences where binary is True:\n",
      "      diff = (features - self.mean) * binary\n",
      "      diff **= 2\n",
      "      # regularize the estimate of std by adding 0.1\n",
      "      self.std = np.sqrt(0.1 + diff.sum(axis=0)/count0)\n",
      "      return self\n",
      "We add 0.1 to the direct estimate of the standard deviation to avoid underestimating \n",
      "the value of the standard deviation when there are only a few samples, all of which \n",
      "may be exactly the same. The exact value used does not matter much for the final \n",
      "result, but we need to avoid division by zero.\n",
      "The transform method needs to take care of maintaining the binary structure  \n",
      "as follows:\n",
      "    def transform(self, features):\n",
      "        if self.axis == 1:\n",
      "            features = features.T\n",
      "        binary = (features > 0)\n",
      "        features = features - self.mean\n",
      "        features /= self.std\n",
      "        features *= binary\n",
      "        if self.axis == 1:\n",
      "            features = features.T\n",
      "        return features\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 200, 'page_label': '180'}\n",
      "Recommendations\n",
      "[  180 ]\n",
      "Notice how we took care of transposing the input matrix when the axis is 1 and  \n",
      "then transformed it back so that the return value has the same shape as the input. \n",
      "The inverse_transform method performs the inverse operation to transform as \n",
      "shown here:\n",
      "    def inverse_transform(self, features, copy=True):\n",
      "        if copy:\n",
      "            features = features.copy()\n",
      "        if self.axis == 1:\n",
      "            features = features.T\n",
      "        features *= self.std\n",
      "        features += self.mean\n",
      "        if self.axis == 1:\n",
      "            features = features.T\n",
      "        return features\n",
      "Finally, we add the fit_transform method which, as the name indicates, combines \n",
      "both the fit and transform operations:\n",
      "    def fit_transform(self, features):\n",
      "        return self.fit(features).transform(features)\n",
      "The methods that we defined (fit, transform, transform_inverse, and fit_\n",
      "transform) were the same as the objects defined in the sklearn.preprocessing \n",
      "module. In the following sections, we will first normalize the inputs, generate \n",
      "normalized predictions, and finally apply the inverse transformation to obtain  \n",
      "the final predictions.\n",
      "A neighborhood approach to recommendations\n",
      "The neighborhood concept can be implemented in two ways: user neighbors or \n",
      "movie neighbors. User neighborhoods are based on a very simple concept: to know \n",
      "how a user will rate a movie, find the users most similar to them, and look at their \n",
      "ratings. We will only consider user neighbors for the moment. At the end of this \n",
      "section, we will discuss how the code can be adapted to compute movie neighbors.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 201, 'page_label': '181'}\n",
      "Chapter 8\n",
      "[  181 ]\n",
      "One of the interesting techniques that we will now explore is to just see which \n",
      "movies each user has rated, even without taking a look at what rating was given. \n",
      "Even with a binary matrix where we have an entry equal to 1 when a user rates a \n",
      "movie, and 0 when they did not, we can make useful predictions. In hindsight, this \n",
      "makes perfect sense; we do not completely randomly choose movies to watch, but \n",
      "instead pick those where we already have an expectation of liking them. We also \n",
      "do not make random choices of which movies to rate, but perhaps only rate those \n",
      "we feel most strongly about (naturally, there are exceptions, but on average this is \n",
      "probably true).\n",
      "We can visualize the values of the matrix as an image, where each rating is depicted \n",
      "as a little square. Black represents the absence of a rating and the gray levels \n",
      "represent the rating value.\n",
      "The code to visualize the data is very simple (you can adapt it to show a larger fraction \n",
      "of the matrix than is possible to show in this book), as shown in the following:\n",
      ">>> from matplotlib import pyplot as plt\n",
      ">>> # Build an instance of the object we defined above\n",
      ">>> norm = NormalizePositive(axis=1)\n",
      ">>> binary = (train > 0)\n",
      ">>> train = norm.fit_transform(train)\n",
      ">>> # plot just 200x200 area for space reasons\n",
      ">>> plt.imshow(binary[:200, :200], interpolation='nearest')\n",
      "The following screenshot is the output of this code:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 202, 'page_label': '182'}\n",
      "Recommendations\n",
      "[  182 ]\n",
      "We can see that the matrix is sparsemost of the squares are black. We can also see \n",
      "that some users rate a lot more movies than others and that some movies are the \n",
      "target of many more ratings than others.\n",
      "We are now going to use this binary matrix to make predictions of movie ratings. \n",
      "The general algorithm will be (in pseudo code) as follows:\n",
      "1. For each user, rank every other user in terms of closeness. For this step,  \n",
      "we will use the binary matrix and use correlation as the measure of  \n",
      "closeness (interpreting the binary matrix as zeros and ones allows  \n",
      "us to perform this computation).\n",
      "2. When we need to estimate a rating for a (user, movie) pair, we look at all the \n",
      "users who have rated that movie and split them into two: the most similar \n",
      "half and the most dissimilar half. We then use the average of the most similar \n",
      "half as the prediction.\n",
      "We can use the scipy.spatial.distance.pdist function to obtain the distance \n",
      "between all the users as a matrix. This function returns the correlation distance, \n",
      "which transforms the correlation value by inverting it so that larger numbers  \n",
      "mean less similar. Mathematically, the correlation distance is \n",
      " , where \n",
      "  is  \n",
      "the correlation value. The code is as follows:\n",
      ">>> from scipy.spatial import distance\n",
      ">>> # compute all pair-wise distances:\n",
      ">>> dists = distance.pdist(binary, 'correlation')\n",
      ">>> # Convert to square form, so that dists[i,j]\n",
      ">>> # is distance between binary[i] and binary[j]:\n",
      ">>> dists = distance.squareform(dists)\n",
      "We can use this matrix to select the nearest neighbors of each user. These are the \n",
      "users that most resemble it.\n",
      ">>> neighbors = dists.argsort(axis=1)\n",
      "Now, we iterate over all users to estimate predictions for all inputs:\n",
      ">>> # We are going to fill this matrix with results\n",
      ">>> filled = train.copy()\n",
      ">>> for u in range(filled.shape[0]):\n",
      "...     # n_u is neighbors of user\n",
      "...     n_u = neighbors[u, 1:]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 203, 'page_label': '183'}\n",
      "Chapter 8\n",
      "[  183 ]\n",
      "...     # t_u is training data\n",
      "...     for m in range(filled.shape[1]):\n",
      "...         # get relevant reviews in order!\n",
      "...         revs = [train[neigh, m]\n",
      "...                    for neigh in n_u\n",
      "...                         if binary  [neigh, m]]\n",
      "...         if len(revs):\n",
      "...             # n is the number of reviews for this movie\n",
      "...             n = len(revs)\n",
      "...             # consider half of the reviews plus one\n",
      "...             n //= 2\n",
      "...             n += 1\n",
      "...             revs = revs[:n]\n",
      "...             filled[u,m] = np.mean(revs )\n",
      "The tricky part in the preceding snippet is indexing by the right values to select the \n",
      "neighbors who have rated the movie. Then, we choose the half that is closest to the \n",
      "user (in the rev[:n] line) and average those. Because some films have many reviews \n",
      "and others very few, it is hard to find a single number of users for all cases. Choosing \n",
      "half of the available data is a more generic approach.\n",
      "To obtain the final result, we need to un-normalize the predictions as follows:\n",
      ">>> predicted = norm.inverse_transform(filled)\n",
      "We can use the same metrics we learned about in the previous chapter:\n",
      ">>> from sklearn import metrics\n",
      ">>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n",
      ">>> print('R2 score (binary neighbors): {:.1%}'.format(r2))\n",
      "R2 score (binary neighbors): 29.5%\n",
      "The preceding code computes the result for user neighbors, but we can use it to \n",
      "compute the movie neighbors by simply transposing the input matrix. In fact, the \n",
      "code computes neighbors of whatever are the rows of its input matrix.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 204, 'page_label': '184'}\n",
      "Recommendations\n",
      "[  184 ]\n",
      "So we can rerun the following code, by just inserting the following line at the top:\n",
      ">>> reviews = reviews.T\n",
      ">>> # use same code as before \n",
      ">>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n",
      ">>> print('R2 score (binary movie neighbors): {:.1%}'.format(r2))\n",
      "R2 score (binary movie neighbors): 29.8%\n",
      "Thus we can see that the results are not that different.\n",
      "In this book's code repository, the neighborhood code has been wrapped into a \n",
      "simple function, which makes it easier to reuse.\n",
      "A regression approach to recommendations\n",
      "An alternative to neighborhoods is to formulate recommendations as a regression \n",
      "problem and apply the methods that we learned in the previous chapter.\n",
      "We also consider why this problem is not a good fit for a classification formulation. \n",
      "We could certainly attempt to learn a five-class model, using one class for each \n",
      "possible movie rating. There are two problems with this approach:\n",
      " The different possible errors are not at all the same. For example, mistaking a \n",
      "5-star movie for a 4-star one is not as serious a mistake as mistaking a 5-star \n",
      "movie for a 1-star one.\n",
      " Intermediate values make sense. Even if our inputs are only integer values, it \n",
      "is perfectly meaningful to say that the prediction is 4.3. We can see that this is \n",
      "a different prediction than 3.5, even if they both round to 4.\n",
      "These two factors together mean that classification is not a good fit to the problem. \n",
      "The regression framework is a better fit.\n",
      "For a basic approach, we again have two choices: we can build movie-specific or \n",
      "user-specific models. In our case, we are going to first build user-specific models. \n",
      "This means that, for each user, we take the movies that the user has rated as our \n",
      "target variable. The inputs are the ratings of other users. We hypothesize that this \n",
      "will give a high value to users who are similar to our user (or a negative value to \n",
      "users who like the same movies that our user dislikes).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 205, 'page_label': '185'}\n",
      "Chapter 8\n",
      "[  185 ]\n",
      "Setting up the train and test matrices is as before (including running the \n",
      "normalization steps). Therefore, we jump directly to the learning step. First,  \n",
      "we instantiate a regressor as follows:\n",
      ">>> reg = ElasticNetCV(alphas=[\n",
      "                   0.0125, 0.025, 0.05, .125, .25, .5, 1., 2., 4.])\n",
      "We build a data matrix, which will contain a rating for every (user, movie) pair. We \n",
      "initialize it as a copy of the training data:\n",
      ">>> filled = train.copy()\n",
      "Now, we iterate over all the users, and each time learn a regression model based only \n",
      "on the data that that user has given us:\n",
      ">>> for u in range(train.shape[0]):\n",
      "...     curtrain = np.delete(train, u, axis=0)\n",
      "...     # binary records whether this rating is present\n",
      "...     bu = binary[u]\n",
      "...     # fit the current user based on everybody else\n",
      "...     reg.fit(curtrain[:,bu].T, train[u, bu])\n",
      "...     # Fill in all the missing ratings\n",
      "...     filled[u, ~bu] = reg.predict(curtrain[:,~bu].T)\n",
      "Evaluating the method can be done exactly as before:\n",
      ">>> predicted = norm.inverse_transform(filled)\n",
      ">>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n",
      ">>> print('R2 score (user regression): {:.1%}'.format(r2))\n",
      "R2 score (user regression): 32.3%\n",
      "As before, we can adapt this code to perform movie regression by using the \n",
      "transposed matrix.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 206, 'page_label': '186'}\n",
      "Recommendations\n",
      "[  186 ]\n",
      "Combining multiple methods\n",
      "We now combine the aforementioned methods in a single prediction. This seems \n",
      "intuitively a good idea, but how can we do this in practice? Perhaps, the first thought \n",
      "that comes to mind is that we can average the predictions. This might give decent \n",
      "results, but there is no reason to think that all estimated predictions should be treated \n",
      "the same. It might be that one is better than others.\n",
      "We can try a weighted average, multiplying each prediction by a given weight before \n",
      "summing it all up. How do we find the best weights, though? We learn them from \n",
      "the data, of course!\n",
      "Ensemble learning\n",
      "We are using a general technique in machine learning, which is \n",
      "not only applicable in regression: ensemble learning. We learn an \n",
      "ensemble (that is, a set) of predictors. Then, we to combine them to \n",
      "obtain a single output. What is interesting is that we can see each \n",
      "prediction as being a new feature, and we are now just combining \n",
      "features based on training data, which is what we have been doing \n",
      "all along. Note that we are doing so for regression here, but the same \n",
      "reasoning is applicable to classification: you learn several classifiers, \n",
      "then a master classifier, which takes the output of all of them and \n",
      "gives a final prediction. Different forms of ensemble learning differ on \n",
      "how you combine the base predictors.\n",
      "In order to combine the methods, we will use a technique called stacked learning. \n",
      "The idea is you learn a set of predictors, then you use the output of these predictors \n",
      "as features for another predictor. You can even have several layers, where each layer \n",
      "learns by using the output of the previous layer as features for its prediction. Have a \n",
      "look at the following diagram:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 207, 'page_label': '187'}\n",
      "Chapter 8\n",
      "[  187 ]\n",
      "In order to fit this combination model, we will split the training data into two. \n",
      "Alternatively, we could have used cross-validation (the original stacked learning \n",
      "model worked like this). However, in this case, we have enough data to obtain good \n",
      "estimates by leaving some aside.\n",
      "As when fitting hyperparameters, though, we need two layers of training/testing \n",
      "splits: a first, higher-level split, and then, inside the training split, a second split to  \n",
      "be able to fit the stacked learner, as show in the following:\n",
      ">>> train,test = load_ml100k.get_train_test(random_state=12)\n",
      ">>> # Now split the training again into two subgroups\n",
      ">>> tr_train,tr_test = load_ml100k.get_train_test(train,  \n",
      "    random_state=34)\n",
      ">>> # Call all the methods we previously defined:\n",
      ">>> # these have been implemented as functions:\n",
      ">>> tr_predicted0 = regression.predict(tr_train)\n",
      ">>> tr_predicted1 = regression.predict(tr_train.T).T\n",
      ">>> tr_predicted2 = corrneighbours.predict(tr_train)\n",
      ">>> tr_predicted3 = corrneighbours.predict(tr_train.T).T\n",
      ">>> tr_predicted4 = norm.predict(tr_train)\n",
      ">>> tr_predicted5 = norm.predict(tr_train.T).T\n",
      ">>> # Now assemble these predictions into a single array:\n",
      ">>> stack_tr = np.array([\n",
      "...     tr_predicted0[tr_test > 0],\n",
      "...     tr_predicted1[tr_test > 0],\n",
      "...     tr_predicted2[tr_test > 0],\n",
      "...     tr_predicted3[tr_test > 0],\n",
      "...     tr_predicted4[tr_test > 0],\n",
      "...     tr_predicted5[tr_test > 0],\n",
      "...     ]).T\n",
      ">>> # Fit a simple linear regression\n",
      ">>> lr = linear_model.LinearRegression()\n",
      ">>> lr.fit(stack_tr, tr_test[tr_test > 0])\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 208, 'page_label': '188'}\n",
      "Recommendations\n",
      "[  188 ]\n",
      "Now, we apply the whole process to the testing split and evaluate:\n",
      ">>> stack_te = np.array([\n",
      "...     tr_predicted0.ravel(),\n",
      "...     tr_predicted1.ravel(),\n",
      "...     tr_predicted2.ravel(),\n",
      "...     tr_predicted3.ravel(),\n",
      "...     tr_predicted4.ravel(),\n",
      "...     tr_predicted5.ravel(),\n",
      "...     ]).T\n",
      ">>> predicted = lr.predict(stack_te).reshape(train.shape)\n",
      "Evaluation is as before:\n",
      ">>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n",
      ">>> print('R2 stacked: {:.2%}'.format(r2))\n",
      "R2 stacked: 33.15%\n",
      "The result of stacked learning is better than what any single method had achieved. It \n",
      "is quite typical that combining methods is a simple way to obtain a small performance \n",
      "boost, but that the results are not earth shattering.\n",
      "By having a flexible way to combine multiple methods, we can simply try any idea \n",
      "we wish by adding it into the mix of learners and letting the system fold it into the \n",
      "prediction. We can, for example, replace the neighborhood criterion in the nearest \n",
      "neighbor code.\n",
      "However, we do have to be careful to not overfit our dataset. In fact, if we  \n",
      "randomly try too many things, some of them will work well on this dataset, but \n",
      "will not generalize. Even though we are splitting our data, we are not rigorously \n",
      "cross-validating our design decisions. In order to have a good estimate, and if data \n",
      "is plentiful, you should leave a portion of the data untouched until you have a final \n",
      "model that is about to go into production. Then, testing your model on this held out \n",
      "data gives you an unbiased prediction of how well you should expect it to work in \n",
      "the real world.\n",
      "Basket analysis\n",
      "The methods we have discussed so far work well when you have numeric ratings of \n",
      "how much a user liked a product. This type of information is not always available, as \n",
      "it requires active behavior on the part of consumers.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 209, 'page_label': '189'}\n",
      "Chapter 8\n",
      "[  189 ]\n",
      "Basket analysis is an alternative mode of learning recommendations. In this mode, \n",
      "our data consists only of what items were bought together; it does not contain \n",
      "any information on whether individual items were enjoyed or not. Even if users \n",
      "sometimes buy items they regret, on average, knowing their purchases gives you \n",
      "enough information to build good recommendations. It is often easier to get this \n",
      "data rather than rating data, as many users will not provide ratings, while the basket \n",
      "data is generated as a side effect of shopping. The following screenshot shows you a \n",
      "snippet of Amazon.com's web page for Tolstoy's classic book War and Peace, which \n",
      "demonstrates a common way to use these results:\n",
      "This mode of learning is not only applicable to actual shopping baskets, naturally. \n",
      "It is applicable in any setting where you have groups of objects together and need \n",
      "to recommend another. For example, recommending additional recipients to a \n",
      "user writing an e-mail is done by Gmail and could be implemented using similar \n",
      "techniques (we do not know what Gmail uses internally; perhaps, they combine \n",
      "multiple techniques, as we did earlier). Or, we could use these methods to develop \n",
      "an app to recommend web pages to visit based on your browsing history. Even if \n",
      "we are handling purchases, it may make sense to group all purchases by a customer \n",
      "into a single basket, independently of whether the items were bought together or on \n",
      "separate transactions. This depends on the business context, but keep in mind that \n",
      "the techniques are flexible and can be useful in many settings.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 210, 'page_label': '190'}\n",
      "Recommendations\n",
      "[  190 ]\n",
      "Beer and diapers. One of the stories that is often mentioned in the \n",
      "context of basket analysis is the diapers and beer story. It says that when \n",
      "supermarkets first started to look at their data, they found that diapers \n",
      "were often bought together with beer. Supposedly, it was the father who \n",
      "would go out to the supermarket to buy diapers and then would pick \n",
      "up some beer as well. There has been much discussion of whether this \n",
      "is true or just an urban myth. In this case, it seems that it is true. In the \n",
      "early 1990s, Osco Drug did discover that in the early evening, beer and \n",
      "diapers were bought together, and it did surprise the managers who had, \n",
      "until then, never considered these two products to be similar. What is not \n",
      "true is that this led the store to move the beer display closer to the diaper \n",
      "section. Also, we have no idea whether it was really that fathers were \n",
      "buying beer and diapers together more than mothers (or grandparents).\n",
      "Obtaining useful predictions\n",
      "It is not just \"customers who bought X also bought Y\" even though that is how \n",
      "many online retailers phrase it (see the Amazon.com screenshot given earlier); a real \n",
      "system cannot work like this. Why not? Because such a system would get fooled by \n",
      "very frequently bought items and would simply recommend that which is popular \n",
      "without any personalization.\n",
      "For example, at a supermarket, many customers buy bread every time they shop \n",
      "or close to it (for the sake of argument, let us say that 50 percent of visits include \n",
      "bread). So, if you focus on any particular item, say dishwasher soap and look at what \n",
      "is frequently bought with dishwasher soap, you might find that bread is frequently \n",
      "bought with soap. In fact, just by random chance, 50 percent of the times someone \n",
      "buys dishwasher soap, they buy bread. However, bread is frequently bought with \n",
      "anything else just because everybody buys bread very often.\n",
      "What we are really looking for is \"customers who bought X, are statistically more likely \n",
      "to buy Y than the average customer who has not bought X\". If you buy dishwasher \n",
      "soap, you are likely to buy bread, but not more so than the baseline. Similarly, a \n",
      "bookstore that simply recommended bestsellers no matter which books you had \n",
      "already bought would not be doing a good job of personalizing recommendations.\n",
      "Analyzing supermarket shopping baskets\n",
      "As an example, we will look at a dataset consisting of anonymous transactions at a \n",
      "supermarket in Belgium. This dataset was made available by Tom Brijs at Hasselt \n",
      "University. Due to privacy concerns, the data has been anonymized, so we only have \n",
      "a number for each product and a basket is a set of numbers. The data file is available \n",
      "from several online sources (including this book's companion website).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 211, 'page_label': '191'}\n",
      "Chapter 8\n",
      "[  191 ]\n",
      "We begin by loading the dataset and looking at some statistics (this is always a  \n",
      "good idea):\n",
      ">>> from collections import defaultdict\n",
      ">>> from itertools import chain\n",
      ">>> # File is downloaded as a compressed file\n",
      ">>> import gzip\n",
      ">>> # file format is a line per transaction\n",
      ">>> # of the form '12 34 342 5...'\n",
      ">>> dataset = [[int(tok) for tok in line.strip().split()]\n",
      "...         for line in gzip.open('retail.dat.gz')]\n",
      ">>> # It is more convenient to work with sets\n",
      ">>> dataset = [set(d) for d in dataset]\n",
      ">>> # count how often each product was purchased:\n",
      ">>> counts = defaultdict(int)\n",
      ">>> for elem in chain(*dataset):\n",
      "...     counts[elem] += 1\n",
      "We can see the resulting counts summarized in the following table:\n",
      "# of times bought # of products\n",
      "Just once 2,224\n",
      "2 or 3 2,438\n",
      "4 to 7 2,508\n",
      "8 to 15 2,251\n",
      "16 to 31 2,182\n",
      "32 to 63 1,940\n",
      "64 to 127 1,523\n",
      "128 to 511 1,225\n",
      "512 or more 179\n",
      "There are many products that have only been bought a few times. For example,  \n",
      "33 percent of products were bought four or fewer times. However, this represents only \n",
      "1 percent of purchases. This phenomenon that many products are only purchased a \n",
      "small number of times is sometimes labeled the long tail and has only become more \n",
      "prominent as the Internet made it cheaper to stock and sell niche items. In order to be \n",
      "able to provide recommendations for these products, we would need a lot more data.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 212, 'page_label': '192'}\n",
      "Recommendations\n",
      "[  192 ]\n",
      "There are a few open source implementations of basket analysis algorithms out \n",
      "there, but none that are well integrated with scikit-learn or any of the other packages \n",
      "we have been using. Therefore, we are going to implement one classic algorithm \n",
      "ourselves. This algorithm is called the Apriori algorithm, and it is a bit old (it was \n",
      "published in 1994 by Rakesh Agrawal and Ramakrishnan Srikant), but it still works \n",
      "(algorithms, of course, never stop working, they just get superceded by better ideas).\n",
      "Formally, the Apriori algorithm takes a collection of sets (that is, your shopping \n",
      "baskets) and returns sets that are very frequent as subsets (that is, items that together \n",
      "are part of many shopping baskets).\n",
      "The algorithm works using a bottom-up approach: starting with the smallest \n",
      "candidates (those composed of one single element), it builds up, adding one element \n",
      "at a time. Formally, the algorithm takes a set of baskets and the minimum input \n",
      "that should be considered (a parameter we will call minsupport). The first step is \n",
      "to consider all baskets with just one element with minimal support. Then, these are \n",
      "combined in all possible ways to build up two-element baskets. These are filtered in \n",
      "order to keep only those that have minimal support. Then, all possible three-element \n",
      "baskets are considered and those with minimal support are kept, and so on. The trick \n",
      "of Apriori is that when building a larger basket, it only needs to consider those that are \n",
      "built up of smaller sets.\n",
      "The following diagram presents a schematic view of the algorithm:\n",
      "We shall now implement this algorithm in code. We need to define the minimum \n",
      "support we are looking for:\n",
      ">>> minsupport = 80\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 213, 'page_label': '193'}\n",
      "Chapter 8\n",
      "[  193 ]\n",
      "Support is the number of times a set of products was purchased together. The goal  \n",
      "of Apriori is to find itemsets with high support. Logically, any itemset with more \n",
      "than minimal support can only be composed of items which themselves have at  \n",
      "least minimal support:\n",
      ">>> valid = set(k for k,v in counts.items()\n",
      "...           if (v >= minsupport))\n",
      "Our initial itemsets are singletons (sets with a single element). In particular, all \n",
      "singletons that have at least minimal support are frequent itemsets:\n",
      ">>>  itemsets = [frozenset([v]) for v in valid]\n",
      "Now, our loop is given as follows:\n",
      ">>> freqsets = []\n",
      ">>> for i in range(16):\n",
      "...     nextsets = []\n",
      "...     tested = set()\n",
      "...     for it in itemsets:\n",
      "...         for v in valid:\n",
      "...             if v not in it:\n",
      "...                 # Create a new candidate set by adding v to it\n",
      "...                 c = (it | frozenset([v]))\n",
      "...                 # check If we have tested it already\n",
      "...                 if c in tested:\n",
      "...                     continue\n",
      "...                 tested.add(c)\n",
      "...\n",
      "...                 # Count support by looping over dataset\n",
      "...                 # This step is slow.\n",
      "...                 # Check `apriori.py` for a better implementation.\n",
      "...                 support_c = sum(1 for d in dataset if  \n",
      "                    d.issuperset(c))\n",
      "...                 if support_c > minsupport:\n",
      "...                     nextsets.append(c)\n",
      "...     freqsets.extend(nextsets)\n",
      "...     itemsets = nextsets\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 214, 'page_label': '194'}\n",
      "Recommendations\n",
      "[  194 ]\n",
      "...     if not len(itemsets):\n",
      "...         break\n",
      ">>> print(\"Finished!\")\n",
      "Finished!\n",
      "This works correctly, but it is slow. A better implementation has more infrastructure \n",
      "to avoid having to loop over all the datasets to get the count (support_c). In \n",
      "particular, we keep track of which shopping baskets have which frequent itemsets. \n",
      "This accelerates the loop but makes the code harder to follow. Therefore we will \n",
      "not show it here. As usual, you can find both the implementations on this book's \n",
      "companion website. The code there is also wrapped into a function that can be \n",
      "applied to other datasets.\n",
      "The Apriori algorithm returns frequent itemsets, that is, baskets that are present \n",
      "above a certain threshold (given by the minsupport variable in the code).\n",
      "Association rule mining\n",
      "Frequent itemsets are not very useful by themselves. The next step is to build \n",
      "association rules. Because of this final goal, the whole field of basket analysis is \n",
      "sometimes called association rule mining.\n",
      "An association rule is a statement of the type \"If X, then Y\", for example, \"if a customer \n",
      "bought War and Peace, then they will buy Anna Karenina\". Note that the rule is not \n",
      "deterministic (not all customers who buy X will buy Y), but it is rather cumbersome to \n",
      "always spell it out: \"if a customer bought X, he is more likely than baseline to buy Y\"; \n",
      "thus, we say \"if X, then Y\", but we mean it in a probabilistic sense.\n",
      "Interestingly, both the antecedent and the conclusion may contain multiple objects: \n",
      "costumers who bought X, Y, and Z also bought A, B, and C. Multiple antecedents may \n",
      "allow you to make more specific predictions than are possible from a single item.\n",
      "You can get from a frequent set to a rule by just trying all the possible combinations \n",
      "of X implies Y. It is easy to generate many of these rules. However, you only want to \n",
      "have valuable rules. Therefore, we need to measure the value of a rule. A commonly \n",
      "used measure is called the lift. The lift is the ratio between the probability obtained \n",
      "by applying the rule and the baseline, as in the following formula:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 215, 'page_label': '195'}\n",
      "Chapter 8\n",
      "[  195 ]\n",
      "In the preceding formula, P(Y) is the fraction of all the transactions that include \n",
      "Y, while P(Y|X) is the fraction of transactions that include Y, given that they also \n",
      "include X. Using the lift helps avoid the problem of recommending bestsellers; for a \n",
      "bestseller, both P(Y) and P(Y|X) will be large. Therefore, the lift will be close to one \n",
      "and the rule will be deemed irrelevant. In practice, we wish to have values of lift of at \n",
      "least 10, perhaps even 100.\n",
      "Refer to the following code:\n",
      ">>> minlift = 5.0\n",
      ">>> nr_transactions = float(len(dataset))\n",
      ">>> for itemset in freqsets:\n",
      "...       for item in itemset:\n",
      "...         consequent = frozenset([item])\n",
      "...         antecedent = itemset-consequent\n",
      "...         base = 0.0\n",
      "...         # acount: antecedent count\n",
      "...         acount = 0.0\n",
      "...     \n",
      "...         # ccount : consequent count\n",
      "...         ccount = 0.0\n",
      "...         for d in dataset:\n",
      "...           if item in d: base += 1\n",
      "...           if d.issuperset(itemset): ccount += 1\n",
      "...           if d.issuperset(antecedent): acount += 1\n",
      "...         base /= nr_transactions\n",
      "...         p_y_given_x = ccount/acount\n",
      "...         lift = p_y_given_x / base\n",
      "...         if lift > minlift:\n",
      "...             print('Rule {0} ->  {1} has lift {2}'\n",
      "...                   .format(antecedent, consequent,lift))\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 216, 'page_label': '196'}\n",
      "Recommendations\n",
      "[  196 ]\n",
      "Some of the results are shown in the following table. The counts are the number \n",
      "of transactions which include the consequent alone (that is, the base rate at which \n",
      "that product is bought), all the items in the antecedent, and all the items in the \n",
      "antecedent and the consequent.\n",
      "Antecedent Consequent Consequent \n",
      "count\n",
      "Antecedent \n",
      "count\n",
      "Antecedent & \n",
      "consequent \n",
      "count\n",
      "Lift\n",
      "1,378, 1,379, \n",
      "1,380\n",
      "1,269 279 (0.3 \n",
      "percent)\n",
      "80 57 225\n",
      "48, 41, 976 117 1026 (1.1 \n",
      "percent)\n",
      "122 51 35\n",
      "48, 41, \n",
      "1,6011\n",
      "16,010 1316 (1.5 \n",
      "percent )\n",
      "165 159 64\n",
      "We can see, for example, that there were 80 transactions in which 1,378, 1,379, \n",
      "and 1,380 were bought together. Of these, 57 also included 1,269, so the estimated \n",
      "conditional probability is 57/80  71 percent. Compared to the fact that only 0.3 \n",
      "percent of all transactions included 1,269, this gives us a lift of 255.\n",
      "The need to have a decent number of transactions in these counts in order to be  \n",
      "able to make relatively solid inferences is why we must first select frequent itemsets. \n",
      "If we were to generate rules from an infrequent itemset, the counts would be very \n",
      "small; due to this, the relative values would be meaningless (or subject to very large \n",
      "error bars).\n",
      "Note that there are many more association rules discovered from this dataset: the \n",
      "algorithm discovers 1,030 rules (requiring support for the baskets of at least 80 and \n",
      "a minimum lift of 5). This is still a small dataset when compared to what is now \n",
      "possible with the web. With datasets containing millions of transactions, you can \n",
      "expect to generate many thousands of rules, even millions.\n",
      "However, for each customer or each product, only a few rules will be relevant at any \n",
      "given time. So each costumer only receives a small number of recommendations.\n",
      "More advanced basket analysis\n",
      "There are now other algorithms for basket analysis that run faster than Apriori. The \n",
      "code we saw earlier was simple and it was good enough for us, as we only had circa \n",
      "100 thousand transactions. If we had many millions, it might be worthwhile to use \n",
      "a faster algorithm. Note, though, that learning association rules can often be done \n",
      "offline, where efficiency is not as great a concern.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 217, 'page_label': '197'}\n",
      "Chapter 8\n",
      "[  197 ]\n",
      "There are also methods to work with temporal information, leading to rules that \n",
      "take into account the order in which you have made your purchases. Consider, as \n",
      "an example, that someone buying supplies for a large party may come back for trash \n",
      "bags. Thus, it may make sense to propose trash bags on the first visit. However, it \n",
      "would not make sense to propose party supplies to everyone who buys a trash bag.\n",
      "Summary\n",
      "In this chapter, we started by using regression for rating predictions. We saw a couple \n",
      "of different ways in which to do so, and then combined them all in a single prediction \n",
      "by learning a set of weights. This technique, ensemble learning, in particular stacked \n",
      "learning, is a general technique that can be used in many situations, not just for \n",
      "regression. It allows you to combine different ideas even if their internal mechanics \n",
      "are completely different; you can combine their final outputs.\n",
      "In the second half of the chapter, we switched gears and looked at another mode of \n",
      "producing recommendations: shopping basket analysis or association rule mining. In \n",
      "this mode, we try to discover (probabilistic) association rules of the \"customers who \n",
      "bought X are likely to be interested in Y\" form. This takes advantage of the data that \n",
      "is generated from sales alone without requiring users to numerically rate items. This \n",
      "is not available in scikit-learn at this moment, so we wrote our own code.\n",
      "Association rule mining needs to be careful to not simply recommend bestsellers \n",
      "to every user (otherwise, what is the point of personalization?). In order to do this, \n",
      "we learned about measuring the value of rules in relation to the baseline, using a \n",
      "measure called the lift of a rule.\n",
      "At this point in the book, we have seen the major modes of machine learning: \n",
      "classification. In the next two chapters, we will look at techniques used for two \n",
      "specific kinds of data, music and images. Our first goal will be to build a music  \n",
      "genre classifier.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 218, 'page_label': '198'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 219, 'page_label': '199'}\n",
      "[ 199 ]\n",
      "Classification  Music Genre \n",
      "Classification\n",
      "So far, we have had the luxury that every training data instance could easily be \n",
      "described by a vector of feature values. In the Iris dataset, for example, the flowers \n",
      "are represented by vectors containing values for length and width of certain aspects \n",
      "of a flower. In the text-based examples, we could transform the text into a bag of \n",
      "word representations and manually craft our own features that captured certain \n",
      "aspects of the texts.\n",
      "It will be different in this chapter, when we try to classify songs by their genre. Or, \n",
      "how would we, for instance, represent a three-minute-long song? Should we take \n",
      "the individual bits of its MP3 representation? Probably not, since treating it like a \n",
      "text and creating something like a \"bag of sound bites\" would certainly be way too \n",
      "complex. Somehow, we will, nevertheless, have to convert a song into a series of \n",
      "values that describe it sufficiently.\n",
      "Sketching our roadmap\n",
      "This chapter will show how we can come up with a decent classifier in a domain \n",
      "that is outside our comfort zone. For one, we will have to use sound-based features, \n",
      "which are much more complex than the text-based ones we have used before. \n",
      "And then we will learn how to deal with multiple classes, whereas we have only \n",
      "encountered binary classification problems up to now. In addition, we will get to \n",
      "know new ways of measuring classification performance.\n",
      "Let us assume a scenario in which, for some reason, we find a bunch of randomly \n",
      "named MP3 files on our hard disk, which are assumed to contain music. Our task is \n",
      "to sort them according to the music genre into different folders such as jazz, classical, \n",
      "country, pop, rock, and metal.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 220, 'page_label': '200'}\n",
      "Classification  Music Genre Classification\n",
      "[ 200 ]\n",
      "Fetching the music data\n",
      "We will use the GTZAN dataset, which is frequently used to benchmark music genre \n",
      "classification tasks. It is organized into 10 distinct genres, of which we will use only 6 \n",
      "for the sake of simplicity: Classical, Jazz, Country, Pop, Rock, and Metal. The dataset \n",
      "contains the first 30 seconds of 100 songs per genre. We can download the dataset \n",
      "from http://opihi.cs.uvic.ca/sound/genres.tar.gz.\n",
      "The tracks are recorded at 22,050 Hz (22,050 readings \n",
      "per second) mono in the WAV format.\n",
      "Converting into a WAV format\n",
      "Sure enough, if we would want to test our classifier later on our private MP3 \n",
      "collection, we would not be able to extract much meaning. This is because MP3 is \n",
      "a lossy music compression format that cuts out parts that the human ear cannot \n",
      "perceive. This is nice for storing because with MP3 you can fit 10 times as many \n",
      "songs on your device. For our endeavor, however, it is not so nice. For classification, \n",
      "we will have an easier game with WAV files, because they can be directly read by the \n",
      "scipy.io.wavfile package. We would, therefore, have to convert our MP3 files in \n",
      "case we want to use them with our classifier.\n",
      "In case you don't have a conversion tool nearby, you might want \n",
      "to check out SoX at http://sox.sourceforge.net. It claims \n",
      "to be the Swiss Army Knife of sound processing, and we agree \n",
      "with this bold claim.\n",
      "One advantage of having all our music files in the WAV format is that it is directly \n",
      "readable by the SciPy toolkit:\n",
      ">>> sample_rate, X = scipy.io.wavfile.read(wave_filename)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 221, 'page_label': '201'}\n",
      "Chapter 9\n",
      "[  201 ]\n",
      "X now contains the samples and sample_rate is the rate at which they were taken. \n",
      "Let us use that information to peek into some music files to get a first impression of \n",
      "what the data looks like.\n",
      "Looking at music\n",
      "A very convenient way to get a quick impression of what the songs of the diverse \n",
      "genres \"look\" like is to draw a spectrogram for a set of songs of a genre. A \n",
      "spectrogram is a visual representation of the frequencies that occur in a song. It \n",
      "shows the intensity for the frequencies at the y axis in the specified time intervals at \n",
      "the x axis. That is, the darker the color, the stronger the frequency is in the particular \n",
      "time window of the song.\n",
      "Matplotlib provides the convenient function specgram() that performs most of the \n",
      "under-the-hood calculation and plotting for us:\n",
      ">>> import scipy\n",
      ">>> from matplotlib.pyplot import specgram\n",
      ">>> sample_rate, X = scipy.io.wavfile.read(wave_filename)\n",
      ">>> print sample_rate, X.shape\n",
      "22050, (661794,)\n",
      ">>> specgram(X, Fs=sample_rate, xextent=(0,30))\n",
      "The WAV file we just read in was sampled at a rate of 22,050 Hz and  \n",
      "contains 661,794 samples.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 222, 'page_label': '202'}\n",
      "Classification  Music Genre Classification\n",
      "[ 202 ]\n",
      "If we now plot the spectrogram for these first 30 seconds for diverse WAV files, we \n",
      "can see that there are commonalities between songs of the same genre, as shown in \n",
      "the following image:\n",
      "Just glancing at the image, we immediately see the difference in the spectrum \n",
      "between, for example, metal and classical songs. While metal songs have high \n",
      "intensity over most of the frequency spectrum all the time (they're energetic!), \n",
      "classical songs show a more diverse pattern over time.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 223, 'page_label': '203'}\n",
      "Chapter 9\n",
      "[  203 ]\n",
      "It should be possible to train a classifier that discriminates at least between Metal and \n",
      "Classical songs with high enough accuracy. Other genre pairs like Country and Rock \n",
      "could pose a bigger challenge, though. This looks like a real challenge to us, since we \n",
      "need to discriminate not only between two classes, but between six. We need to be \n",
      "able to discriminate between all of them reasonably well.\n",
      "Decomposing music into sine wave components\n",
      "Our plan is to extract individual frequency intensities from the raw sample  \n",
      "readings (stored in X earlier) and feed them into a classifier. These frequency \n",
      "intensities can be extracted by applying the so-called fast Fourier transform (FFT). \n",
      "As the theory behind FFT is outside the scope of this chapter, let us just look at an \n",
      "example to get an intuition of what it accomplishes. Later on, we will treat it as a \n",
      "black box feature extractor.\n",
      "For example, let us generate two WAV files, sine_a.wav and sine_b.wav, \n",
      "that contain the sound of 400 Hz and 3,000 Hz sine waves respectively. The \n",
      "aforementioned \"Swiss Army Knife\", SoX, is one way to achieve this:\n",
      "$ sox --null -r 22050 sine_a.wav synth 0.2 sine 400\n",
      "$ sox --null -r 22050 sine_b.wav synth 0.2 sine 3000\n",
      "In the following charts, we have plotted their first 0.008 seconds. Below we can see \n",
      "the FFT of the sine waves. Not surprisingly, we see a spike at 400 Hz and 3,000 Hz \n",
      "below the corresponding sine waves.\n",
      "Now, let us mix them both, giving the 400 Hz sound half the volume of the  \n",
      "3,000 Hz one:\n",
      "$ sox --combine mix --volume 1 sine_b.wav --volume 0.5 sine_a.wav  \n",
      "sine_mix.wav\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 224, 'page_label': '204'}\n",
      "Classification  Music Genre Classification\n",
      "[ 204 ]\n",
      "We see two spikes in the FFT plot of the combined sound, of which the 3,000 Hz \n",
      "spike is almost double the size of the 400 Hz.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 225, 'page_label': '205'}\n",
      "Chapter 9\n",
      "[  205 ]\n",
      "For real music, we quickly see that the FFT doesn't look as beautiful as in the \n",
      "preceding toy example:\n",
      "Using FFT to build our first classifier\n",
      "Nevertheless, we can now create some kind of musical fingerprint of a song using \n",
      "FFT. If we do that for a couple of songs and manually assign their corresponding \n",
      "genres as labels, we have the training data that we can feed into our first classifier.\n",
      "Increasing experimentation agility\n",
      "Before we dive into the classifier training, let us first spend some thoughts on \n",
      "experimentation agility. Although we have the word \"fast\" in FFT, it is much slower \n",
      "than the creation of the features in our text-based chapters. And because we are still \n",
      "in an experimentation phase, we might want to think about how we could speed up \n",
      "the whole feature creation process.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 226, 'page_label': '206'}\n",
      "Classification  Music Genre Classification\n",
      "[ 206 ]\n",
      "Of course, the creation of the FFT per file will be the same each time we are running \n",
      "the classifier. We could, therefore, cache it and read the cached FFT representation \n",
      "instead of the complete WAV file. We do this with the create_fft() function, \n",
      "which, in turn, uses scipy.fft() to create the FFT. For the sake of simplicity (and \n",
      "speed!), let us fix the number of FFT components to the first 1,000 in this example. \n",
      "With our current knowledge, we do not know whether these are the most important \n",
      "ones with regard to music genre classificationonly that they show the highest \n",
      "intensities in the preceding FFT example. If we would later want to use more or \n",
      "fewer FFT components, we would of course have to recreate the FFT representations \n",
      "for each sound file.\n",
      "import os\n",
      "import scipy\n",
      "def create_fft(fn):\n",
      "    sample_rate, X = scipy.io.wavfile.read(fn)\n",
      "    fft_features = abs(scipy.fft(X)[:1000])\n",
      "    base_fn, ext = os.path.splitext(fn)\n",
      "    data_fn = base_fn + \".fft\"\n",
      "    scipy.save(data_fn, fft_features)\n",
      "We save the data using NumPy's save() function, which always appends .npy  \n",
      "to the filename. We only have to do this once for every WAV file needed for  \n",
      "training or predicting.\n",
      "The corresponding FFT reading function is read_fft():\n",
      "import glob\n",
      "def read_fft(genre_list, base_dir=GENRE_DIR):\n",
      "    X = []\n",
      "    y = []\n",
      "    for label, genre in enumerate(genre_list):\n",
      "        genre_dir = os.path.join(base_dir, genre, \"*.fft.npy\")\n",
      "        file_list = glob.glob(genre_dir)\n",
      "        for fn in file_list:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 227, 'page_label': '207'}\n",
      "Chapter 9\n",
      "[  207 ]\n",
      "            fft_features = scipy.load(fn)\n",
      "            X.append(fft_features[:1000])\n",
      "            y.append(label)\n",
      "    return np.array(X), np.array(y)\n",
      "In our scrambled music directory, we expect the following music genres:\n",
      "genre_list = [\"classical\", \"jazz\", \"country\", \"pop\", \"rock\", \"metal\"]\n",
      "Training the classifier\n",
      "Let us use the logistic regression classifier, which has already served us well in the \n",
      "Chapter 6, Classification II - Sentiment Analysis. The added difficulty is that we are \n",
      "now faced with a multiclass classification problem, whereas up to now we had to \n",
      "discriminate only between two classes.\n",
      "Just to mention one aspect that is surprising is the evaluation of accuracy rates \n",
      "when first switching from binary to multiclass classification. In binary classification \n",
      "problems, we have learned that an accuracy of 50 percent is the worst case, as it could \n",
      "have been achieved by mere random guessing. In multiclass settings, 50 percent can \n",
      "already be very good. With our six genres, for instance, random guessing would result \n",
      "in only 16.7 percent (equal class sizes assumed).\n",
      "Using a confusion matrix to measure accuracy in multiclass problems\n",
      "With multiclass problems, we should not only be interested in how well we manage \n",
      "to correctly classify the genres. In addition, we should also look into which genres \n",
      "we actually confuse with each other. This can be done with the so-called confusion \n",
      "matrix, as shown in the following:\n",
      ">>> from sklearn.metrics import confusion_matrix\n",
      ">>> cm = confusion_matrix(y_test, y_pred)\n",
      ">>> print(cm)\n",
      "[[26  1  2  0  0  2]\n",
      " [ 4  7  5  0  5  3]\n",
      " [ 1  2 14  2  8  3]\n",
      " [ 5  4  7  3  7  5]\n",
      " [ 0  0 10  2 10 12]\n",
      " [ 1  0  4  0 13 12]]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 228, 'page_label': '208'}\n",
      "Classification  Music Genre Classification\n",
      "[ 208 ]\n",
      "This prints the distribution of labels that the classifier predicted for the test set  \n",
      "for every genre. The diagonal represents the correct classifications. Since we have  \n",
      "six genres, we have a six-by-six matrix. The first row in the matrix says that for  \n",
      "31 Classical songs (sum of first row), it predicted 26 to belong to the genre Classical, \n",
      "1 to be a Jazz song, 2 to belong to the Country genre, and 2 to be Metal songs. \n",
      "The diagonal shows the correct classifications. In the first row, we see that out \n",
      "of (26+1+2+2)=31 songs, 26 have been correctly classified as classical and 5 were \n",
      "misclassifications. This is actually not that bad. The second row is more sobering: \n",
      "only 7 out of 24 Jazz songs have been correctly classifiedthat is, only 29 percent.\n",
      "Of course, we follow the train/test split setup from the previous chapters, so that we \n",
      "actually have to record the confusion matrices per cross-validation fold. We have to \n",
      "average and normalize later on, so that we have a range between 0 (total failure) and \n",
      "1 (everything classified correctly).\n",
      "A graphical visualization is often much easier to read than NumPy arrays. The \n",
      "matshow() function of matplotlib is our friend:\n",
      "from matplotlib import pylab\n",
      "def plot_confusion_matrix(cm, genre_list, name, title):\n",
      "    pylab.clf()\n",
      "    pylab.matshow(cm, fignum=False, cmap='Blues', \n",
      "                  vmin=0, vmax=1.0)\n",
      "    ax = pylab.axes()    ax.set_xticks(range(len(genre_list)))\n",
      "    ax.set_xticklabels(genre_list)\n",
      "    ax.xaxis.set_ticks_position(\"bottom\")\n",
      "    ax.set_yticks(range(len(genre_list)))\n",
      "    ax.set_yticklabels(genre_list)\n",
      "    pylab.title(title)\n",
      "    pylab.colorbar()\n",
      "    pylab.grid(False)\n",
      "    pylab.xlabel('Predicted class')\n",
      "    pylab.ylabel('True class')\n",
      "    pylab.grid(False)\n",
      "    pylab.show()\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 229, 'page_label': '209'}\n",
      "Chapter 9\n",
      "[  209 ]\n",
      "When you create a confusion matrix, be sure to choose a color map (the \n",
      "cmap parameter of matshow()) with an appropriate color ordering \n",
      "so that it is immediately visible what a lighter or darker color means. \n",
      "Especially discouraged for these kinds of graphs are rainbow color \n",
      "maps, such as matplotlib's default jet or even the Paired color map.\n",
      "The final graph looks like the following:\n",
      "For a perfect classifier, we would have expected a diagonal of dark squares from the \n",
      "left-upper corner to the right lower one, and light colors for the remaining area. In \n",
      "the preceding graph, we immediately see that our FFT-based classifier is far away \n",
      "from being perfect. It only predicts Classical songs correctly (dark square). For Rock, \n",
      "for instance, it preferred the label Metal most of the time.\n",
      "Obviously, using FFT points in the right direction (the Classical genre was not \n",
      "that bad), but is not enough to get a decent classifier. Surely, we can play with the \n",
      "number of FFT components (fixed to 1,000). But before we dive into parameter \n",
      "tuning, we should do our research. There we find that FFT is indeed not a bad \n",
      "feature for genre classificationit is just not refined enough. Shortly, we will see \n",
      "how we can boost our classification performance by using a processed version of it.\n",
      "Before we do that, however, we will learn another method of measuring \n",
      "classification performance.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 230, 'page_label': '210'}\n",
      "Classification  Music Genre Classification\n",
      "[ 210 ]\n",
      "An alternative way to measure classifier \n",
      "performance using receiver-operator \n",
      "characteristics\n",
      "We already learned that measuring accuracy is not enough to truly evaluate \n",
      "a classifier. Instead, we relied on precision-recall (P/R) curves to get a deeper \n",
      "understanding of how our classifiers perform.\n",
      "There is a sister of P/R curves, called receiver-operator-characteristics (ROC), which \n",
      "measures similar aspects of the classifier's performance, but provides another view \n",
      "of the classification performance. The key difference is that P/R curves are more \n",
      "suitable for tasks where the positive class is much more interesting than the negative \n",
      "one, or where the number of positive examples is much less than the number of \n",
      "negative ones. Information retrieval and fraud detection are typical application areas. \n",
      "On the other hand, ROC curves provide a better picture on how well the classifier \n",
      "behaves in general.\n",
      "To better understand the differences, let us consider the performance of the \n",
      "previously trained classifier in classifying country songs correctly, as shown  \n",
      "in the following graph:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 231, 'page_label': '211'}\n",
      "Chapter 9\n",
      "[  211 ]\n",
      "On the left, we see the P/R curve. For an ideal classifier, we would have the curve \n",
      "going from the top left directly to the top right and then to the bottom right, resulting \n",
      "in an area under curve (AUC) of 1.0.\n",
      "The right graph depicts the corresponding ROC curve. It plots the True Positive Rate \n",
      "over the False Positive Rate. There, an ideal classifier would have a curve going from \n",
      "the lower left to the top left, and then to the top right. A random classifier would be \n",
      "a straight line from the lower left to the upper right, as shown by the dashed line, \n",
      "having an AUC of 0.5. Therefore, we cannot compare an AUC of a P/R curve with \n",
      "that of an ROC curve.\n",
      "Independent of the curve, when comparing two different classifiers on the same \n",
      "dataset, we are always safe to assume that a higher AUC of a P/R curve for one \n",
      "classifier also means a higher AUC of the corresponding ROC curve and vice versa. \n",
      "Thus, we never bother to generate both. More on this can be found in the very \n",
      "insightful paper The Relationship Between Precision-Recall and ROC Curves by Davis \n",
      "and Goadrich (ICML, 2006).\n",
      "The following table summarizes the differences between P/R and ROC curves:\n",
      "x axis y axis\n",
      "P/R\n",
      "ROC\n",
      "Looking at the definitions of both curves' x and y axis, we see that the True Positive \n",
      "Rate in the ROC curve's y axis is the same as Recall of the P/R graph's x axis.\n",
      "The False Positive Rate measures the fraction of true negative examples that were \n",
      "falsely identified as positive ones, giving a 0 in a perfect case (no false positives) \n",
      "and 1 otherwise. Contrast this to the precision, where we track exactly the opposite, \n",
      "namely the fraction of true positive examples that we correctly classified as such.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 232, 'page_label': '212'}\n",
      "Classification  Music Genre Classification\n",
      "[ 212 ]\n",
      "Going forward, let us use ROC curves to measure our classifiers' performance to \n",
      "get a better feeling for it. The only challenge for our multiclass problem is that both \n",
      "ROC and P/R curves assume a binary classification problem. For our purpose, let us, \n",
      "therefore, create one chart per genre that shows how the classifier performed a one \n",
      "versus rest classification:\n",
      "from sklearn.metrics import roc_curve\n",
      "y_pred = clf.predict(X_test)\n",
      "for label in labels:\n",
      "    y_label_test = scipy.asarray(y_test==label, dtype=int)\n",
      "    proba = clf.predict_proba(X_test)\n",
      "    proba_label = proba[:,label] \n",
      "    # calculate false and true positive rates as well as the\n",
      "    # ROC thresholds\n",
      "    fpr, tpr, roc_thres = roc_curve(y_label_test, proba_label)\n",
      "    # plot tpr over fpr ...\n",
      "The outcomes are the following six ROC plots. As we have already found out, our \n",
      "first version of a classifier only performs well on Classical songs. Looking at the \n",
      "individual ROC curves, however, tells us that we are really underperforming for \n",
      "most of the other genres. Only Jazz and Country provide some hope. The remaining \n",
      "genres are clearly not usable.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 233, 'page_label': '213'}\n",
      "Chapter 9\n",
      "[  213 ]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 234, 'page_label': '214'}\n",
      "Classification  Music Genre Classification\n",
      "[ 214 ]\n",
      "Improving classification performance \n",
      "with Mel Frequency Cepstral Coefficients\n",
      "We already learned that FFT is pointing in the right direction, but in itself it will \n",
      "not be enough to finally arrive at a classifier that successfully manages to organize \n",
      "our scrambled directory of songs of diverse music genres into individual genre \n",
      "directories. We need a somewhat more advanced version of it.\n",
      "At this point, it is always wise to acknowledge that we have to do more research. \n",
      "Other people might have had similar challenges in the past and already have found \n",
      "out new ways that might also help us. And, indeed, there is even a yearly conference \n",
      "dedicated to only music genre classification, organized by the International Society \n",
      "for Music Information Retrieval (ISMIR). Apparently, Automatic Music Genre \n",
      "Classification (AMGC) is an established subfield of Music Information Retrieval. \n",
      "Glancing over some of the AMGC papers, we see that there is a bunch of work \n",
      "targeting automatic genre classification that might help us.\n",
      "One technique that seems to be successfully applied in many of those works is \n",
      "called Mel Frequency Cepstral Coefficients. The Mel Frequency Cepstrum (MFC) \n",
      "encodes the power spectrum of a sound, which is the power of each frequency the \n",
      "sound contains. It is calculated as the Fourier transform of the logarithm of the \n",
      "signal's spectrum. If that sounds too complicated, simply remember that the name \n",
      "\"cepstrum\" originates from \"spectrum\" having the first four characters reversed. MFC \n",
      "has been successfully used in speech and speaker recognition. Let's see whether it \n",
      "also works in our case.\n",
      "We are in a lucky situation in that someone else already needed exactly this and \n",
      "published an implementation of it as the Talkbox SciKit. We can install it from \n",
      "https://pypi.python.org/pypi/scikits.talkbox. Afterward, we can call  \n",
      "the mfcc() function, which calculates the MFC coefficients, as follows:\n",
      ">>> from scikits.talkbox.features import mfcc\n",
      ">>> sample_rate, X = scipy.io.wavfile.read(fn)\n",
      ">>> ceps, mspec, spec = mfcc(X)\n",
      ">>> print(ceps.shape)\n",
      "(4135, 13)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 235, 'page_label': '215'}\n",
      "Chapter 9\n",
      "[  215 ]\n",
      "The data we would want to feed into our classifier is stored in ceps, which contains \n",
      "13 coefficients (default value for the nceps parameter of mfcc()) for each of the 4,135 \n",
      "frames for the song with the filename fn. Taking all of the data would overwhelm \n",
      "our classifier. What we could do, instead, is to do an averaging per coefficient over \n",
      "all the frames. Assuming that the start and end of each song are possibly less genre \n",
      "specific than the middle part of it, we also ignore the first and last 10 percent:\n",
      "x = np.mean(ceps[int(num_ceps*0.1):int(num_ceps*0.9)], axis=0)\n",
      "Sure enough, the benchmark dataset we will be using contains only the first  \n",
      "30 seconds of each song, so that we would not need to cut off the last 10 percent.  \n",
      "We do it, nevertheless, so that our code works on other datasets as well, which  \n",
      "are most likely not truncated.\n",
      "Similar to our work with FFT, we certainly would also want to cache the once \n",
      "generated MFCC features and read them instead of recreating them each time  \n",
      "we train our classifier.\n",
      "This leads to the following code:\n",
      "def write_ceps(ceps, fn):\n",
      "    base_fn, ext = os.path.splitext(fn)\n",
      "    data_fn = base_fn + \".ceps\"\n",
      "    np.save(data_fn, ceps)\n",
      "    print(\"Written to %s\" % data_fn)\n",
      "def create_ceps(fn):\n",
      "    sample_rate, X = scipy.io.wavfile.read(fn)\n",
      "    ceps, mspec, spec = mfcc(X)\n",
      "    write_ceps(ceps, fn)\n",
      "def read_ceps(genre_list, base_dir=GENRE_DIR):\n",
      "    X, y = [], []\n",
      "    for label, genre in enumerate(genre_list):\n",
      "        for fn in glob.glob(os.path.join(\n",
      "                            base_dir, genre, \"*.ceps.npy\")):\n",
      "            ceps = np.load(fn)\n",
      "            num_ceps = len(ceps)\n",
      "            X.append(np.mean(\n",
      "                     ceps[int(num_ceps*0.1):int(num_ceps*0.9)], axis=0))\n",
      "            y.append(label)\n",
      "    return np.array(X), np.array(y)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 236, 'page_label': '216'}\n",
      "Classification  Music Genre Classification\n",
      "[ 216 ]\n",
      "We get the following promising results with a classifier that uses only 13 features  \n",
      "per song:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 237, 'page_label': '217'}\n",
      "Chapter 9\n",
      "[  217 ]\n",
      "The classification performances for all genres have improved. Classical and Metal are \n",
      "even at almost 1.0 AUC. And indeed, also the confusion matrix in the following plot \n",
      "looks much better now. We can clearly see the diagonal showing that the classifier \n",
      "manages to classify the genres correctly in most of the cases. This classifier is actually \n",
      "quite usable to solve our initial task.\n",
      "If we would want to improve on this, this confusion matrix tells us quickly what to \n",
      "focus on: the non-white spots on the non-diagonal places. For instance, we have a \n",
      "darker spot where we mislabel Rock songs as being Jazz with considerable probability. \n",
      "To fix this, we would probably need to dive deeper into the songs and extract things \n",
      "such as drum patterns and similar genre specific characteristics. And thenwhile \n",
      "glancing over the ISMIR paperswe also have read about the so-called Auditory \n",
      "Filterbank Temporal Envelope (AFTE) features, which seem to outperform MFCC \n",
      "features in certain situations. Maybe we should have a look at them as well?\n",
      "The nice thing is that, only equipped with ROC curves and confusion matrices, we \n",
      "are free to pull in other experts' knowledge in terms of feature extractors without \n",
      "requiring ourselves to fully understand their inner workings. Our measurement \n",
      "tools will always tell us, when the direction is right and when to change it. Of course, \n",
      "being a machine learner who is eager to learn, we will always have the dim feeling \n",
      "that there is an exciting algorithm buried somewhere in a black box of our feature \n",
      "extractors, which is just waiting for us to be understood.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 238, 'page_label': '218'}\n",
      "Classification  Music Genre Classification\n",
      "[ 218 ]\n",
      "Summary\n",
      "In this chapter, we totally stepped out of our comfort zone when we built a music \n",
      "genre classifier. Not having a deep understanding of music theory, at first we failed \n",
      "to train a classifier that predicts the music genre of songs with reasonable accuracy \n",
      "using FFT. But, then, we created a classifier that showed really usable performance \n",
      "using MFC features.\n",
      "In both the cases, we used features that we understood only enough to know how \n",
      "and where to put them into our classifier setup. The one failed, the other succeeded. \n",
      "The difference between them is that in the second case we relied on features that \n",
      "were created by experts in the field.\n",
      "And that is totally OK. If we are mainly interested in the result, we sometimes \n",
      "simply have to take shortcutswe just have to make sure to take these shortcuts \n",
      "from experts in the specific domains. And because we had learned how to correctly \n",
      "measure the performance in this new multiclass classification problem, we took these \n",
      "shortcuts with confidence.\n",
      "In the next chapter, we will look at how to apply techniques you have learned  \n",
      "in the rest of this book to this specific type of data. We will learn how to use the \n",
      "mahotas computer vision package to preprocess images using traditional image \n",
      "processing functions.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 239, 'page_label': '219'}\n",
      "[ 219 ]\n",
      "Computer Vision\n",
      "Image analysis and computer vision have always been important in industrial  \n",
      "and scientific applications. With the popularization of cell phones with powerful \n",
      "cameras and Internet connections, images now are increasingly generated by \n",
      "consumers. Therefore, there are opportunities to make use of computer vision to \n",
      "provide a better user experience in new contexts.\n",
      "In this chapter, we will look at how to apply techniques you have learned in the rest \n",
      "of this book to this specific type of data. In particular, we will learn how to use the \n",
      "mahotas computer vision package to extract features from images. These features can \n",
      "be used as input to the same classification methods we studied in other chapters. We \n",
      "will apply these techniques to publicly available datasets of photographs. We will \n",
      "also see how the same features can be used on another problem, that is, the problem \n",
      "of finding similar looking images.\n",
      "Finally, at the end of this chapter, we will learn about using local features. These \n",
      "are relatively new methods (the first of these methods to achieve state-of-the-art \n",
      "performance, the scale-invariant feature transform (SIFT), was introduced in 1999) \n",
      "and achieve very good results in many tasks.\n",
      "Introducing image processing\n",
      "From the point of view of the computer, an image is a large rectangular array of pixel \n",
      "values. Our goal is to process this image and to arrive at a decision for our application.\n",
      "The first step will be to load the image from disk, where it is typically stored in an \n",
      "image-specific format such as PNG or JPEG, the former being a lossless compression \n",
      "format, and the latter a lossy compression one that is optimized for visual assessment \n",
      "of photographs. Then, we may wish to perform preprocessing on the images (for \n",
      "example, normalizing them for illumination variations).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 240, 'page_label': '220'}\n",
      "Computer Vision\n",
      "[  220 ]\n",
      "We will have a classification problem as a driver for this chapter. We want to be \n",
      "able to learn a support vector machine (or other) classifier that can be trained from \n",
      "images. Therefore, we will use an intermediate representation, extracting numeric \n",
      "features from the images before applying machine learning.\n",
      "Loading and displaying images\n",
      "In order to manipulate images, we will use a package called mahotas. You can obtain \n",
      "mahotas from https://pypi.python.org/pypi/mahotas and read its manual \n",
      "at http://mahotas.readthedocs.org. Mahotas is an open source package (MIT \n",
      "license, so it can be used in any project) that was developed by one of the authors \n",
      "of this book. Fortunately, it is based on NumPy. The NumPy knowledge you have \n",
      "acquired so far can be used for image processing. There are other image packages, \n",
      "such as scikit-image (skimage), the ndimage (n-dimensional image) module in \n",
      "SciPy, and the Python bindings for OpenCV. All of these work natively with NumPy \n",
      "arrays, so you can even mix and match functionality from different packages to build \n",
      "a combined pipeline.\n",
      "We start by importing mahotas, with the mh abbreviation, which we will use \n",
      "throughout this chapter, as follows:\n",
      ">>> import mahotas as mh\n",
      "Now, we can load an image file using imread as follows:\n",
      ">>> image = mh.imread('scene00.jpg')\n",
      "The scene00.jpg file (this file is contained in the dataset available on this book's \n",
      "companion code repository) is a color image of height h and width w; the image will \n",
      "be an array of shape (h, w, 3). The first dimension is the height, the second is the \n",
      "width, and the third is red/green/blue. Other systems put the width in the first \n",
      "dimension, but this is the convention that is used by all NumPy-based packages. The \n",
      "type of the array will typically be np.uint8 (an unsigned 8-bit integer). These are the \n",
      "images that your camera takes or that your monitor can fully display.\n",
      "Some specialized equipment, used in scientific and technical applications, can take \n",
      "images with higher bit resolution (that is, with more sensitivity to small variations in \n",
      "brightness). Twelve or sixteen bits are common in this type of equipment. Mahotas \n",
      "can deal with all these types, including floating point images. In many computations, \n",
      "even if the original data is composed of unsigned integers, it is advantageous to \n",
      "convert to floating point numbers in order to simplify handling of rounding and \n",
      "overflow issues.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 241, 'page_label': '221'}\n",
      "Chapter 10\n",
      "[  221 ]\n",
      "Mahotas can use a variety of different input/output backends. \n",
      "Unfortunately, none of them can load all image formats that exist \n",
      "(there are hundreds, with several variations of each). However, \n",
      "loading PNG and JPEG images is supported by all of them. We \n",
      "will focus on these common formats and refer you to the mahotas \n",
      "documentation on how to read uncommon formats.\n",
      "We can display the image on screen using matplotlib, the plotting library we have \n",
      "already used several times, as follows:\n",
      ">>> from matplotlib import pyplot as plt\n",
      ">>> plt.imshow(image)\n",
      ">>> plt.show()\n",
      "As shown in the following, this code shows the image using the convention that \n",
      "the first dimension is the height and the second the width. It correctly handles color \n",
      "images as well. When using Python for numerical computation, we benefit from the \n",
      "whole ecosystem working well together: mahotas works with NumPy arrays, which \n",
      "can be displayed with matplotlib; later we will compute features from images to use \n",
      "with scikit-learn.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 242, 'page_label': '222'}\n",
      "Computer Vision\n",
      "[  222 ]\n",
      "Thresholding\n",
      "Thresholding is a very simple operation: we transform all pixel values above a \n",
      "certain threshold to 1 and all those below it to 0 (or by using Booleans, transform \n",
      "it to True and False). The important question in thresholding is to select a good \n",
      "value to use as the threshold limit. Mahotas implements a few methods for choosing \n",
      "a threshold value from the image. One is called Otsu, after its inventor. The first \n",
      "necessary step is to convert the image to grayscale, with rgb2gray in the mahotas.\n",
      "colors submodule.\n",
      "Instead of rgb2gray, we could also have just the mean value of the red, green, \n",
      "and blue channels, by callings image.mean(2). The result, however, would not \n",
      "be the same, as rgb2gray uses different weights for the different colors to give a \n",
      "subjectively more pleasing result. Our eyes are not equally sensitive to the three  \n",
      "basic colors.\n",
      ">>> image = mh.colors.rgb2grey(image, dtype=np.uint8)\n",
      ">>> plt.imshow(image) # Display the image\n",
      "By default, matplotlib will display this single-channel image as a false color image, \n",
      "using red for high values and blue for low values. For natural images, a grayscale  \n",
      "is more appropriate. You can select it with:\n",
      ">>> plt.gray()\n",
      "Now the image is shown in gray scale. Note that only the way in which the pixel \n",
      "values are interpreted and shown has changed and the image data is untouched.  \n",
      "We can continue our processing by computing the threshold value:\n",
      ">>> thresh = mh.thresholding.otsu(image)\n",
      ">>> print('Otsu threshold is {}.'.format(thresh))\n",
      "Otsu threshold is 138.\n",
      ">>> plt.imshow(image > thresh)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 243, 'page_label': '223'}\n",
      "Chapter 10\n",
      "[  223 ]\n",
      "When applied to the previous screenshot, this method finds the threshold to be 138, \n",
      "which separates the ground from the sky above, as shown in the following image:\n",
      "Gaussian blurring\n",
      "Blurring your image may seem odd, but it often serves to reduce noise, which helps \n",
      "with further processing. With mahotas, it is just a function call:\n",
      ">>> im16 = mh.gaussian_filter(image, 16)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 244, 'page_label': '224'}\n",
      "Computer Vision\n",
      "[  224 ]\n",
      "Notice that we did not convert the grayscale image to unsigned integers: we just \n",
      "made use of the floating point result as it is. The second argument to the gaussian_\n",
      "filter function is the size of the filter (the standard deviation of the filter). Larger \n",
      "values result in more blurring, as shown in the following screenshot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 245, 'page_label': '225'}\n",
      "Chapter 10\n",
      "[  225 ]\n",
      "We can use the screenshot on the left and threshold with Otsu (using the same \n",
      "previous code). Now, the boundaries are smoother, without the jagged edges,  \n",
      "as shown in the following screenshot:\n",
      "Putting the center in focus\n",
      "The final example shows how to mix NumPy operators with a tiny bit of filtering to get \n",
      "an interesting result. We start with the Lena image and split it into the color channels:\n",
      ">>> im = mh.demos.load('lena')\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 246, 'page_label': '226'}\n",
      "Computer Vision\n",
      "[  226 ]\n",
      "This is an image of a young woman that has been often for image processing demos. \n",
      "It is shown in the following screenshot:\n",
      "To split the red, green, and blue channels, we use the following code:\n",
      ">>> r,g,b = im.transpose(2,0,1)\n",
      "Now, we filter the three channels separately and build a composite image out of it \n",
      "with mh.as_rgb. This function takes three two-dimensional arrays, performs contrast \n",
      "stretching to make each be an 8-bit integer array, and then stacks them, returning a \n",
      "color RGB image:\n",
      ">>> r12 = mh.gaussian_filter(r, 12.)\n",
      ">>> g12 = mh.gaussian_filter(g, 12.)\n",
      ">>> b12 = mh.gaussian_filter(b, 12.)\n",
      ">>> im12 = mh.as_rgb(r12, g12, b12)\n",
      "Now, we blend the two images from the center away to the edges. First, we need to \n",
      "build a weights array W, which will contain at each pixel a normalized value, which is \n",
      "its distance to the center:\n",
      ">>> h, w = r.shape # height and width\n",
      ">>> Y, X = np.mgrid[:h,:w]\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 247, 'page_label': '227'}\n",
      "Chapter 10\n",
      "[  227 ]\n",
      "We used the np.mgrid object, which returns arrays of size (h, w), with values \n",
      "corresponding to the y and x coordinates, respectively. The next steps are as follows:\n",
      ">>> Y = Y - h/2. # center at h/2\n",
      ">>> Y = Y / Y.max() # normalize to -1 .. +1\n",
      ">>> X = X - w/2.\n",
      ">>> X = X / X.max()\n",
      "We now use a Gaussian function to give the center region a high value:\n",
      ">>> C = np.exp(-2.*(X**2+ Y**2))\n",
      ">>> # Normalize again to 0..1\n",
      ">>> C = C - C.min()\n",
      ">>> C = C / C.ptp()\n",
      ">>> C = C[:,:,None] # This adds a dummy third dimension to C\n",
      "Notice how all of these manipulations are performed using NumPy arrays and not \n",
      "some mahotas-specific methodology. Finally, we can combine the two images to \n",
      "have the center be in sharp focus and the edges softer:\n",
      ">>> ringed = mh.stretch(im*C + (1-C)*im12)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 248, 'page_label': '228'}\n",
      "Computer Vision\n",
      "[  228 ]\n",
      "Basic image classification\n",
      "We will start with a small dataset that was collected especially for this book. It has \n",
      "three classes: buildings, natural scenes (landscapes), and pictures of texts. There are \n",
      "30 images in each category, and they were all taken using a cell phone camera with \n",
      "minimal composition. The images are similar to those that would be uploaded to \n",
      "a modern website by users with no photography training. This dataset is available \n",
      "from this book's website or the GitHub code repository. Later in this chapter, we will \n",
      "look at a harder dataset with more images and more categories.\n",
      "When classifying images, we start with a large rectangular array of numbers (pixel \n",
      "values). Nowadays, millions of pixels are common. We could try to feed all these \n",
      "numbers as features into the learning algorithm. This is not a very good idea. This is \n",
      "because the relationship of each pixel (or even each small group of pixels) to the final \n",
      "result is very indirect. Also, having millions of pixels, but only as a small number \n",
      "of example images, results in a very hard statistical learning problem. This is an \n",
      "extreme form of the P greater than N type of problem we discussed in Chapter 7, \n",
      "Regression. Instead, a good approach is to compute features from the image and use \n",
      "those features for classification.\n",
      "Having said that, I will point out that, in fact, there are a few methods that do work \n",
      "directly from the pixel values. They have feature computation submodules inside \n",
      "them. They may even attempt to learn good features automatically. These methods \n",
      "are the topic of current research. They typically work best with very large datasets \n",
      "(millions of images).\n",
      "We previously used an example of the scene class. The following are examples of the \n",
      "text and building classes:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 249, 'page_label': '229'}\n",
      "Chapter 10\n",
      "[  229 ]\n",
      "Computing features from images\n",
      "With mahotas, it is very easy to compute features from images. There is a submodule \n",
      "named mahotas.features, where feature computation functions are available.\n",
      "A commonly used set of texture features is the Haralick. As with many methods in \n",
      "image processing, the name is due to its inventor. These features are texture-based: \n",
      "they distinguish between images that are smooth from those that are patterned, and \n",
      "between different patterns. With mahotas, it is very easy to compute them as follows:\n",
      ">>> haralick_features = mh.features.haralick(image)\n",
      ">>> haralick_features_mean = np.mean(haralick_features, axis=0)\n",
      ">>> haralick_features_all = np.ravel(haralick_features)\n",
      "The mh.features.haralick function returns a 4x13 array. The first dimension refers \n",
      "to four possible directions in which to compute the features (vertical, horizontal, \n",
      "diagonal, and the anti-diagonal). If we are not interested in the direction specifically, \n",
      "we can use the average over all the directions (shown in the earlier code as haralick_\n",
      "features_mean). Otherwise, we can use all the features separately (using haralick_\n",
      "features_all). This decision should be informed by the properties of the dataset. \n",
      "In our case, we reason that the horizontal and vertical directions should be kept \n",
      "separately. Therefore, we will use haralick_features_all.\n",
      "There are a few other feature sets implemented in mahotas. Linear binary patterns \n",
      "are another texture-based feature set, which is very robust against illumination \n",
      "changes. There are other types of features, including local features, which we will \n",
      "discuss later in this chapter.\n",
      "With these features, we use a standard classification method such as logistic \n",
      "regression as follows:\n",
      ">>> from glob import glob\n",
      ">>> images = glob('SimpleImageDataset/*.jpg')\n",
      ">>> features = []\n",
      ">>> labels = []\n",
      ">>> for im in images:\n",
      "...   labels.append(im[:-len('00.jpg')])\n",
      "...   im = mh.imread(im)\n",
      "...   im = mh.colors.rgb2gray(im, dtype=np.uint8)\n",
      "...   features.append(mh.features.haralick(im).ravel())\n",
      ">>> features = np.array(features)\n",
      ">>> labels = np.array(labels)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 250, 'page_label': '230'}\n",
      "Computer Vision\n",
      "[  230 ]\n",
      "The three classes have very different textures. Buildings have sharp edges and big \n",
      "blocks where the color is similar (the pixel values are rarely exactly the same, but \n",
      "the variation is slight). Text is made of many sharp dark-light transitions, with small \n",
      "black areas in a sea of white. Natural scenes have smoother variations with fractal-like \n",
      "transitions. Therefore, a classifier based on texture is expected to do well.\n",
      "As a classifier, we are going to use a logistic regression classifier with preprocessing \n",
      "of the features as follows:\n",
      ">>> from sklearn.pipeline import Pipeline\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      ">>> from sklearn.linear_model import LogisticRegression\n",
      ">>> clf = Pipeline([('preproc', StandardScaler()),\n",
      "                    ('classifier', LogisticRegression())])\n",
      "Since our dataset is small, we can use leave-one-out regression as follows:\n",
      ">>> from sklearn import cross_validation\n",
      ">>> cv = cross_validation.LeaveOneOut(len(images))\n",
      ">>> scores = cross_validation.cross_val_score(\n",
      "...     clf, features, labels, cv=cv)\n",
      ">>> print('Accuracy: {:.1%}'.format(scores.mean()))\n",
      "Accuracy: 81.1%\n",
      "Eighty-one percent is not bad for the three classes (random guessing would \n",
      "correspond to 33 percent). We can do better, however, by writing our own features.\n",
      "Writing your own features\n",
      "A feature is nothing magical. It is simply a number that we computed from an image. \n",
      "There are several feature sets already defined in the literature. These often have the \n",
      "added advantage that they have been designed and studied to be invariant to many \n",
      "unimportant factors. For example, linear binary patterns are completely invariant \n",
      "to multiplying all pixel values by a number or adding a constant to all these values. \n",
      "This makes this feature set robust against illumination changes of images.\n",
      "However, it is also possible that your particular use case would benefit from a few \n",
      "specially designed features.\n",
      "A simple type of feature that is not shipped with mahotas is a color histogram. \n",
      "Fortunately, this feature is easy to implement. A color histogram partitions the color \n",
      "space into a set of bins, and then counts how many pixels fall into each of the bins.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 251, 'page_label': '231'}\n",
      "Chapter 10\n",
      "[  231 ]\n",
      "The images are in RGB format, that is, each pixel has three values: R for red, G for \n",
      "green, and B for blue. Since each of these components is an 8-bit value, the total is  \n",
      "17 million different colors. We are going to reduce this number to only 64 colors  \n",
      "by grouping colors into bins. We will write a function to encapsulate this algorithm \n",
      "as follows:\n",
      "def chist(im):\n",
      "To bin the colors, we first divide the image by 64, rounding down the pixel values  \n",
      "as follows:\n",
      "    im = im // 64\n",
      "This makes the pixel values range from 0 to 3, which gives a total of 64 different colors.\n",
      "Separate the red, green, and blue channels as follows:\n",
      "    r,g,b = im.transpose((2,0,1))\n",
      "    pixels = 1 * r + 4 * b + 16 * g\n",
      "    hist = np.bincount(pixels.ravel(), minlength=64)\n",
      "    hist = hist.astype(float)\n",
      "Convert to log scale, as seen in the following code snippet. This is not strictly \n",
      "necessary, but makes for better features. We use np.log1p, which computes log(h+1). \n",
      "This ensures that zero values are kept as zero values (mathematically, the logarithm \n",
      "of zero is not defined, and NumPy prints a warning if you attempt to compute it).\n",
      "    hist = np.log1p(hist)\n",
      "    return hist\n",
      "We can adapt the previous processing code to use the function we wrote very easily:\n",
      ">>> features = []\n",
      ">>> for im in images:\n",
      "...   image = mh.imread(im)\n",
      "...   features.append(chist(im))\n",
      "Using the same cross-validation code we used earlier, we obtain 90 percent accuracy. \n",
      "The best results, however, come from combining all the features, which we can \n",
      "implement as follows:\n",
      ">>> features = []\n",
      ">>> for im in images:\n",
      "...   imcolor = mh.imread(im)\n",
      "...   im = mh.colors.rgb2gray(imcolor, dtype=np.uint8)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 252, 'page_label': '232'}\n",
      "Computer Vision\n",
      "[  232 ]\n",
      "...   features.append(np.concatenate([\n",
      "...           mh.features.haralick(im).ravel(),\n",
      "...           chist(imcolor),\n",
      "...       ]))\n",
      "By using all of these features, we get 95.6 percent accuracy, as shown in the following \n",
      "code snippet:\n",
      ">>> scores = cross_validation.cross_val_score(\n",
      "...     clf, features, labels, cv=cv)\n",
      ">>> print('Accuracy: {:.1%}'.format(scores.mean()))\n",
      "Accuracy: 95.6%\n",
      "This is a perfect illustration of the principle that good algorithms are the easy \n",
      "part. You can always use an implementation of state-of-the-art classification from \n",
      "scikit-learn. The real secret and added value often comes in feature design and \n",
      "engineering. This is where knowledge of your dataset is valuable.\n",
      "Using features to find similar images\n",
      "The basic concept of representing an image by a relatively small number of features \n",
      "can be used for more than just classification. For example, we can also use it to find \n",
      "similar images to a given query image (as we did before with text documents).\n",
      "We will compute the same features as before, with one important difference: we \n",
      "will ignore the bordering area of the picture. The reason is that due to the amateur \n",
      "nature of the compositions, the edges of the picture often contain irrelevant elements. \n",
      "When the features are computed over the whole image, these elements are taken into \n",
      "account. By simply ignoring them, we get slightly better features. In the supervised \n",
      "example, it is not as important, as the learning algorithm will then learn which \n",
      "features are more informative and weigh them accordingly. When working in an \n",
      "unsupervised fashion, we need to be more careful to ensure that our features are \n",
      "capturing important elements of the data. This is implemented in the loop as follows:\n",
      ">>> features = []\n",
      ">>> for im in images:\n",
      "...   imcolor = mh.imread(im)\n",
      "...   # ignore everything in the 200 pixels closest to the borders\n",
      "...   imcolor = imcolor[200:-200, 200:-200]\n",
      "...   im = mh.colors.rgb2gray(imcolor, dtype=np.uint8)\n",
      "...   features.append(np.concatenate([\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 253, 'page_label': '233'}\n",
      "Chapter 10\n",
      "[  233 ]\n",
      "...           mh.features.haralick(im).ravel(),\n",
      "...           chist(imcolor),\n",
      "...       ]))\n",
      "We now normalize the features and compute the distance matrix as follows:\n",
      ">>> sc = StandardScaler()\n",
      ">>> features = sc.fit_transform(features)\n",
      ">>> from scipy.spatial import distance\n",
      ">>> dists = distance.squareform(distance.pdist(features))\n",
      "We will plot just a subset of the data (every 10th element) so that the query will be on \n",
      "top and the returned \"nearest neighbor\" at the bottom, as shown in the following:\n",
      ">>> fig, axes = plt.subplots(2, 9)\n",
      ">>> for ci,i in enumerate(range(0,90,10)):\n",
      "...     left = images[i]\n",
      "...     dists_left = dists[i]\n",
      "...     right = dists_left.argsort()\n",
      "...     # right[0] is same as left[i], so pick next closest\n",
      "...     right = right[1]\n",
      "...     right = images[right]\n",
      "...     left = mh.imread(left)\n",
      "...     right = mh.imread(right)\n",
      "...     axes[0, ci].imshow(left)\n",
      "...     axes[1, ci].imshow(right)\n",
      "The result is shown in the following screenshot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 254, 'page_label': '234'}\n",
      "Computer Vision\n",
      "[  234 ]\n",
      "It is clear that the system is not perfect, but can find images that are at least visually \n",
      "similar to the queries. In all but one case, the image found comes from the same class \n",
      "as the query.\n",
      "Classifying a harder dataset\n",
      "The previous dataset was an easy dataset for classification using texture features. \n",
      "In fact, many of the problems that are interesting from a business point of view are \n",
      "relatively easy. However, sometimes we may be faced with a tougher problem and \n",
      "need better and more modern techniques to get good results.\n",
      "We will now test a public dataset, which has the same structure: several photographs \n",
      "split into a small number of classes. The classes are animals, cars, transportation, and \n",
      "natural scenes.\n",
      "When compared to the three class problem we discussed previously, these classes are \n",
      "harder to tell apart. Natural scenes, buildings, and texts have very different textures. \n",
      "In this dataset, however, texture and color are not as clear marker, of the image class. \n",
      "The following is one example from the animal class:\n",
      "And here is another example from the car class:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 255, 'page_label': '235'}\n",
      "Chapter 10\n",
      "[  235 ]\n",
      "Both objects are against natural backgrounds, and with large smooth areas inside the \n",
      "objects. This is a harder problem than the simple dataset, so we will need to use more \n",
      "advanced methods. The first improvement will be to use a slightly more powerful \n",
      "classifier. The logistic regression that scikit-learn provides is a penalized form of \n",
      "logistic regression, which contains an adjustable parameter, C. By default, C = 1.0, \n",
      "but this may not be optimal. We can use grid search to find a good value for this \n",
      "parameter as follows:\n",
      ">>> from sklearn.grid_search import GridSearchCV\n",
      ">>> C_range = 10.0 ** np.arange(-4, 3)\n",
      ">>> grid = GridSearchCV(LogisticRegression(), param_grid={'C' : C_range})\n",
      ">>> clf = Pipeline([('preproc', StandardScaler()),\n",
      "...                ('classifier', grid)])\n",
      "The data is not organized in a random order inside the dataset: similar images are close \n",
      "together. Thus, we use a cross-validation schedule that considers the data shuffled so \n",
      "that each fold has a more representative training set, as shown in the following:\n",
      ">>> cv = cross_validation.KFold(len(features), 5,\n",
      "...                      shuffle=True, random_state=123)\n",
      ">>> scores = cross_validation.cross_val_score(\n",
      "...    clf, features, labels, cv=cv)\n",
      ">>> print('Accuracy: {:.1%}'.format(scores.mean()))\n",
      "Accuracy: 72.1%\n",
      "This is not so bad for four classes, but we will now see if we can do better by using \n",
      "a different set of features. In fact, we will see that we need to combine these features \n",
      "with other methods to get the best possible results.\n",
      "Local feature representations\n",
      "A relatively recent development in the computer vision world has been the \n",
      "development of local-feature based methods. Local features are computed on a small \n",
      "region of the image, unlike the previous features we considered, which had been \n",
      "computed on the whole image. Mahotas supports computing a type of these features, \n",
      "Speeded Up Robust Features (SURF). There are several others, the most well-known \n",
      "being the original proposal of SIFT. These features are designed to be robust against \n",
      "rotational or illumination changes (that is, they only change their value slightly when \n",
      "illumination changes).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 256, 'page_label': '236'}\n",
      "Computer Vision\n",
      "[  236 ]\n",
      "When using these features, we have to decide where to compute them. There are \n",
      "three possibilities that are commonly used:\n",
      " Randomly\n",
      " In a grid\n",
      " Detecting interesting areas of the image (a technique known as keypoint \n",
      "detection or interest point detection)\n",
      "All of these are valid and will, under the right circumstances, give good results. \n",
      "Mahotas supports all three. Using interest point detection works best if you have a \n",
      "reason to expect that your interest point will correspond to areas of importance in  \n",
      "the image.\n",
      "We will be using the interest point method. Computing the features with mahotas is \n",
      "easy: import the right submodule and call the surf.surf function as follows:\n",
      ">>> from mahotas.features import surf\n",
      ">>> image = mh.demos.load('lena')\n",
      ">>> image = mh.colors.rgb2gray(im, dtype=np.uint8)\n",
      ">>> descriptors = surf.surf(image, descriptor_only=True)\n",
      "The descriptors_only=True flag means that we are only interested in the \n",
      "descriptors themselves, and not in their pixel location, size, or orientation. \n",
      "Alternatively, we could have used the dense sampling method, using the surf.\n",
      "dense function as follows:\n",
      ">>> from mahotas.features import surf\n",
      ">>> descriptors = surf.dense(image, spacing=16)\n",
      "This returns the value of the descriptors computed on points that are at a distance of \n",
      "16 pixels from each other. Since the position of the points is fixed, the metainformation \n",
      "on the interest points is not very interesting and is not returned by default. In either \n",
      "case, the result (descriptors) is an n-times-64 array, where n is the number of points \n",
      "sampled. The number of points depends on the size of your images, their content, and \n",
      "the parameters you pass to the functions. In this example, we are using the default \n",
      "settings, and we obtain a few hundred descriptors per image.\n",
      "We cannot directly feed these descriptors to a support vector machine, logistic \n",
      "regressor, or similar classification system. In order to use the descriptors from the \n",
      "images, there are several solutions. We could just average them, but the results of \n",
      "doing so are not very good as they throw away all location specific information. In  \n",
      "that case, we would have just another global feature set based on edge measurements.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 257, 'page_label': '237'}\n",
      "Chapter 10\n",
      "[  237 ]\n",
      "The solution we will use here is the bag of words model, which is a very recent idea. \n",
      "It was published in this form first in 2004. This is one of those obvious-in-hindsight \n",
      "ideas: it is very simple to implement and achieves very good results.\n",
      "It may seem strange to speak of words when dealing with images. It may be easier \n",
      "to understand if you think that you have not written words, which are easy to \n",
      "distinguish from each other, but orally spoken audio. Now, each time a word is \n",
      "spoken, it will sound slightly different, and different speakers will have their own \n",
      "pronunciation. Thus, a word's waveform will not be identical every time it is spoken. \n",
      "However, by using clustering on these waveforms, we can hope to recover most of \n",
      "the structure so that all the instances of a given word are in the same cluster. Even \n",
      "if the process is not perfect (and it will not be), we can still talk of grouping the \n",
      "waveforms into words.\n",
      "We perform the same operation with image data: we cluster together similar looking \n",
      "regions from all images and call these visual words.\n",
      "The number of words used does not usually have a big impact on the final \n",
      "performance of the algorithm. Naturally, if the number is extremely small \n",
      "(10 or 20, when you have a few thousand images), then the overall system \n",
      "will not perform well. Similarly, if you have too many words (many more \n",
      "than the number of images, for example), the system will also not perform \n",
      "well. However, in between these two extremes, there is often a very large \n",
      "plateau, where you can choose the number of words without a big impact \n",
      "on the result. As a rule of thumb, using a value such as 256, 512, or 1,024 if \n",
      "you have very many images should give you a good result.\n",
      "We are going to start by computing the features as follows:\n",
      ">>> alldescriptors = []\n",
      ">>> for im in images:\n",
      "...   im = mh.imread(im, as_grey=True)\n",
      "...   im = im.astype(np.uint8)\n",
      "...   alldescriptors.append(surf.dense(image, spacing=16))\n",
      ">>> # get all descriptors into a single array\n",
      ">>> concatenated = np.concatenate(alldescriptors)\n",
      ">>> print('Number of descriptors: {}'.format(\n",
      "...        len(concatenated)))\n",
      "Number of descriptors: 2489031\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 258, 'page_label': '238'}\n",
      "Computer Vision\n",
      "[  238 ]\n",
      "This results in over 2 million local descriptors. Now, we use k-means clustering  \n",
      "to obtain the centroids. We could use all the descriptors, but we are going to use  \n",
      "a smaller sample for extra speed, as shown in the following:\n",
      ">>> # use only every 64th vector\n",
      ">>> concatenated = concatenated[::64]\n",
      ">>> from sklearn.cluster import KMeans\n",
      ">>> k = 256\n",
      ">>> km = KMeans(k)\n",
      ">>> km.fit(concatenated)\n",
      "After this is done (which will take a while), the km object contains information about \n",
      "the centroids. We now go back to the descriptors and build feature vectors as follows:\n",
      ">>> sfeatures = []\n",
      ">>> for d in alldescriptors:\n",
      "...   c = km.predict(d)\n",
      "...   sfeatures.append(\n",
      "...       np.array([np.sum(c == ci) for ci in range(k)])\n",
      "...   )\n",
      ">>> # build single array and convert to float\n",
      ">>> sfeatures = np.array(sfeatures, dtype=float)\n",
      "The end result of this loop is that sfeatures[fi, fj] is the number of times that \n",
      "the image fi contains the element fj. The same could have been computed faster \n",
      "with the np.histogram function, but getting the arguments just right is a little tricky. \n",
      "We convert the result to floating point as we do not want integer arithmetic (with its \n",
      "rounding semantics).\n",
      "The result is that each image is now represented by a single array of features, of \n",
      "the same size (the number of clusters, in our case 256). Therefore, we can use our \n",
      "standard classification methods as follows:\n",
      ">>> scores = cross_validation.cross_val_score(\n",
      "...    clf, sfeatures, labels, cv=cv)\n",
      ">>> print('Accuracy: {:.1%}'.format(scores.mean()))\n",
      "Accuracy: 62.6%\n",
      "This is worse than before! Have we gained nothing?\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 259, 'page_label': '239'}\n",
      "Chapter 10\n",
      "[  239 ]\n",
      "In fact, we have, as we can combine all features together to obtain 76.1 percent \n",
      "accuracy, as follows:\n",
      ">>> combined = np.hstack([features, features])\n",
      ">>> scores = cross_validation.cross_val_score(\n",
      "...    clf, combined, labels, cv=cv)\n",
      ">>> print('Accuracy: {:.1%}'.format(scores.mean()))\n",
      "Accuracy: 76.1%\n",
      "This is the best result we have, better than any single feature set. This is due to the \n",
      "fact that the local SURF features are different enough to add new information to the \n",
      "global image features we had before and improve the combined result.\n",
      "Summary\n",
      "We learned the classical feature-based approach to handling images in a machine \n",
      "learning context: by converting from a million pixels to a few numeric features, we \n",
      "are able to directly use a logistic regression classifier. All of the technologies that \n",
      "we learned in the other chapters suddenly become directly applicable to image \n",
      "problems. We saw one example in the use of image features to find similar images in \n",
      "a dataset.\n",
      "We also learned how to use local features, in a bag of words model, for classification. \n",
      "This is a very modern approach to computer vision and achieves good results while \n",
      "being robust to many irrelevant aspects of the image, such as illumination, and \n",
      "even uneven illumination in the same image. We also used clustering as a useful \n",
      "intermediate step in classification rather than as an end in itself.\n",
      "We focused on mahotas, which is one of the major computer vision libraries in \n",
      "Python. There are others that are equally well maintained. Skimage (scikit-image) \n",
      "is similar in spirit, but has a different set of features. OpenCV is a very good C++ \n",
      "library with a Python interface. All of these can work with NumPy arrays and you \n",
      "can mix and match functions from different libraries to build complex computer \n",
      "vision pipelines.\n",
      "In the next chapter, you will learn a different form of machine learning: \n",
      "dimensionality reduction. As we saw in several earlier chapters, including when \n",
      "using images in this chapter, it is very easy to computationally generate many \n",
      "features. However, often we want to have a reduced number of features for speed \n",
      "and visualization, or to improve our results. In the next chapter, we will see how to \n",
      "achieve this.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 260, 'page_label': '240'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 261, 'page_label': '241'}\n",
      "[ 241 ]\n",
      "Dimensionality Reduction\n",
      "Garbage in, garbage outthroughout the book, we saw this pattern also holds true \n",
      "when applying machine learning methods to training data. Looking back, we realize \n",
      "that the most interesting machine learning challenges always involved some sort of \n",
      "feature engineering, where we tried to use our insight into the problem to carefully \n",
      "crafted additional features that the machine learner hopefully picks up.\n",
      "In this chapter, we will go in the opposite direction with dimensionality reduction \n",
      "involving cutting away features that are irrelevant or redundant. Removing features \n",
      "might seem counter-intuitive at first thought, as more information should always  \n",
      "be better than less information. Also, even if we had redundant features in our \n",
      "dataset, would not the learning algorithm be able to quickly figure it out and set  \n",
      "their weights to 0? The following are several good reasons that are still in practice  \n",
      "for trimming down the dimensions as much as possible:\n",
      " Superfluous features can irritate or mislead the learner. This is not the case \n",
      "with all machine learning methods (for example, Support Vector Machines \n",
      "love high dimensional spaces). However, most of the models feel safer with \n",
      "fewer dimensions.\n",
      " Another argument against high dimensional feature spaces is that more \n",
      "features mean more parameters to tune and a higher risk to overfit.\n",
      " The data we retrieved to solve our task might have just artificially high \n",
      "dimensionality, whereas the real dimension might be small.\n",
      " Fewer dimensions = faster training = more parameter variations to try out in \n",
      "the same time frame = better end result.\n",
      " Visualizationif we want to visualize the data we are restricted to two or \n",
      "three dimensions.\n",
      "So, here we will show how to get rid of the garbage within our data while keeping \n",
      "the real valuable part of it.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 262, 'page_label': '242'}\n",
      "Dimensionality Reduction\n",
      "[  242 ]\n",
      "Sketching our roadmap\n",
      "Dimensionality reduction can be roughly grouped into feature selection and \n",
      "feature extraction methods. We already employed some kind of feature selection \n",
      "in almost every chapter when we invented, analyzed, and then probably dropped \n",
      "some features. In this chapter, we will present some ways that use statistical \n",
      "methods, namely correlation and mutual information, to be able to do so in vast \n",
      "feature spaces. Feature extraction tries to transform the original feature space into \n",
      "a lower-dimensional feature space. This is especially useful when we cannot get \n",
      "rid of features using selection methods, but still we have too many features for our \n",
      "learner. We will demonstrate this using principal component analysis (PCA), linear \n",
      "discriminant analysis (LDA), and multidimensional scaling (MDS).\n",
      "Selecting features\n",
      "If we want to be nice to our machine learning algorithm, we provide it with features \n",
      "that are not dependent on each other, yet highly dependent on the value to be \n",
      "predicted. This means that each feature adds salient information. Removing any of \n",
      "the features will lead to a drop in performance.\n",
      "If we have only a handful of features, we could draw a matrix of scatter plots (one \n",
      "scatter plot for every feature pair combination). Relationships between the features \n",
      "could then be easily spotted. For every feature pair showing an obvious dependence, \n",
      "we would then think of whether we should remove one of them or better design a \n",
      "newer, cleaner feature out of both.\n",
      "Most of the time, however, we have more than a handful of features to choose from. \n",
      "Just think of the classification task where we had a bag of words to classify the \n",
      "quality of an answer, which would require a 1,000 by 1,000 scatter plot. In this case, \n",
      "we need a more automated way to detect overlapping features and to resolve them. \n",
      "We will present two general ways to do so in the following subsections, namely \n",
      "filters and wrappers.\n",
      "Detecting redundant features using filters\n",
      "Filters try to clean up the feature forest independent of any later used machine \n",
      "learning method. They rely on statistical methods to find which of the features \n",
      "are redundant or irrelevant. In case of redundant features, it keeps only one per \n",
      "redundant feature group. Irrelevant features will simply be removed. In general,  \n",
      "the filter works as depicted in the following workflow:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 263, 'page_label': '243'}\n",
      "Chapter 11\n",
      "[  243 ]\n",
      "Select features\n",
      "that are not\n",
      "redundant\n",
      "y\n",
      "Select features\n",
      "that are not\n",
      "irrelevant\n",
      "All features\n",
      "x1, x2, ..., xN\n",
      "Some features\n",
      "x2, x7, ..., xM\n",
      "Resulting\n",
      "features\n",
      "x2, x10, x14\n",
      "Correlation\n",
      "Using correlation, we can easily see linear relationships between pairs of features. \n",
      "In the following graphs, we can see different degrees of correlation, together with \n",
      "a potential linear dependency plotted as a red-dashed line (fitted 1-dimensional \n",
      "polynomial). The correlation coefficient ( ) 1 2,Cor X X( ) 1 2,Cor X X  at the top of the individual graphs \n",
      "is calculated using the common Pearson correlation coefficient (Pearson r value) by \n",
      "means of the pearsonr() function of scipy.stat.\n",
      "Given two equal-sized data series, it returns a tuple of the correlation coefficient \n",
      "value and the p-value. The p-value describes how likely it is that the data series has \n",
      "been generated by an uncorrelated system. In other words, the higher the p-value, \n",
      "the less we should trust the correlation coefficient:\n",
      ">>> from scipy.stats import pearsonr\n",
      ">>> pearsonr([1,2,3], [1,2,3.1])\n",
      ">>> (0.99962228516121843, 0.017498096813278487)\n",
      ">>> pearsonr([1,2,3], [1,20,6])\n",
      ">>> (0.25383654128340477, 0.83661493668227405)\n",
      "In the first case, we have a clear indication that both series are correlated. In the \n",
      "second case, we still have a clearly non-zero r  value.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 264, 'page_label': '244'}\n",
      "Dimensionality Reduction\n",
      "[  244 ]\n",
      "However, the p-value of 0.84 tells us that the correlation coefficient is not significant \n",
      "and we should not pay too close attention to it. Have a look at the following graphs:\n",
      "In the first three cases that have high correlation coefficients, we would probably \n",
      "want to throw out either 1X  or 2X  because they seem to convey similar, if not the \n",
      "same, information.\n",
      "In the last case, however, we should keep both features. In our application, this \n",
      "decision would, of course, be driven by this p-value.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 265, 'page_label': '245'}\n",
      "Chapter 11\n",
      "[  245 ]\n",
      "Although, it worked nicely in the preceding example, reality is seldom nice to us. \n",
      "One big disadvantage of correlation-based feature selection is that it only detects \n",
      "linear relationships (a relationship that can be modelled by a straight line). If we use \n",
      "correlation on a non-linear data, we see the problem. In the following example, we \n",
      "have a quadratic relationship:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 266, 'page_label': '246'}\n",
      "Dimensionality Reduction\n",
      "[  246 ]\n",
      "Although, the human eye immediately sees the relationship between X1 and X2 \n",
      "in all but the bottom-right graph, the correlation coefficient does not. It's obvious \n",
      "that correlation is useful to detect linear relationships, but fails for everything \n",
      "else. Sometimes, it already helps to apply simple transformations to get a linear \n",
      "relationship. For instance, in the preceding plot, we would have got a high \n",
      "correlation coefficient if we had drawn X2 over X1 squared. Normal data,  \n",
      "however, does not often offer this opportunity.\n",
      "Luckily, for non-linear relationships, mutual information comes to the rescue.\n",
      "Mutual information\n",
      "When looking at the feature selection, we should not focus on the type of \n",
      "relationship as we did in the previous section (linear relationships). Instead, we \n",
      "should think in terms of how much information one feature provides (given that  \n",
      "we already have another).\n",
      "To understand this, let's pretend that we want to use features from house_size, \n",
      "number_of_levels, and avg_rent_price feature set to train a classifier that outputs \n",
      "whether the house has an elevator or not. In this example, we intuitively see that \n",
      "knowing house_size we don't need to know number_of_levels anymore, as it \n",
      "contains, somehow, redundant information. With avg_rent_price, it's different \n",
      "because we cannot infer the value of rental space simply from the size of the house \n",
      "or the number of levels it has. Thus, it would be wise to keep only one of them in \n",
      "addition to the average price of rental space.\n",
      "Mutual information formalizes the aforementioned reasoning by calculating how \n",
      "much information two features have in common. However, unlike correlation, it \n",
      "does not rely on a sequence of data, but on the distribution. To understand how it \n",
      "works, we have to dive a bit into information entropy.\n",
      "Let's assume we have a fair coin. Before we flip it, we will have maximum \n",
      "uncertainty as to whether it will show heads or tails, as both have an equal \n",
      "probability of 50 percent. This uncertainty can be measured by means of  \n",
      "Claude Shannon's information entropy:\n",
      "( ) ( ) ( )2\n",
      "1\n",
      "log\n",
      "n\n",
      "i i\n",
      "i\n",
      "H X p X p X\n",
      "=\n",
      "=  \n",
      "In our fair coin case, we have two cases: Let 0x  be the case of head and 1x  the case of \n",
      "tail with ( ) ( )0 1 0.5p X p X= = .\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 267, 'page_label': '247'}\n",
      "Chapter 11\n",
      "[  247 ]\n",
      "Thus, it concludes to:\n",
      "( ) ( ) ( ) ( ) ( ) ( )\n",
      "( )\n",
      "0 2 0 1 2 1 2\n",
      "2\n",
      "log log 0.5 log 0.5 0.5\n",
      "log 0.5 1.0\n",
      "H X p x p x p x p x=  =   \n",
      " =\n",
      "For convenience, we can also use scipy.stats.entropy([0.5, \n",
      "0.5], base=2). We set the base parameter to 2 to get the same \n",
      "result as earlier. Otherwise, the function will use the natural \n",
      "logarithm via np.log(). In general, the base does not matter (as \n",
      "long as you use it consistently).\n",
      "Now, imagine we knew upfront that the coin is actually not that fair with heads \n",
      "having a chance of 60 percent showing up after flipping:\n",
      "( ) ( ) ( )2 20.6 log 0.6 0.4 log 0.4 0.97H X =     =\n",
      "We see that this situation is less uncertain. The uncertainty will decrease the farther \n",
      "away we get from 0.5 reaching the extreme value of 0 for either 0 percent or 100 \n",
      "percent of heads showing up, as we can see in the following graph:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 268, 'page_label': '248'}\n",
      "Dimensionality Reduction\n",
      "[  248 ]\n",
      "We will now modify entropy ( )H X  by applying it to two features instead of one in \n",
      "such a way that it measures how much uncertainty is removed from X when we learn \n",
      "about Y. Then, we can catch how one feature reduces the uncertainty of another.\n",
      "For example, without having any further information about the weather, we are \n",
      "totally uncertain whether it's raining outside or not. If we now learn that the grass \n",
      "outside is wet, the uncertainty has been reduced (we will still have to check whether \n",
      "the sprinkler had been turned on).\n",
      "More formally, mutual information is defined as:\n",
      "( ) ( ) ( )\n",
      "( ) ( )\n",
      "2\n",
      "1 1\n",
      ",\n",
      "; , log\n",
      "m n i j\n",
      "i j\n",
      "i j i j\n",
      "P X Y\n",
      "I X Y P X Y\n",
      "P X P Y= =\n",
      "= \n",
      "This looks a bit intimidating, but is really not more than sums and products.  \n",
      "For instance, the calculation of ()P  is done by binning the feature values and  \n",
      "then calculating the fraction of values in each bin. In the following plots, we  \n",
      "have set the number of bins to ten.\n",
      "In order to restrict mutual information to the interval [0,1], we have to divide it by \n",
      "their added individual entropy, which gives us the normalized mutual information:\n",
      "( ) ( )\n",
      "( ) ( )\n",
      ";; I X YNI X Y H X H Y= +\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 269, 'page_label': '249'}\n",
      "Chapter 11\n",
      "[  249 ]\n",
      "The nice thing about mutual information is that unlike correlation, it does not look \n",
      "only at linear relationships, as we can see in the following graphs:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 270, 'page_label': '250'}\n",
      "Dimensionality Reduction\n",
      "[  250 ]\n",
      "As we can see, mutual information is able to indicate the strength of a linear \n",
      "relationship. The following diagram shows that, it also works for squared relationships:\n",
      "So, what we would have to do is to calculate the normalized mutual information for \n",
      "all feature pairs. For every pair having too high value (we would have to determine \n",
      "what this means), we would then drop one of them. In case of regression, we could \n",
      "drop this feature that has too low mutual information with the desired result value.\n",
      "This might work for a not too-big set of features. At some point, however, this \n",
      "procedure can be really expensive, because the amount of calculation grows \n",
      "quadratically (as we are computing the mutual information between feature pairs).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 271, 'page_label': '251'}\n",
      "Chapter 11\n",
      "[  251 ]\n",
      "Another huge disadvantage of filters is that they drop features that seem to be not \n",
      "useful in isolation. More often than not, there are a handful of features that seem to \n",
      "be totally independent of the target variable, yet when combined together they rock. \n",
      "To keep these, we need wrappers.\n",
      "Asking the model about the features using wrappers\n",
      "While filters can help tremendously in getting rid of useless features, they can \n",
      "go only so far. After all the filtering, there might still be some features that are \n",
      "independent among themselves and show some degree of dependence with the \n",
      "result variable, but yet they are totally useless from the model's point of view. Just \n",
      "think of the following data that describes the XOR function. Individually, neither A \n",
      "nor B would show any signs of dependence on Y, whereas together they clearly do:\n",
      "A B Y\n",
      "0 0 0\n",
      "0 1 1\n",
      "1 0 1\n",
      "1 1 0\n",
      "So, why not ask the model itself to give its vote on the individual features? This is \n",
      "what wrappers do, as we can see in the following process chart:\n",
      "Train model\n",
      "withyand check\n",
      "the importance\n",
      "of individual\n",
      "features\n",
      "y\n",
      "Feature set too big\n",
      "Drop features\n",
      "that are\n",
      "unimportant\n",
      "Current\n",
      "features,\n",
      "initialized with\n",
      "all features\n",
      "x1, x2, ..., xN\n",
      "Importance of\n",
      "individual\n",
      "features\n",
      "Resulting\n",
      "features\n",
      "x2, x10, x14\n",
      "Yes\n",
      "No\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 272, 'page_label': '252'}\n",
      "Dimensionality Reduction\n",
      "[  252 ]\n",
      "Here, we pushed the calculation of feature importance to the model training process. \n",
      "Unfortunately (but understandably), feature importance is not determined as a \n",
      "binary, but as a ranking value. So, we still have to specify where to make the cut, \n",
      "what part of the features are we willing to take, and what part do we want to drop?\n",
      "Coming back to scikit-learn, we find various excellent wrapper classes in the sklearn.\n",
      "feature_selection package. A real workhorse in this field is RFE, which stands for \n",
      "recursive feature elimination. It takes an estimator and the desired number of features \n",
      "to keep as parameters and then trains the estimator with various feature sets as long as \n",
      "it has found a subset of features that is small enough. The RFE instance itself pretends \n",
      "to be like an estimator, thereby, indeed, wrapping the provided estimator.\n",
      "In the following example, we create an artificial classification problem of 100 samples \n",
      "using datasets' convenient make_classification() function. It lets us specify \n",
      "the creation of 10 features, out of which only three are really valuable to solve the \n",
      "classification problem:\n",
      ">>> from sklearn.feature_selection import RFE\n",
      ">>> from sklearn.linear_model import LogisticRegression\n",
      ">>> from sklearn.datasets import make_classification\n",
      ">>> X,y = make_classification(n_samples=100, n_features=10,  \n",
      "n_informative=3, random_state=0)\n",
      ">>> clf = LogisticRegression()\n",
      ">>> clf.fit(X, y)\n",
      ">>> selector = RFE(clf, n_features_to_select=3)\n",
      ">>> selector = selector.fit(X, y)\n",
      ">>> print(selector.support_)\n",
      "[False  True False  True False False False False  True False]\n",
      ">>> print(selector.ranking_)\n",
      "[4 1 3 1 8 5 7 6 1 2]\n",
      "The problem in real-world scenarios is, of course, how can we know the right value \n",
      "for n_features_to_select? Truth is, we can't. However, most of the time we can \n",
      "use a sample of the data and play with it using different settings to quickly get a \n",
      "feeling for the right ballpark.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 273, 'page_label': '253'}\n",
      "Chapter 11\n",
      "[  253 ]\n",
      "The good thing is that we don't have to be that exact using wrappers. Let's try different \n",
      "values for n_features_to_select to see how support_ and ranking_ change:\n",
      "n_features_\n",
      "to_select\n",
      "support_ ranking_\n",
      "1 [False False False  True False False False False False \n",
      "False]\n",
      "[ 6  3  5  1 10  7  9  \n",
      "8  2  4]\n",
      "2 [False False False  True False False False False  True \n",
      "False]\n",
      "[5 2 4 1 9 6 8 7 1 3]\n",
      "3 [False  True False  True False False False False  True \n",
      "False]\n",
      "[4 1 3 1 8 5 7 6 1 2]\n",
      "4 [False  True False  True False False False False  True  \n",
      "True]\n",
      "[3 1 2 1 7 4 6 5 1 1]\n",
      "5 [False  True  True  True False False False False  True  \n",
      "True]\n",
      "[2 1 1 1 6 3 5 4 1 1]\n",
      "6 [ True  True  True  True False False False False  True  \n",
      "True]\n",
      "[1 1 1 1 5 2 4 3 1 1]\n",
      "7 [ True  True  True  True False  True False False  True  \n",
      "True]\n",
      "[1 1 1 1 4 1 3 2 1 1]\n",
      "8 [ True  True  True  True False  True False  True  True  \n",
      "True]\n",
      "[1 1 1 1 3 1 2 1 1 1]\n",
      "9 [ True  True  True  True False  True  True  True  True  \n",
      "True]\n",
      "[1 1 1 1 2 1 1 1 1 1]\n",
      "10 [ True  True  True  True  True  True  True  True  True  \n",
      "True]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "We see that the result is very stable. Features that have been used when requesting \n",
      "smaller feature sets keep on getting selected when letting more features in. At last, \n",
      "we rely on our train/test set splitting to warn us when we go the wrong way.\n",
      "Other feature selection methods\n",
      "There are several other feature selection methods that you will discover while \n",
      "reading through machine learning literature. Some even don't look like being a \n",
      "feature selection method because they are embedded into the learning process (not \n",
      "to be confused with the aforementioned wrappers). Decision trees, for instance, have \n",
      "a feature selection mechanism implanted deep in their core. Other learning methods \n",
      "employ some kind of regularization that punishes model complexity, thus driving \n",
      "the learning process towards good performing models that are still \"simple\". They do \n",
      "this by decreasing the less impactful features importance to zero and then dropping \n",
      "them (L1-regularization).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 274, 'page_label': '254'}\n",
      "Dimensionality Reduction\n",
      "[  254 ]\n",
      "So watch out! Often, the power of machine learning methods has to be attributed to \n",
      "their implanted feature selection method to a great degree.\n",
      "Feature extraction\n",
      "At some point, after we have removed redundant features and dropped irrelevant \n",
      "ones, we, often, still find that we have too many features. No matter what learning \n",
      "method we use, they all perform badly and given the huge feature space we \n",
      "understand that they actually cannot do better. We realize that we have to cut living \n",
      "flesh; we have to get rid of features, for which all common sense tells us that they \n",
      "are valuable. Another situation when we need to reduce the dimensions and feature \n",
      "selection does not help much is when we want to visualize data. Then, we need to \n",
      "have at most three dimensions at the end to provide any meaningful graphs.\n",
      "Enter feature extraction methods. They restructure the feature space to make it more \n",
      "accessible to the model or simply cut down the dimensions to two or three so that we \n",
      "can show dependencies visually.\n",
      "Again, we can distinguish feature extraction methods as being linear or non-linear \n",
      "ones. Also, as seen before in the Selecting features section, we will present one method \n",
      "for each type (principal component analysis as a linear and non-linear version of \n",
      "multidimensional scaling). Although, they are widely known and used, they are only \n",
      "representatives for many more interesting and powerful feature extraction methods.\n",
      "About principal component analysis\n",
      "Principal component analysis (PCA) is often the first thing to try out if you want to \n",
      "cut down the number of features and do not know what feature extraction method \n",
      "to use. PCA is limited as it's a linear method, but chances are that it already goes far \n",
      "enough for your model to learn well enough. Add to this the strong mathematical \n",
      "properties it offers and the speed at which it finds the transformed feature space and \n",
      "is later able to transform between original and transformed features; we can almost \n",
      "guarantee that it also will become one of your frequently used machine learning tools.\n",
      "Summarizing it, given the original feature space, PCA finds a linear projection of \n",
      "itself in a lower dimensional space that has the following properties:\n",
      " The conserved variance is maximized.\n",
      " The final reconstruction error (when trying to go back from transformed \n",
      "features to original ones) is minimized.\n",
      "As PCA simply transforms the input data, it can be applied both to classification  \n",
      "and regression problems. In this section, we will use a classification task to discuss \n",
      "the method.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 275, 'page_label': '255'}\n",
      "Chapter 11\n",
      "[  255 ]\n",
      "Sketching PCA\n",
      "PCA involves a lot of linear algebra, which we do not want to go into. Nevertheless, \n",
      "the basic algorithm can be easily described as follows:\n",
      "1. Center the data by subtracting the mean from it.\n",
      "2. Calculate the covariance matrix.\n",
      "3. Calculate the eigenvectors of the covariance matrix.\n",
      "If we start with N  features, then the algorithm will return a transformed feature \n",
      "space again with N  dimensions (we gained nothing so far). The nice thing about  \n",
      "this algorithm, however, is that the eigenvalues indicate how much of the variance  \n",
      "is described by the corresponding eigenvector.\n",
      "Let's assume we start with 1000N =  features and we know that our model does not \n",
      "work well with more than 20  features. Then, we simply pick the 20  eigenvectors \n",
      "with the highest eigenvalues.\n",
      "Applying PCA\n",
      "Let's consider the following artificial dataset, which is visualized in the following left \n",
      "plot diagram:\n",
      ">>> x1 = np.arange(0, 10, .2)\n",
      ">>> x2 = x1+np.random.normal(loc=0, scale=1, size=len(x1))\n",
      ">>> X = np.c_[(x1, x2)]\n",
      ">>> good = (x1>5) | (x2>5) # some arbitrary classes\n",
      ">>> bad = ~good # to make the example look good\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 276, 'page_label': '256'}\n",
      "Dimensionality Reduction\n",
      "[  256 ]\n",
      "Scikit-learn provides the PCA class in its decomposition package. In this example, we \n",
      "can clearly see that one dimension should be enough to describe the data. We can \n",
      "specify this using the n_components parameter:\n",
      ">>> from sklearn import linear_model, decomposition, datasets\n",
      ">>> pca = decomposition.PCA(n_components=1)\n",
      "Also, here we can use the fit() and transform() methods of pca (or its fit_\n",
      "transform() combination) to analyze the data and project it in the transformed \n",
      "feature space:\n",
      ">>> Xtrans = pca.fit_transform(X)\n",
      "As we have specified, Xtrans contains only one dimension. You can see the result in \n",
      "the preceding right plot diagram. The outcome is even linearly separable in this case. \n",
      "We would not even need a complex classifier to distinguish between both classes.\n",
      "To get an understanding of the reconstruction error, we can have a look at the \n",
      "variance of the data that we have retained in the transformation:\n",
      ">>> print(pca.explained_variance_ratio_)\n",
      ">>> [ 0.96393127]\n",
      "This means that after going from two to one dimension, we are still left with  \n",
      "96 percent of the variance.\n",
      "Of course, it's not always this simple. Oftentimes, we don't know what number of \n",
      "dimensions is advisable upfront. In that case, we leave n_components parameter \n",
      "unspecified when initializing PCA to let it calculate the full transformation. After \n",
      "fitting the data, explained_variance_ratio_ contains an array of ratios in \n",
      "decreasing order: The first value is the ratio of the basis vector describing the \n",
      "direction of the highest variance, the second value is the ratio of the direction of the \n",
      "second highest variance, and so on. After plotting this array, we quickly get a feel \n",
      "of how many components we would need: the number of components immediately \n",
      "before the chart has its elbow is often a good guess.\n",
      "Plots displaying the explained variance over the number of \n",
      "components is called a Scree plot. A nice example of combining a Scree \n",
      "plot with a grid search to find the best setting for the classification \n",
      "problem can be found at http://scikit-learn.sourceforge.\n",
      "net/stable/auto_examples/plot_digits_pipe.html.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 277, 'page_label': '257'}\n",
      "Chapter 11\n",
      "[  257 ]\n",
      "Limitations of PCA and how LDA can help\n",
      "Being a linear method, PCA has, of course, its limitations when we are faced with \n",
      "data that has non-linear relationships. We won't go into details here, but it's sufficient \n",
      "to say that there are extensions of PCA, for example, Kernel PCA, which introduces a \n",
      "non-linear transformation so that we can still use the PCA approach.\n",
      "Another interesting weakness of PCA, which we will cover here, is when it's being \n",
      "applied to special classification problems. Let's replace good = (x1 > 5) | (x2 > 5) \n",
      "with good = x1 > x2 to simulate such a special case and we quickly see the problem:\n",
      "Here, the classes are not distributed according to the axis with the highest variance, \n",
      "but the second highest variance. Clearly, PCA falls flat on its face. As we don't \n",
      "provide PCA with any cues regarding the class labels, it cannot do any better.\n",
      "Linear Discriminant Analysis (LDA) comes to the rescue here. It's a method \n",
      "that tries to maximize the distance of points belonging to different classes, while \n",
      "minimizing the distances of points of the same class. We won't give any more details \n",
      "regarding how in particular the underlying theory works, just a quick tutorial on \n",
      "how to use it:\n",
      ">>> from sklearn import lda\n",
      ">>> lda_inst = lda.LDA(n_components=1)\n",
      ">>> Xtrans = lda_inst.fit_transform(X, good)\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 278, 'page_label': '258'}\n",
      "Dimensionality Reduction\n",
      "[  258 ]\n",
      "That's all. Note that in contrast to the previous PCA example, we provide the class \n",
      "labels to the fit_transform() method. Thus, PCA is an unsupervised feature \n",
      "extraction method, whereas LDA is a supervised one. The result looks as expected:\n",
      "So, why then consider PCA at all and not simply use LDA? Well, it's not that simple. \n",
      "With an increasing number of classes and fewer samples per class, LDA does not \n",
      "look that well any more. Also, PCA seems to be not as sensitive to different training \n",
      "sets as LDA. So, when we have to advise which method to use, we can only suggest a \n",
      "clear \"it depends\".\n",
      "Multidimensional scaling\n",
      "Although, PCA tries to use optimization for retained variance, multidimensional \n",
      "scaling (MDS) tries to retain the relative distances as much as possible when \n",
      "reducing the dimensions. This is useful when we have a high-dimensional  \n",
      "dataset and want to get a visual impression.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 279, 'page_label': '259'}\n",
      "Chapter 11\n",
      "[  259 ]\n",
      "MDS does not care about the data points themselves; instead, it's interested in the \n",
      "dissimilarities between pairs of data points and interprets these as distances. The \n",
      "first thing the MDS algorithm is doing is, therefore, taking all the N  datapoints of \n",
      "dimension k  and calculates a distance matrix using a distance function od , which \n",
      "measures the (most of the time, Euclidean) distance in the original feature space:\n",
      "( ) ( )\n",
      "( ) ( )\n",
      "11 1 1 1 1\n",
      "1 1\n",
      ", ,\n",
      ", ,\n",
      "N o o N\n",
      "k Nk o N o N N\n",
      "X X d X X d X X\n",
      "X X d X X d X X\n",
      "  \n",
      "            \n",
      "/midhorizellipsis/midhorizellipsis\n",
      "/vertellipsis /downslopeellipsis/vertellipsis/vertellipsis/downslopeellipsis /vertellipsis\n",
      "/midhorizellipsis/midhorizellipsis\n",
      "Now, MDS tries to position the individual datapoints in the lower dimensional \n",
      "space such that the new distance there resembles the distances in the original space \n",
      "as much as possible. As MDS is often used for visualization, the choice of the lower \n",
      "dimension is most of the time two or three.\n",
      "Let's have a look at the following simple data consisting of three datapoints in  \n",
      "five-dimensional space. Two of the datapoints are close by and one is very distinct  \n",
      "and we want to visualize this in three and two dimensions:\n",
      ">>> X = np.c_[np.ones(5), 2 * np.ones(5), 10 * np.ones(5)].T\n",
      ">>> print(X)\n",
      "[[  1.   1.   1.   1.   1.]\n",
      " [  2.   2.   2.   2.   2.]\n",
      " [ 10.  10.  10.  10.  10.]]\n",
      "Using the MDS class in scikit-learn's manifold package, we first specify that we want \n",
      "to transform X into a three-dimensional Euclidean space:\n",
      ">>> from sklearn import manifold\n",
      ">>> mds = manifold.MDS(n_components=3)\n",
      ">>> Xtrans = mds.fit_transform(X)\n",
      "To visualize it in two dimensions, we would need to set n_components accordingly.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 280, 'page_label': '260'}\n",
      "Dimensionality Reduction\n",
      "[  260 ]\n",
      "The results can be seen in the following two graphs. The triangle and circle are both \n",
      "close together, whereas the star is far away:\n",
      "Let's have a look at the slightly more complex Iris dataset. We will use it later to \n",
      "contrast LDA with PCA. The Iris dataset contains four attributes per flower. With \n",
      "the preceding code, we would project it into three-dimensional space while keeping \n",
      "the relative distances between the individual flowers as much as possible. In the \n",
      "previous example, we did not specify any metric, so MDS will default to Euclidean. \n",
      "This means that flowers that were \"different\" according to their four attributes \n",
      "should also be far away in the MDS-scaled three-dimensional space and flowers that \n",
      "were similar should be near together now, as shown in the following diagram:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 281, 'page_label': '261'}\n",
      "Chapter 11\n",
      "[  261 ]\n",
      "Reducing the dimensional reduction to three and two dimensions with PCA instead, \n",
      "we see the expected bigger spread of the flowers belonging to the same class, as \n",
      "shown in the following diagram:\n",
      "Of course, using MDS requires an understanding of the individual feature's units; \n",
      "maybe we are using features that cannot be compared using the Euclidean metric. \n",
      "For instance, a categorical variable, even when encoded as an integer (0= circle,  \n",
      "1= star, 2= triangle, and so on), cannot be compared using Euclidean (is circle  \n",
      "closer to star than to triangle?).\n",
      "However, once we are aware of this issue, MDS is a useful tool that reveals similarities \n",
      "in our data that otherwise would be difficult to see in the original feature space.\n",
      "Looking a bit deeper into MDS, we realize that it's not a single algorithm, but rather \n",
      "a family of different algorithms, of which we have used just one. The same was true \n",
      "for PCA. Also, in case you realize that neither PCA nor MDS solves your problem, \n",
      "just look at the many other manifold learning algorithms that are available in the \n",
      "scikit-learn toolkit.\n",
      "However, before you get overwhelmed by the many different algorithms, it's always \n",
      "best to start with the simplest one and see how far you get with it. Then, take the \n",
      "next more complex one and continue from there.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 282, 'page_label': '262'}\n",
      "Dimensionality Reduction\n",
      "[  262 ]\n",
      "Summary\n",
      "You learned that sometimes you can get rid of complete features using feature \n",
      "selection methods. We also saw that in some cases, this is not enough and we have \n",
      "to employ feature extraction methods that reveal the real and the lower-dimensional \n",
      "structure in our data, hoping that the model has an easier game with it.\n",
      "For sure, we only scratched the surface of the huge body of available dimensionality \n",
      "reduction methods. Still, we hope that we got you interested in this whole field, as \n",
      "there are lots of other methods waiting for you to be picked up. At the end, feature \n",
      "selection and extraction is an art, just like choosing the right learning method or \n",
      "training model.\n",
      "The next chapter covers the use of Jug, a little Python framework to manage \n",
      "computations in a way that takes advantage of multiple cores or multiple machines. \n",
      "You will also learn about AWS, the Amazon Cloud.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 283, 'page_label': '263'}\n",
      "[ 263 ]\n",
      "Bigger Data\n",
      "It's not easy to say what big data is. We will adopt an operational definition: when \n",
      "data is so large that it becomes cumbersome to work with, we will talk about big \n",
      "data. In some areas, this might mean petabytes of data or trillions of transactions: \n",
      "data which will not fit into a single hard drive. In other cases, it may be one hundred \n",
      "times smaller, but still difficult to work with.\n",
      "Why has data itself become an issue? While computers keep getting faster and have \n",
      "more memory, the size of the data has grown as well. In fact, data has grown faster \n",
      "than computational speed and few algorithms scale linearly with the size of the  \n",
      "input datataken together, this means that data has grown faster than our ability  \n",
      "to process it.\n",
      "We will first build on some of the experience of the previous chapters and work with \n",
      "what we can call medium data setting (not quite big data, but not small either). For \n",
      "this, we will use a package called jug, which allows us to perform the following tasks:\n",
      " Break up your pipeline into tasks\n",
      " Cache (memoize) intermediate results\n",
      " Make use of multiple cores, including multiple computers on a grid\n",
      "The next step is to move to true big data and we will see how to use the cloud  \n",
      "for computation purpose. In particular, you will learn about the Amazon Web \n",
      "Services infrastructure. In this section, we introduce another Python package  \n",
      "called StarCluster to manage clusters.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 284, 'page_label': '264'}\n",
      "Bigger Data\n",
      "[  264 ]\n",
      "Learning about big data\n",
      "The expression \"big data\" does not mean a specific amount of data, neither in the \n",
      "number of examples nor in the number of gigabytes, terabytes, or petabytes occupied \n",
      "by the data. It means that data has been growing faster than processing power. This \n",
      "implies the following:\n",
      " Some of the methods and techniques that worked well in the past now  \n",
      "need to be redone or replaced as they do not scale well to the new size  \n",
      "of the input data\n",
      " Algorithms cannot assume that all the input data can fit in RAM\n",
      " Managing data becomes a major task in itself\n",
      " Using computer clusters or multicore machines becomes a necessity and not \n",
      "a luxury\n",
      "This chapter will focus on this last piece of the puzzle: how to use multiple cores \n",
      "(either on the same machine or on separate machines) to speed up and organize  \n",
      "your computations. This will also be useful in other medium-sized data tasks.\n",
      "Using jug to break up your pipeline into tasks\n",
      "Often, we have a simple pipeline: we preprocess the initial data, compute features, \n",
      "and then call a machine learning algorithm with the resulting features.\n",
      "Jug is a package developed by Luis Pedro Coelho, one of the authors of this book. It's \n",
      "open source (using the liberal MIT License) and can be useful in many areas, but was \n",
      "designed specifically around data analysis problems. It simultaneously solves several \n",
      "problems, for example:\n",
      " It can memoize results to disk (or a database), which means that if you ask it to \n",
      "compute something you have already computed before, the result is instead \n",
      "read from disk.\n",
      " It can use multiple cores or even multiple computers on a cluster. Jug was \n",
      "also designed to work very well in batch computing environments, which \n",
      "use queuing systems such as PBS (Portable Batch System), LSF (Load \n",
      "Sharing Facility), or Grid Engine. This will be used in the second half  \n",
      "of the chapter as we build online clusters and dispatch jobs to them.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 285, 'page_label': '265'}\n",
      "Chapter 12\n",
      "[  265 ]\n",
      "An introduction to tasks in jug\n",
      "Tasks are the basic building block of jug. A task is composed of a function and values \n",
      "for its arguments. Consider this simple example:\n",
      "def double(x):\n",
      "    return 2*x\n",
      "In this chapter, the code examples will generally have to be typed in script files. \n",
      "Thus, they will not be shown with the >>> marker. Commands that should be  \n",
      "typed at the shell will be indicated by preceding them with $.\n",
      "A task could be \"call double with argument 3\". Another task would be \"call double \n",
      "with argument 642.34\". Using jug, we can build these tasks as follows:\n",
      "from jug import Task\n",
      "t1 = Task(double, 3)\n",
      "t2 = Task(double, 642.34)\n",
      "Save this to a file called jugfile.py (which is just a regular Python file). Now, we \n",
      "can run jug execute to run the tasks. This is something you type on the command \n",
      "line, not at the Python prompt, so we show it marked with a dollar sign ($):\n",
      "$ jug execute\n",
      "You will also get some feedback on the tasks (jug will say that two tasks named \n",
      "double were run). Run jug execute again and it will tell you that it did nothing! \n",
      "It does not need to. In this case, we gained little, but if the tasks took a long time to \n",
      "compute, it would have been very useful.\n",
      "You may notice that a new directory also appeared on your hard drive named \n",
      "jugfile.jugdata with a few weirdly named files. This is the memoization cache.  \n",
      "If you remove it, jug execute will run all your tasks again.\n",
      "Often, it's good to distinguish between pure functions, which simply take their \n",
      "inputs and return a result, from more general functions that can perform actions \n",
      "(such as reading from files, writing to files, accessing global variables, modify their \n",
      "arguments, or anything that the language allows). Some programming languages, \n",
      "such as Haskell, even have syntactic ways to distinguish pure from impure functions.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 286, 'page_label': '266'}\n",
      "Bigger Data\n",
      "[  266 ]\n",
      "With jug, your tasks do not need to be perfectly pure. It's even recommended that \n",
      "you use tasks to read in your data or write out your results. However, accessing \n",
      "and modifying global variables will not work well: the tasks may be run in any \n",
      "order in different processors. The exceptions are global constants, but even this may \n",
      "confuse the memoization system (if the value is changed between runs). Similarly, \n",
      "you should not modify the input values. Jug has a debug mode (use jug execute \n",
      "--debug), which slows down your computation, but will give you useful error \n",
      "messages if you make this sort of mistake.\n",
      "The preceding code works, but is a bit cumbersome. You are always repeating the \n",
      "Task(function, argument) construct. Using a bit of Python magic, we can make \n",
      "the code even more natural as follows:\n",
      "from jug import TaskGenerator\n",
      "from time import sleep\n",
      "@TaskGenerator\n",
      "def double(x):\n",
      "    sleep(4)\n",
      "    return 2*x\n",
      "@TaskGenerator\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "@TaskGenerator\n",
      "def print_final_result(oname, value):\n",
      "    with open(oname, 'w') as output:\n",
      "        output.write('Final result: {}\\n'.format(value))\n",
      "y = double(2)\n",
      "z = double(y)\n",
      "y2 = double(7)\n",
      "z2 = double(y2)\n",
      "print_final_result('output.txt', add(z,z2))\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 287, 'page_label': '267'}\n",
      "Chapter 12\n",
      "[  267 ]\n",
      "Except for the use of TaskGenerator, the preceding code could be a standard Python \n",
      "file! However, using TaskGenerator, it actually creates a series of tasks and it is now \n",
      "possible to run it in a way that takes advantage of multiple processors. Behind the \n",
      "scenes, the decorator transforms your functions so that they do not actually execute \n",
      "when called, but create a Task object. We also take advantage of the fact that we can \n",
      "pass tasks to other tasks and this results in a dependency being generated.\n",
      "You may have noticed that we added a few sleep(4) calls in the preceding code. \n",
      "This simulates running a long computation. Otherwise, this example is so fast that \n",
      "there is no point in using multiple processors.\n",
      "We start by running jug status, which results in the output shown in the  \n",
      "following screenshot:\n",
      "Now, we start two processes simultaneously (using the & operator in the background):\n",
      "$ jug execute &\n",
      "$ jug execute &\n",
      "Now, we run jug status again:\n",
      "We can see that the two initial double operators are running at the same time. After \n",
      "about 8 seconds, the whole process will finish and the output.txt file will be written.\n",
      "By the way, if your file was called anything other than jugfile.py, you would then \n",
      "have to specify it explicitly on the command line. For example, if your file was called \n",
      "analysis.py, you would run the following command:\n",
      "$ jug execute analysis.py\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 288, 'page_label': '268'}\n",
      "Bigger Data\n",
      "[  268 ]\n",
      "This is the only disadvantage of not using the name jugfile.py. So, feel free to use \n",
      "more meaningful names.\n",
      "Looking under the hood\n",
      "How does jug work? At the basic level, it's very simple. A Task is a function plus \n",
      "its argument. Its arguments may be either values or other tasks. If a task takes other \n",
      "tasks, there is a dependency between the two tasks (and the second one cannot be \n",
      "run until the results of the first task are available).\n",
      "Based on this, jug recursively computes a hash for each task. This hash value encodes \n",
      "the whole computation to get the result. When you run jug execute, for each task, \n",
      "there is a little loop that runs the logic depicted in the following flowchart:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 289, 'page_label': '269'}\n",
      "Chapter 12\n",
      "[  269 ]\n",
      "The default backend writes the file to disk (in this funny directory named jugfile.\n",
      "jugdata/). Another backend is available, which uses a Redis database. With proper \n",
      "locking, which jug takes care of, this also allows for many processors to execute \n",
      "tasks; each process will independently look at all the tasks and run the ones that have \n",
      "not run yet and then write them back to the shared backend. This works on either the \n",
      "same machine (using multicore processors) or in multiple machines as long as they \n",
      "all have access to the same backend (for example, using a network disk or the Redis \n",
      "databases). In the second half of this chapter, we will discuss computer clusters, but \n",
      "for now let's focus on multiple cores.\n",
      "You can also understand why it's able to memoize intermediate results. If the \n",
      "backend already has the result of a task, it's not run again. On the other hand, if you \n",
      "change the task, even in minute ways (by altering one of the parameters), its hash \n",
      "will change. Therefore, it will be rerun. Furthermore, all tasks that depend on it will \n",
      "also have their hashes changed and they will be rerun as well.\n",
      "Using jug for data analysis\n",
      "Jug is a generic framework, but it's ideally suited for medium-scale data analysis. \n",
      "As you develop your analysis pipeline, it's good to have intermediate results \n",
      "automatically saved. If you have already computed the preprocessing step before \n",
      "and are only changing the features you compute, you do not want to recompute \n",
      "the preprocessing step. If you have already computed the features, but want to try \n",
      "combining a few new ones into the mix, you also do not want to recompute all your \n",
      "other features.\n",
      "Jug is also specifically optimized to work with NumPy arrays. Whenever your tasks \n",
      "return or receive NumPy arrays, you are taking advantage of this optimization. Jug \n",
      "is another piece of this ecosystem where everything works together.\n",
      "We will now look back at Chapter 10, Computer Vision. In that chapter, we learned \n",
      "how to compute features on images. Remember that the basic pipeline consisted of \n",
      "the following features:\n",
      " Loading image files\n",
      " Computing features\n",
      " Combining these features\n",
      " Normalizing the features\n",
      " Creating a classifier\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 290, 'page_label': '270'}\n",
      "Bigger Data\n",
      "[  270 ]\n",
      "We are going to redo this exercise, but this time with the use of jug. The advantage of \n",
      "this version is that it's now possible to add a new feature or classifier without having \n",
      "to recompute all of the pipeline.\n",
      "We start with a few imports as follows:\n",
      "from jug import TaskGenerator\n",
      "import mahotas as mh\n",
      "from glob import glob\n",
      "Now, we define the first task generators and feature computation functions:\n",
      "@TaskGenerator\n",
      "def compute_texture(im):\n",
      "    from features import texture\n",
      "    imc = mh.imread(im)\n",
      "    return texture(mh.colors.rgb2gray(imc))\n",
      "@TaskGenerator\n",
      "def chist_file(fname):\n",
      "    from features import chist\n",
      "    im = mh.imread(fname)\n",
      "    return chist(im)\n",
      "The features module we import is the one from Chapter 10, Computer Vision.\n",
      "We write functions that take the filename as input instead of the image \n",
      "array. Using the full images would also work, of course, but this is a small \n",
      "optimization. A filename is a string, which is small if it gets written to the \n",
      "backend. It's also very fast to compute a hash if needed. It also ensures \n",
      "that the images are only loaded by the processes that need them.\n",
      "We can use TaskGenerator on any function. This is true even for functions, which \n",
      "we did not write, such as np.array, np.hstack, or the following command:\n",
      "import numpy as np\n",
      "to_array = TaskGenerator(np.array)\n",
      "hstack = TaskGenerator(np.hstack)\n",
      "haralicks = []\n",
      "chists = []\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 291, 'page_label': '271'}\n",
      "Chapter 12\n",
      "[  271 ]\n",
      "labels = []\n",
      "# Change this variable to point to\n",
      "# the location of the dataset on disk\n",
      "basedir = '../SimpleImageDataset/'\n",
      "# Use glob to get all the images\n",
      "images = glob('{}/*.jpg'.format(basedir))\n",
      "for fname in sorted(images):\n",
      "    haralicks.append(compute_texture(fname))\n",
      "    chists.append(chist_file(fname))\n",
      "    # The class is encoded in the filename as xxxx00.jpg\n",
      "    labels.append(fname[:-len('00.jpg')])\n",
      "haralicks = to_array(haralicks)\n",
      "chists = to_array(chists)\n",
      "labels = to_array(labels)\n",
      "One small inconvenience of using jug is that we must always write functions to \n",
      "output the results to files, as shown in the preceding examples. This is a small  \n",
      "price to pay for the extra convenience of using jug.\n",
      "@TaskGenerator\n",
      "def accuracy(features, labels):\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.pipeline import Pipeline\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "    from sklearn import cross_validation\n",
      "    clf = Pipeline([('preproc', StandardScaler()),\n",
      "                ('classifier', LogisticRegression())])\n",
      "    cv = cross_validation.LeaveOneOut(len(features))\n",
      "    scores = cross_validation.cross_val_score(\n",
      "        clf, features, labels, cv=cv)\n",
      "    return scores.mean()\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 292, 'page_label': '272'}\n",
      "Bigger Data\n",
      "[  272 ]\n",
      "Note that we are only importing sklearn inside this function. This is a small \n",
      "optimization. This way, sklearn is only imported when it's really needed:\n",
      "scores_base = accuracy(haralicks, labels)\n",
      "scores_chist = accuracy(chists, labels)\n",
      "combined = hstack([chists, haralicks])\n",
      "scores_combined  = accuracy(combined, labels)\n",
      "Finally, we write and call a function to print out all results. It expects its argument to \n",
      "be a list of pairs with the name of the algorithm and the results:\n",
      "@TaskGenerator\n",
      "def print_results(scores):\n",
      "    with open('results.image.txt', 'w') as output:\n",
      "        for k,v in scores:\n",
      "            output.write('Accuracy [{}]: {:.1%}\\n'.format(\n",
      "                k, v.mean()))\n",
      "print_results([\n",
      "        ('base', scores_base),\n",
      "        ('chists', scores_chist),\n",
      "        ('combined' , scores_combined),\n",
      "        ])\n",
      "That's it. Now, on the shell, run the following command to run this pipeline using jug:\n",
      "$ jug execute image-classification.py\n",
      "Reusing partial results\n",
      "For example, let's say you want to add a new feature (or even a set of features). As \n",
      "we saw in Chapter 10, Computer Vision, this is easy to do by changing the feature \n",
      "computation code. However, this would imply recomputing all the features again, \n",
      "which is wasteful, particularly, if you want to test new features and techniques quickly.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 293, 'page_label': '273'}\n",
      "Chapter 12\n",
      "[  273 ]\n",
      "We now add a set of features, that is, another type of texture feature called linear \n",
      "binary patterns. This is implemented in mahotas; we just need to call a function,  \n",
      "but we wrap it in TaskGenerator:\n",
      "@TaskGenerator\n",
      "def compute_lbp(fname):\n",
      "    from mahotas.features import lbp\n",
      "    imc = mh.imread(fname)\n",
      "    im = mh.colors.rgb2grey(imc)\n",
      "    # The parameters 'radius' and 'points' are set to typical values\n",
      "    # check the documentation for their exact meaning\n",
      "    return lbp(im, radius=8, points=6)\n",
      "We replace the previous loop to have an extra function call:\n",
      "lbps = []\n",
      "for fname in sorted(images):\n",
      "    # the rest of the loop as before\n",
      "    lbps.append(compute_lbp(fname))\n",
      "lbps = to_array(lbps)\n",
      "We call accuracy with these newer features:\n",
      "scores_lbps = accuracy(lbps, labels)\n",
      "combined_all = hstack([chists, haralicks, lbps])\n",
      "scores_combined_all = accuracy(combined_all, labels)\n",
      "print_results([\n",
      "        ('base', scores_base),\n",
      "        ('chists', scores_chist),\n",
      "        ('lbps', scores_lbps),\n",
      "        ('combined' , scores_combined),\n",
      "        ('combined_all' , scores_combined_all),\n",
      "        ])\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 294, 'page_label': '274'}\n",
      "Bigger Data\n",
      "[  274 ]\n",
      "Now, when you run jug execute again, the new features will be computed, \n",
      "but the old features will be loaded from the cache. This is when jug can be very \n",
      "powerful. It ensures that you always get the results you want while saving you from \n",
      "unnecessarily recomputing cached results. You will also see that adding this feature \n",
      "set improves on the previous methods.\n",
      "Not all features of jug could be mentioned in this chapter, but here is a summary of \n",
      "the most potentially interesting ones we didn't cover in the main text:\n",
      " jug invalidate: This declares that all results from a given function should \n",
      "be considered invalid and in need of recomputation. This will also recompute \n",
      "any downstream computation, which depended (even indirectly) on the \n",
      "invalidated results.\n",
      " jug status --cache: If jug status takes too long, you can use the \n",
      "--cache flag to cache the status and speed it up. Note that this will not  \n",
      "detect any changes to the jugfile, but you can always use --cache --clear \n",
      "to remove the cache and start again.\n",
      " jug cleanup: This removes any extra files in the memoization cache. This is \n",
      "a garbage collection operation.\n",
      "There are other, more advanced features, which allow you to \n",
      "look at values that have been computed inside the jugfile. Read \n",
      "up on features such as barriers in the jug documentation online \n",
      "at http://jug.rtfd.org.\n",
      "Using Amazon Web Services\n",
      "When you have a lot of data and a lot of computation to be performed, you might start \n",
      "to crave more computing power. Amazon (http://aws.amazon.com) allows you to \n",
      "rent computing power by the hour. Thus, you can access a large amount of computing \n",
      "power without having to precommit by purchasing a large number of machines \n",
      "(including the costs of managing the infrastructure). There are other competitors in this \n",
      "market, but Amazon is the largest player, so we briefly cover it here.\n",
      "Amazon Web Services (AWS) is a large set of services. We will focus only on the \n",
      "Elastic Compute Cloud (EC2) service. This service offers you virtual machines and \n",
      "disk space, which can be allocated and deallocated quickly.\n",
      "There are three modes of use. First is a reserved mode, whereby you prepay to have \n",
      "cheaper per-hour access, a fixed per-hour rate, and a variable rate, which depends on \n",
      "the overall compute market (when there is less demand, the costs are lower; when \n",
      "there is more demand, the prices go up).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 295, 'page_label': '275'}\n",
      "Chapter 12\n",
      "[  275 ]\n",
      "On top of this general system, there are several types of machines available with \n",
      "varying costs, from a single core to a multicore system with a lot of RAM or even \n",
      "graphical processing units (GPUs). We will later see that you can also get several of \n",
      "the cheaper machines and build yourself a virtual cluster. You can also choose to get \n",
      "a Linux or Windows server (with Linux being slightly cheaper). In this chapter, we \n",
      "will work on our examples on Linux, but most of this information will be valid for \n",
      "Windows machines as well.\n",
      "For testing, you can use a single machine in the free tier. This allows you to play \n",
      "around with the system, get used to the interface, and so on. Note, though, that this \n",
      "machine contains a slow CPU.\n",
      "The resources can be managed through a web interface. However, it's also possible \n",
      "to do so programmatically and to write scripts that allocate virtual machines, \n",
      "format hard disks, and perform all operations that are possible through the web \n",
      "interface. In fact, while the web interface changes very frequently (and some of the \n",
      "screenshots we show in the book may be out of date by the time it goes to press), \n",
      "the programmatic interface is more stable and the general architecture has remained \n",
      "stable since the service was introduced.\n",
      "Access to AWS services is performed through a traditional username/password \n",
      "system, although Amazon calls the username an access key and the password a secret \n",
      "key. They probably do so to keep it separate from the username/password you \n",
      "use to access the web interface. In fact, you can create as many access/secret key \n",
      "pairs as you wish and give them different permissions. This is helpful for a larger \n",
      "team where a senior user with access to the full web panel can create other keys for \n",
      "developers with fewer privileges.\n",
      "Amazon.com has several regions. These correspond to physical \n",
      "regions of the world: West coast US, East coast US, several Asian \n",
      "locations, a South American one, and two European ones. If you will \n",
      "be transferring data, it's best to keep it close to where you will be \n",
      "transferring to and from. Additionally, keep in mind that if you are \n",
      "handling user information, there may be regulatory issues regulating \n",
      "their transfer to another jurisdiction. In this case, do check with an \n",
      "informed counsel on the implications of transferring data about \n",
      "European customers to the US or any other similar transfer.\n",
      "Amazon Web Services is a very large topic and there are various books  \n",
      "exclusively available to cover AWS. The purpose of this chapter is to give you \n",
      "an overall impression of what is available and what is possible with AWS. In the \n",
      "practical spirit of this book, we do this by working through examples, but we will \n",
      "not exhaust all possibilities.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 296, 'page_label': '276'}\n",
      "Bigger Data\n",
      "[  276 ]\n",
      "Creating your first virtual machines\n",
      "The first step is to go to http://aws.amazon.com/ and create an account. These \n",
      "steps are similar to any other online service. A single machine is free, but to get more, \n",
      "you will need a credit card. In this example, we will use a few machines, so it may \n",
      "cost you a few dollars if you want to run through it. If you are not ready to take out \n",
      "a credit card just yet, you can certainly read the chapter to learn what AWS provides \n",
      "without going through the examples. Then you can make a more informed decision \n",
      "on whether to sign up.\n",
      "Once you sign up for AWS and log in, you will be taken to the console. Here, you will \n",
      "see the many services that AWS provides, as depicted in the following screenshot:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 297, 'page_label': '277'}\n",
      "Chapter 12\n",
      "[  277 ]\n",
      "We pick and click on EC2 (the top element on the leftmost columnthis is the panel \n",
      "shown as it was when this book was written. Amazon regularly makes minor changes, \n",
      "so you may see something slightly different from what we present in the book). We \n",
      "now see the EC2 management console, as shown in the following screenshot:\n",
      "In the top-right corner, you can pick your region (see the Amazon regions \n",
      "information box). Note that you will only see information about the region that you \n",
      "have selected at the moment. Thus, if you mistakenly select the wrong region (or have \n",
      "machines running in multiple regions), your machines may not appear (this seems to \n",
      "be a common pitfall of using the EC2 web management console).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 298, 'page_label': '278'}\n",
      "Bigger Data\n",
      "[  278 ]\n",
      "In EC2 parlance, a running server is called an instance. We select Launch Instance, \n",
      "which leads to the following screen asking us to select the operating system to use:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 299, 'page_label': '279'}\n",
      "Chapter 12\n",
      "[  279 ]\n",
      "Select the Amazon Linux option (if you are familiar with one of the other offered \n",
      "Linux distributions, such as Red Hat, SUSE, or Ubuntu, you can also select one of \n",
      "these, but the configurations will be slightly different). Now that you have selected \n",
      "the software, you will need to select the hardware. In the next screen, you will be \n",
      "asked to select which type of machine to use:\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 300, 'page_label': '280'}\n",
      "Bigger Data\n",
      "[  280 ]\n",
      "We will start with one instance of the t2.micro type (the t1.micro type was an older, \n",
      "even less powerful machine). This is the smallest possible machine and it's free. \n",
      "Keep clicking on Next and accept all of the defaults until you come to the screen \n",
      "mentioning a key pair:\n",
      "We will pick the name awskeys for the key pair. Then check Create a new key \n",
      "pair. Name the key pair file awskeys.pem. Download and save this file somewhere \n",
      "safe! This is the SSH (Secure Shell) key that will enable you to log in to your cloud \n",
      "machine. Accept the remaining defaults and your instance will launch.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 301, 'page_label': '281'}\n",
      "Chapter 12\n",
      "[  281 ]\n",
      "You will now need to wait a few minutes for your instance to come up. Eventually, \n",
      "the instance will be shown in green with the status as running:\n",
      "In the preceding screenshot, you should see the Public IP which can be used to log in \n",
      "to the instance as follows:\n",
      "$ ssh -i awskeys.pem ec2-user@54.93.165.5\n",
      "Therefore, we will be calling the ssh command and passing it the key files that we \n",
      "downloaded earlier as the identity (using the -i option). We are logging in as user \n",
      "ec2-user at the machine with the IP address as 54.93.165.5. This address will, of \n",
      "course, be different in your case. If you choose another distribution for your instance, \n",
      "the username may also change. In this case, try logging in as root, ubuntu (for \n",
      "Ubuntu distribution), or fedora (for fedora distribution).\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 302, 'page_label': '282'}\n",
      "Bigger Data\n",
      "[  282 ]\n",
      "Finally, if you are running a Unix-style operating system (including Mac OS), you \n",
      "may have to tweak its permissions by calling the following command:\n",
      "$ chmod 600 awskeys.pem\n",
      "This sets the read/write permission for the current user only. SSH will otherwise \n",
      "give you an ugly warning.\n",
      "Now you should be able to log in to your machine. If everything is okay, you should \n",
      "see the banner, as shown in the following screenshot:\n",
      "This is a regular Linux box where you have sudo permission: you can run any \n",
      "command as the superuser by prefixing it with sudo. You can run the update \n",
      "command it recommends to get your machine up to speed.\n",
      "Installing Python packages on Amazon Linux\n",
      "If you prefer another distribution, you can use your knowledge of that distribution \n",
      "to install Python, NumPy, and others. Here, we will do it on the standard Amazon \n",
      "distribution. We start by installing several basic Python packages as follows:\n",
      "$ sudo yum -y install python-devel \\\n",
      "    python-pip numpy scipy python-matplotlib\n",
      "To compile mahotas, we will also need a C++ compiler:\n",
      "$ sudo yum -y install gcc-c++\n",
      "Finally, we install git to make sure that we can get the latest version of the code for \n",
      "the book:\n",
      "$ sudo yum -y install git\n",
      "In this system, pip is installed as python-pip. For convenience, we will use pip to \n",
      "upgrade itself. We will then use pip to install the necessary packages as follows:\n",
      "$ sudo pip-python install -U pip\n",
      "$ sudo pip install scikit-learn jug mahotas\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 303, 'page_label': '283'}\n",
      "Chapter 12\n",
      "[  283 ]\n",
      "At this point, you can install any other package you wish using pip.\n",
      "Running jug on our cloud machine\n",
      "We can now download the data and code for the book using this sequence  \n",
      "of commands:\n",
      "$ git clone \\  \n",
      "https://github.com/luispedro/BuildingMachineLearningSystemsWithPython\n",
      "$ cd BuildingMachineLearningSystemsWithPython\n",
      "$ cd ch12\n",
      "Finally, we run this following command:\n",
      "$ jug execute\n",
      "This would work just fine, but we would have to wait a long time for the results. Our \n",
      "free tier machine (of type t2.micro) is not very fast and only has a single processor. \n",
      "So, we will upgrade our machine!\n",
      "We go back to the EC2 console, and right-click on the running instance to get  \n",
      "the pop-up menu. We need to first stop the instance. This is the virtual machine \n",
      "equivalent to powering off. You can stop your machines at any time. At this point, \n",
      "you stop paying for them. Note that you are still using disk space, which also has a \n",
      "cost, billed separately. You can terminate the instance, which will also destroy the \n",
      "disk. This loses any information saved on the machine.\n",
      "Once the machine is stopped, the Change instance type option becomes available. \n",
      "Now, we can select a more powerful instance, for example, a c1.xlarge instance \n",
      "with eight cores. The machine is still off, so you need to start it again (the virtual \n",
      "equivalent to booting up).\n",
      "AWS offers several instance types at different price points. As this \n",
      "information is constantly being revised as more powerful options are \n",
      "introduced and prices change (generally, getting cheaper), we cannot \n",
      "give you many details in the book, but you can find the most up-to-date \n",
      "information on Amazon's website.\n",
      "We need to wait for the instance to come back up. Once it has, look up its IP address \n",
      "in the same fashion as we did before. When you change instance types, your instance \n",
      "will get a new address assigned to it.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 304, 'page_label': '284'}\n",
      "Bigger Data\n",
      "[  284 ]\n",
      "You can assign a fixed IP to an instance using Amazon.com's Elastic \n",
      "IPs functionality, which you will find on the left-hand side of the EC2 \n",
      "console. This is useful if you find yourself creating and modifying \n",
      "instances very often. There is a small cost associated with this feature.\n",
      "With eight cores, you can run eight jug processes simultaneously, as illustrated in the \n",
      "following code:\n",
      "$ # the loop below runs 8 times\n",
      "$ for counter in $(seq 8); do\n",
      ">     jug execute &\n",
      "> done\n",
      "Use jug status to check whether these eight jobs are, in fact, running. After your \n",
      "jobs are finished (which should now happen pretty fast), you can stop the machine \n",
      "and downgrade it again to a t2.micro instance to save money. The micro instance  \n",
      "can be used for free (within certain limits), while the c1.xlarge one we used costs \n",
      "0.064 US dollars per hour (as of February 2015check the AWS website for  \n",
      "up-to-date information).\n",
      "Automating the generation of clusters with StarCluster\n",
      "As we just learned, we can spawn machines using the web interface, but it quickly \n",
      "becomes tedious and error prone. Fortunately, Amazon has an API. This means \n",
      "that we can write scripts, which perform all the operations we discussed earlier, \n",
      "automatically. Even better, others have already developed tools that can be used to \n",
      "mechanize and automate many of the processes you want to perform with AWS.\n",
      "A group at MIT developed exactly such a tool called StarCluster. It happens to be a \n",
      "Python package, so you can install it with Python tools as follows:\n",
      "$ sudo pip install starcluster\n",
      "You can run this from an Amazon machine or from your local machine. Either option \n",
      "will work.\n",
      "We will need to specify what our cluster will look like. We do so by editing a \n",
      "configuration file. We generate a template configuration file by running the  \n",
      "following command:\n",
      "$ starcluster help\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 305, 'page_label': '285'}\n",
      "Chapter 12\n",
      "[  285 ]\n",
      "Then pick the option of generating the configuration file in ~/.starcluster/\n",
      "config. Once this is done, we will manually edit it.\n",
      "Keys, keys, and more keys\n",
      "There are three completely different types of keys that are important \n",
      "when dealing with AWS. First, there is a standard username/password \n",
      "combination, which you use to log in to the website. Second, there is \n",
      "the SSH key system, which is a public/private key system implemented \n",
      "with files; with your public key file, you can log in to remote machines. \n",
      "Third, there is the AWS access key/secret key system, which is just a \n",
      "form of username/password that allows you to have multiple users on \n",
      "the same account (including adding different permissions to each one, \n",
      "but we will not cover these advanced features in this book).\n",
      "To look up our access/secret keys, we go back to the AWS Console, click \n",
      "on our name on the top-right, and select Security Credentials. Now at \n",
      "the bottom of the screen, there should be our access key, which may \n",
      "look something like AAKIIT7HHF6IUSN3OCAA, which we will use as \n",
      "an example in this chapter.\n",
      "Now, edit the configuration file. This is a standard .ini file: a text file where sections \n",
      "start by having their names in brackets and options are specified in the name=value \n",
      "format. The first section is the aws info section and you should copy and paste your \n",
      "keys here:\n",
      "[aws info]\n",
      "AWS_ACCESS_KEY_ID =  AAKIIT7HHF6IUSN3OCAA\n",
      "AWS_SECRET_ACCESS_KEY = <your secret key>\n",
      "Now we come to the fun part, that is, defining a cluster. StarCluster allows you \n",
      "to define as many different clusters as you wish. The starting file has one called \n",
      "smallcluster. It's defined in the cluster smallcluster section. We will edit it  \n",
      "to read as follows:\n",
      "[cluster smallcluster]\n",
      "KEYNAME = mykey\n",
      "CLUSTER_SIZE = 16\n",
      "This changes the number of nodes to 16 instead of the default of two. We can \n",
      "additionally specify which type of instance each node will be and what the initial \n",
      "image is (remember, an image is used to initialized the virtual hard disk, which \n",
      "defines what operating system you will be running and what software is installed). \n",
      "StarCluster has a few predefined images, but you can also build your own.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 306, 'page_label': '286'}\n",
      "Bigger Data\n",
      "[  286 ]\n",
      "We need to create a new SSH key with the following command:\n",
      "$ starcluster createkey mykey -o ~/.ssh/mykey.rsa\n",
      "Now that we have configured a sixteen node cluster and set up the keys, let's try \n",
      "it out:\n",
      "$ starcluster start smallcluster\n",
      "This may take a few minutes as it allocates seventeen new machines. Why seventeen \n",
      "when our cluster is only sixteen nodes? StarCluster always creates a master node.  \n",
      "All of these nodes have the same filesystem, so anything we create on the master \n",
      "node will also be seen by the worker nodes. This also means that we can use jug  \n",
      "on these clusters.\n",
      "These clusters can be used as you wish, but they come pre-equipped with a job queue \n",
      "engine, which makes it ideal for batch processing. The process of using them is simple:\n",
      "1. You log in to the master node.\n",
      "2. You prepare your scripts on the master (or better yet, have them prepared\n",
      "before hand).\n",
      "3. You submit jobs to the queue. A job can be any Unix command.\n",
      "The scheduler will find free nodes and run your job.\n",
      "4. You wait for the jobs to finish.\n",
      "5. You read the results on the master node. You can also now kill \n",
      "all the slave\n",
      "nodes to save money. In any case, do not leave your system running when you\n",
      "do not need it anymore! Otherwise, this will cost you (in dollars-and-cents).\n",
      "Before logging in to the cluster, we will copy our data to it (remember we had earlier \n",
      "cloned the repository onto BuildingMachineLearningSystemsWithPython):\n",
      "$ dir=BuildingMachineLearningSystemsWithPython\n",
      "$ starcluster put smallcluster $dir $dir\n",
      "We used the $dir variable to make the command line fit in a single line. We can log \n",
      "in to the master node with a single command:\n",
      "$ starcluster sshmaster smallcluster\n",
      "We could also have looked up the address of the machine that was generated and \n",
      "used an ssh command as we did earlier, but using the preceding command, it does \n",
      "not matter what the address was, as StarCluster takes care of it behind the scenes  \n",
      "for us.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 307, 'page_label': '287'}\n",
      "Chapter 12\n",
      "[  287 ]\n",
      "As we said earlier, StarCluster provides a batch queuing system for its clusters; you \n",
      "write a script to perform your actions, put it on the queue, and it will run in any \n",
      "available node.\n",
      "At this point, we need to install some packages again. Fortunately, StarCluster has \n",
      "already done half the work. If this was a real project, we would set up a script to \n",
      "perform all the initialization for us. StarCluster can do this. As this is a tutorial, we \n",
      "just run the installation step again:\n",
      "$ pip install jug mahotas scikit-learn\n",
      "We can use the same jugfile system as before, except that now, instead of running it \n",
      "directly on the master, we schedule it on the cluster.\n",
      "First, write a very simple wrapper script as follows:\n",
      "#!/usr/bin/env bash\n",
      "jug execute jugfile.py\n",
      "Call it run-jugfile.sh and use chmod +x run-jugfile.sh to give it executable \n",
      "permission. Now, we can schedule sixteen jobs on the cluster by using the following \n",
      "command:\n",
      "$ for c in $(seq 16); do\n",
      ">    qsub -cwd run-jugfile.sh\n",
      "> done\n",
      "This will create 16 jobs, each of which will run the run-jugfile.sh script, which we \n",
      "will simply call jug. You can still use the master as you wish. In particular, you can, \n",
      "at any moment, run jug status and see the status of the computation. In fact, jug \n",
      "was developed in exactly such an environment, so it works very well in it.\n",
      "Eventually, the computation will finish. At this point, we need to first save our \n",
      "results. Then, we can kill off all the nodes. We create a directory ~/results and  \n",
      "copy our results here:\n",
      "# mkdir ~/results\n",
      "# cp results.image.txt ~/results\n",
      "Now, log off the cluster back to our worker machine:\n",
      "# exit\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 308, 'page_label': '288'}\n",
      "Bigger Data\n",
      "[  288 ]\n",
      "Now, we are back at our AWS machine (notice the $ sign in the next code examples). \n",
      "First, we copy the results back to this computer using the starcluster get \n",
      "command (which is the mirror image of put we used before):\n",
      "$ starcluster get smallcluster results results\n",
      "Finally, we should kill all the nodes to save money as follows:\n",
      "$ starcluster stop smallcluster\n",
      "$ starcluster terminate smallcluster\n",
      "Note that terminating will really destroy the filesystem and all your \n",
      "results. In our case, we have copied the final results to safety manually. \n",
      "Another possibility is to have the cluster write to a filesystem, which is \n",
      "not allocated and destroyed by StarCluster, but is available to you on a \n",
      "regular instance; in fact, the flexibility of these tools is immense. However, \n",
      "these advanced manipulations could not all fit in this chapter.\n",
      "StarCluster has excellent documentation online at http://star.mit.\n",
      "edu/cluster/, which you should read for more information about all \n",
      "the possibilities of this tool. We have seen only a small fraction of the \n",
      "functionality and used only the default settings here.\n",
      "Summary\n",
      "We saw how to use jug, a little Python framework to manage computations in a \n",
      "way that takes advantage of multiple cores or multiple machines. Although this \n",
      "framework is generic, it was built specifically to address the data analysis needs of \n",
      "its author (who is also an author of this book). Therefore, it has several aspects that \n",
      "make it fit in with the rest of the Python machine learning environment.\n",
      "You also learned about AWS, the Amazon Cloud. Using cloud computing is often a \n",
      "more effective use of resources than building in-house computing capacity. This is \n",
      "particularly true if your needs are not constant and are changing. StarCluster even \n",
      "allows for clusters that automatically grow as you launch more jobs and shrink as \n",
      "they terminate.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 309, 'page_label': '289'}\n",
      "Chapter 12\n",
      "[  289 ]\n",
      "This is the end of the book. We have come a long way. You learned how to perform \n",
      "classification when we labeled data and clustering when we do not. You learned \n",
      "about dimensionality reduction and topic modeling to make sense of large datasets. \n",
      "Towards the end, we looked at some specific applications (such as music genre \n",
      "classification and computer vision). For implementations, we relied on Python. This \n",
      "language has an increasingly expanding ecosystem of numeric computing packages \n",
      "built on top of NumPy. Whenever possible, we relied on scikit-learn, but used \n",
      "other packages when necessary. Due to the fact that they all use the same basic data \n",
      "structure (the NumPy multidimensional array), it's possible to mix functionality \n",
      "from different packages seamlessly. All of the packages used in this book are open \n",
      "source and available for use in any project.\n",
      "Naturally, we did not cover every machine learning topic. In the Appendix, we \n",
      "provide pointers to a selection of other resources that will help interested readers \n",
      "learn more about machine learning.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 310, 'page_label': '290'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 311, 'page_label': '291'}\n",
      "[ 291 ]\n",
      "Where to Learn More \n",
      "Machine Learning\n",
      "We are at the end of our book and now take a moment to look at what else is out \n",
      "there that could be useful for our readers.\n",
      "There are many wonderful resources out there to learn more about machine \n",
      "learningway too much to cover them all here. The following list can therefore \n",
      "represent only a small, and very biased, sampling of the resources the authors think \n",
      "are best at the time of writing.\n",
      "Online courses\n",
      "Andrew Ng is a professor at Stanford who runs an online course in machine learning \n",
      "as a massive open online course at Coursera (http://www.coursera.org). It is free \n",
      "of charge, but may represent a significant time investment.\n",
      "Books\n",
      "This book is focused on the practical side of machine learning. We did not present the \n",
      "thinking behind the algorithms or the theory that justifies them. If you are interested \n",
      "in that aspect of machine learning, then we recommend Pattern Recognition and \n",
      "Machine Learning by Christopher Bishop. This is a classical introductory text in the \n",
      "field. It will teach you the nitty-gritty of most of the algorithms we used in this book.\n",
      "If you want to move beyond the introduction and learn all the gory mathematical \n",
      "details, then Machine Learning: A Probabilistic Perspective by Kevin P. Murphy is an \n",
      "excellent option (www.cs.ubc.ca/~murphyk/MLbook). It's very recent (published in \n",
      "2012) and contains the cutting edge of ML research. This 1100 page book can also \n",
      "serve as a reference as very little of machine learning has been left out.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 312, 'page_label': '292'}\n",
      "Where to Learn More Machine Learning\n",
      "[  292 ]\n",
      "Question and answer sites\n",
      "MetaOptimize (http://metaoptimize.com/qa) is a machine learning  \n",
      "question and answer website where many very knowledgeable researchers  \n",
      "and practitioners interact.\n",
      "Cross Validated (http://stats.stackexchange.com) is a general statistics question \n",
      "and answer site, which often features machine learning questions as well.\n",
      "As mentioned in the beginning of the book, if you have questions specific to particular \n",
      "parts of the book, feel free to ask them at TwoToReal (http://www.twotoreal.com). \n",
      "We try to be as quick as possible to jump in and help as best as we can.\n",
      "Blogs\n",
      "Here is an obviously non-exhaustive list of blogs, which are interesting to someone \n",
      "working in machine learning:\n",
      " Machine Learning Theory: http://hunch.net\n",
      "The average pace is approximately one post per month. Posts are more \n",
      "theoretical. They offer additional value in brain teasers.\n",
      " Text & Data Mining by practical means: http://textanddatamining.\n",
      "blogspot.de\n",
      "Average pace is one post per month, very practical, always surprising \n",
      "approaches.\n",
      " Edwin Chen's Blog: http://blog.echen.me\n",
      "The average pace is one post per month, providing more applied topics.\n",
      " Machined Learnings: http://www.machinedlearnings.com\n",
      "The average pace is one post per month, providing more applied topics.\n",
      " FlowingData: http://flowingdata.com\n",
      "The average pace is one post per day, with the posts revolving more  \n",
      "around statistics.\n",
      " Simply Statistics: http://simplystatistics.org\n",
      "Several posts per month, focusing on statistics and big data.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 313, 'page_label': '293'}\n",
      "Appendix\n",
      "[  293 ]\n",
      " Statistical Modeling, Causal Inference, and Social Science:  \n",
      "http://andrewgelman.com\n",
      "One post per day with often funny reads when the author points out flaws in \n",
      "popular media, using statistics.\n",
      "Data sources\n",
      "If you want to play around with algorithms, you can obtain many datasets from the \n",
      "Machine Learning Repository at the University of California at Irvine (UCI). You can \n",
      "find it at http://archive.ics.uci.edu/ml.\n",
      "Getting competitive\n",
      "An excellent way to learn more about machine learning is by trying out a \n",
      "competition! Kaggle (http://www.kaggle.com) is a marketplace of ML competitions \n",
      "and was already mentioned in the introduction. On the website, you will find several \n",
      "different competitions with different structures and often cash prizes.\n",
      "The supervised learning competitions almost always follow the following format: \n",
      "you (and every other competitor) are given access to labeled training data and testing \n",
      "data (without labels). Your task is to submit predictions for testing data. When the \n",
      "competition closes, whoever has the best accuracy wins. The prizes range from glory \n",
      "to cash.\n",
      "Of course, winning something is nice, but you can gain a lot of useful experience \n",
      "just by participating. So, you have to stay tuned after the competition is over as \n",
      "participants start sharing their approaches in the forum. Most of the time, winning is \n",
      "not about developing a new algorithm, but cleverly preprocessing, normalizing, and \n",
      "combining existing methods.\n",
      "All that was left out\n",
      "We did not cover every machine learning package available for Python. Given the \n",
      "limited space, we chose to focus on scikit-learn. However, there are other options \n",
      "and we list a few of them here:\n",
      " MDP toolkit (http://mdp-toolkit.sourceforge.net): Modular toolkit for \n",
      "data processing\n",
      " PyBrain (http://pybrain.org): Python-based Reinforcement Learning, \n",
      "Artificial Intelligence, and Neural Network Library\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 314, 'page_label': '294'}\n",
      "Where to Learn More Machine Learning\n",
      "[  294 ]\n",
      " Machine Learning Toolkit (Milk) (http://luispedro.org/software/milk): \n",
      "This package was developed by one of the authors of this book and covers \n",
      "some algorithms and techniques that are not included in scikit-learn\n",
      " Pattern (http://www.clips.ua.ac.be/pattern): A package that combines \n",
      "web mining, natural language processing, and machine learning, having \n",
      "wrapper APIs for Google, Twitter, and Wikipedia.\n",
      "A more general resource is http://mloss.org, which is a repository of open source \n",
      "machine learning software. As is usually the case with repositories such as this one, \n",
      "the quality varies between excellent well maintained software and projects that were \n",
      "one-offs and then abandoned. It may be worth checking out whether your problem is \n",
      "very specific and none of the more general packages address it.\n",
      "Summary\n",
      "We are now truly at the end. We hope you enjoyed the book and feel well equipped \n",
      "to start your own machine learning adventure.\n",
      "We also hope you learned the importance of carefully testing your methods. In \n",
      "particular, the importance of using correct cross-validation method and not report \n",
      "training test results, which are an over-inflated estimate of how good your method \n",
      "really is.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 315, 'page_label': '295'}\n",
      "[ 295 ]\n",
      "Index\n",
      "A\n",
      "AcceptedAnswerId  99\n",
      "additive smoothing  131\n",
      "add-one smoothing  131\n",
      "Amazon\n",
      "URL  274\n",
      "Amazon Web Services (AWS)\n",
      "about  274\n",
      "accessing  275\n",
      "cluster generation, automating with \n",
      "StarCluster  284-288\n",
      "using  274, 275\n",
      "virtual machines, creating  276-282\n",
      "Anaconda Python distribution\n",
      "URL  6\n",
      "area under curve (AUC)  118\n",
      "Associated Press (AP)  81\n",
      "association rules  194\n",
      "Auditory Filterbank Temporal Envelope \n",
      "(AFTE)  217\n",
      "Automatic Music Genre Classification \n",
      "(AMGC)  214\n",
      "AvgSentLen  106\n",
      "AvgWordLen  106\n",
      "B\n",
      "bag of word approach\n",
      "drawbacks  65\n",
      "less important words, removing  59, 60\n",
      "raw text, converting into  \n",
      "bag of words  54, 55\n",
      "stemming  60\n",
      "word count vectors, normalizing  58, 59\n",
      "words, counting  55-58\n",
      "words, stopping on steroids  63-65\n",
      "BaseEstimator  152\n",
      "basket analysis\n",
      "about  188, 189\n",
      "advanced baskets analysis  196\n",
      "association rule mining  194-196\n",
      "supermarket shopping baskets,  \n",
      "analyzing  190-194\n",
      "useful predictions, obtaining  190\n",
      "BernoulliNB  135\n",
      "big data\n",
      "about  264\n",
      "jug, functioning  268, 269\n",
      "jug, using for data analysis  269-272\n",
      "partial results, reusing  272-274\n",
      "pipeline, breaking into tasks with jug  264\n",
      "tasks, introducing in jug  265-267\n",
      "binary classification  47-49\n",
      "blogs, machine learning\n",
      "URLs  292, 293\n",
      "Body attribute  99\n",
      "C\n",
      "classes  96\n",
      "classification  29\n",
      "classification model\n",
      "building  32-36\n",
      "cross-validation  36-39\n",
      "data, holding  36-39\n",
      "gain or loss function  40\n",
      "search procedure  39\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 316, 'page_label': '296'}\n",
      "[ 296 ]\n",
      "structure  39\n",
      "classifier\n",
      "about  96\n",
      "building, real data used  95\n",
      "building, with FFT  205\n",
      "classy answers, classifying  96\n",
      "confusion matrix, using  207-209\n",
      "creating  100\n",
      "data, fetching  97\n",
      "data instance, tuning  96\n",
      "experimentation agility, increasing  205, 206\n",
      "features, designing  104-107\n",
      "features, engineering  101-103\n",
      "kNN, starting with  100, 101\n",
      "logistic regression, using  112, 207\n",
      "performance, improving with Mel \n",
      "Frequency Cepstrum (MFC) \n",
      "Coefficients  214-217\n",
      "performance, measuring  103, 104\n",
      "performance, measuring  \n",
      "with receiver-operator  \n",
      "characteristics (ROC)  210-212\n",
      "precision, measuring  116-119\n",
      "recall, measuring  116-119\n",
      "roadmap, sketching  96\n",
      "serializing  121\n",
      "slimming  120\n",
      "training  103\n",
      "tuning  96\n",
      "clustering\n",
      "about  66\n",
      "hierarchical clustering  66\n",
      "k-means  66-69\n",
      "posts  72\n",
      "testing  70, 71\n",
      "coefficient of determination  160\n",
      "CommentCount  99\n",
      "compactness  42\n",
      "complex classifier\n",
      "building  39, 40\n",
      "nearest neighbor classifier  43\n",
      "complex dataset\n",
      "about  41\n",
      "feature engineering  42\n",
      "Seeds dataset  41\n",
      "computer vision\n",
      "image processing  219\n",
      "local feature representations  235-238\n",
      "Coursera\n",
      "URL  291\n",
      "CreationDate  98\n",
      "Cross Validated\n",
      "about  292\n",
      "URL  292\n",
      "cross-validation  37-39\n",
      "D\n",
      "data, classifier\n",
      "attributes, preselecting  98\n",
      "fetching  97\n",
      "slimming, to chewable chunks  98\n",
      "training data, creating  100\n",
      "data sources, machine learning  293\n",
      "dimensionality reduction\n",
      "about  87, 241\n",
      "feature extraction  254\n",
      "features, selecting  242\n",
      "multidimensional scaling  258-261\n",
      "roadmap, sketching  242\n",
      "documents\n",
      "comparing, by topics  86-89\n",
      "E\n",
      "Elastic Compute Cluster (EC2) service  274\n",
      "ElasticNet model  165\n",
      "English-language Wikipedia model\n",
      "building  89-92\n",
      "ensemble learning  186\n",
      "Enthought Canopy\n",
      "URL  6\n",
      "F\n",
      "fast Fourier transform (FFT)\n",
      "about  203\n",
      "used, for building classifier  205\n",
      "feature engineering  42\n",
      "feature extraction\n",
      "about  254\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 317, 'page_label': '297'}\n",
      "[ 297 ]\n",
      "Linear Discriminant Analysis (LDA)  257\n",
      "PCA, applying  255, 256\n",
      "PCA, limitations  257\n",
      "PCA, sketching  255\n",
      "principal component analysis (PCA)  254\n",
      "feature selection\n",
      "about  42, 242\n",
      "correlation  243-246\n",
      "methods  253\n",
      "model, features asking for  251-253\n",
      "mutual information  246-251\n",
      "redundant features,  \n",
      "detecting with filters  242\n",
      "fit(document, y=None) method  152\n",
      "F-measure  142\n",
      "free tier  275\n",
      "G\n",
      "GaussianNB  134\n",
      "get_feature_names() method  152\n",
      "Grid Engine  264\n",
      "GridSearchCV  141\n",
      "H\n",
      "hierarchical clustering  66\n",
      "hierarchical Dirichlet process (HDP)  93\n",
      "house prices, predicting with regression\n",
      "about  157-161\n",
      "cross-validation, for regression  162, 163\n",
      "multidimensional regression  161\n",
      "I\n",
      "image processing\n",
      "about  219, 220\n",
      "basic image classification  228\n",
      "center, putting in focus  225-227\n",
      "custom features, writing  230, 231\n",
      "features, computing from images  229, 230\n",
      "features, used for finding similar  \n",
      "images  232-234\n",
      "Gaussian blurring  223-225\n",
      "harder dataset, classifying  234, 235\n",
      "images, displaying  220, 221\n",
      "images, loading  220, 221\n",
      "thresholding  222\n",
      "improvement, classifier\n",
      "bias-variance  108\n",
      "high bias  109\n",
      "high bias, fixing  108\n",
      "high variance, fixing  109\n",
      "high variance problem, hinting  109-111\n",
      "steps  107, 108\n",
      "initial challenge\n",
      "impression of noise example  75\n",
      "solving  73-75\n",
      "Iris dataset\n",
      "about  30\n",
      "classification model, building  32-36\n",
      "features  30\n",
      "visualization  30, 31\n",
      "J\n",
      "jug\n",
      "about  263\n",
      "running, on cloud machine  283, 284\n",
      "using, for data analysis  269-272\n",
      "working  268, 269\n",
      "jug cleanup  274\n",
      "jug invalidate  274\n",
      "jug status --cache  274\n",
      "K\n",
      "Kaggle\n",
      "URL  293\n",
      "k-means  66-69\n",
      "L\n",
      "labels  96\n",
      "Laplace smoothing  131\n",
      "Lasso  165\n",
      "latent Dirichlet allocation (LDA)\n",
      "about  80\n",
      "topic model, building  81-86\n",
      "Wikipedia URL  80\n",
      "lift  194\n",
      "Linear Discriminant Analysis (LDA)  257\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 318, 'page_label': '298'}\n",
      "[ 298 ]\n",
      "local feature representations  235-238\n",
      "logistic regression\n",
      "about  112\n",
      "applying, to post classification  \n",
      "problem  114-116\n",
      "example  112-114\n",
      "using  112\n",
      "LSF (Load Sharing Facility)  264\n",
      "M\n",
      "machine learning\n",
      "about  2\n",
      "resources  291\n",
      "tiny application  13\n",
      "Machine Learning Toolkit (Milk)\n",
      "URL  294\n",
      "matplotlib\n",
      "about  6\n",
      "URL  6\n",
      "matshow() function  208\n",
      "MDP toolkit\n",
      "URL  293\n",
      "Mel Frequency Cepstrum (MFC)\n",
      "used, for improving classification \n",
      "performance  214-217\n",
      "MetaOptimize\n",
      "about  292\n",
      "URL   292\n",
      "MLComp\n",
      "URL  70\n",
      "model, tiny application\n",
      "complex model  20-22\n",
      "data, viewing  22-25\n",
      "model function, calculating  27\n",
      "selecting  18\n",
      "straight line model  18-20\n",
      "testing  26, 27\n",
      "training  26, 27\n",
      "mpmath\n",
      "URL  132\n",
      "multiclass classification  47-49\n",
      "multidimensional regression\n",
      "about  161\n",
      "using  161, 162\n",
      "multidimensional scaling (MDS)  258-261\n",
      "MultinomialNB classifier  135, 141\n",
      "music\n",
      "analyzing  201, 202\n",
      "data, fetching  200\n",
      "decomposing, into sine wave  \n",
      "components  203-205\n",
      "wave format, converting into  200\n",
      "N\n",
      "Nave Bayes classifier\n",
      "about  124, 125\n",
      "arithmetic underflows,  \n",
      "accounting for  132-134\n",
      "BernoulliNB  135\n",
      "classes, using  138, 139\n",
      "GaussianNB  134\n",
      "MultinomialNB  135\n",
      "Nave Bayes theorem  125, 126\n",
      "parameters, tuning  141-145\n",
      "problem, solving  135-138\n",
      "unseen words, accounting for  131\n",
      "using, to classify  127-130\n",
      "working  126\n",
      "Natural Language Toolkit (NLTK)\n",
      "about  60\n",
      "installing  60, 61\n",
      "URL  60\n",
      "vectorizer, extending with  62\n",
      "nearest neighbor classifier  43\n",
      "neighborhood approach,  \n",
      "recommendations 180-183\n",
      "NumAllCaps  106\n",
      "NumExclams  106\n",
      "NumPy\n",
      "about  6\n",
      "examples  6, 7\n",
      "indexing  9\n",
      "learning  7-9\n",
      "nonexisting values, handling  10\n",
      "runtime, comparing  11\n",
      "URL, for examples  7\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 319, 'page_label': '299'}\n",
      "[ 299 ]\n",
      "O\n",
      "one-dimensional regression  158\n",
      "online course, machine learning\n",
      "URL  291\n",
      "opinion mining  123\n",
      "ordinary least squares (OLS) regression  157\n",
      "Otsu  222\n",
      "overfitting  22\n",
      "OwnerUserId  99\n",
      "P\n",
      "parameters, clustering\n",
      "tweaking  76\n",
      "Part Of Speech (POS)  123\n",
      "PBS (Portable Batch System)  264\n",
      "penalized regression\n",
      "about  163\n",
      "ElasticNet, using in scikit-learn  165\n",
      "example, text documents  168, 169\n",
      "hyperparameters, setting in  \n",
      "principled way  170-174\n",
      "L1 penalties  164, 165\n",
      "L2 penalties  164, 165\n",
      "Lasso path, visualizing  166, 167\n",
      "Lasso, using in scikit-learn  165\n",
      "P greater than N scenarios  167\n",
      "Penn Treebank Project\n",
      "URL  148\n",
      "POS tag abbreviations,  \n",
      "Penn Treebank  149, 150\n",
      "PostTypeId attribute  98\n",
      "precision_recall_curve() function  117\n",
      "predictions, rating with regression\n",
      "about  175-177\n",
      "dataset, splitting into training and  \n",
      "testing  177, 178\n",
      "training data, normalizing  178-180\n",
      "pre-processing phase\n",
      "achievements  65\n",
      "goals  65\n",
      "principal component analysis (PCA)\n",
      "about  254\n",
      "applying  255, 256\n",
      "limitations  257\n",
      "properties  254\n",
      "sketching  255\n",
      "PyBrain\n",
      "URL  293\n",
      "Python\n",
      "installing  6\n",
      "packages, installing, on Amazon Linux  282\n",
      "URL  6\n",
      "Q\n",
      "Q&A sites\n",
      "Cross Validated  5\n",
      "Kaggle  5\n",
      "MetaOptimize  5\n",
      "Stack Overflow  5\n",
      "TwoToReal  5\n",
      "R\n",
      "receiver-operator characteristics (ROC)\n",
      "about  210\n",
      "used, for measuring classifier  \n",
      "performance  210-212\n",
      "recommendations\n",
      "about  175\n",
      "multiple methods, combining  186-188\n",
      "neighborhood approach  180-183\n",
      "regression approach  184\n",
      "regression\n",
      "about  165\n",
      "cross-validation  162\n",
      "regression approach, recommendations\n",
      "about  184\n",
      "issues  184\n",
      "regularized regression. See  penalized \n",
      "regression\n",
      "resources, machine learning\n",
      "blogs  292\n",
      "books  291\n",
      "competition  293\n",
      "data sources  293\n",
      "online courses  291\n",
      "question and answer sites  292\n",
      "Ridge Regression  165\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 320, 'page_label': '300'}\n",
      "[ 300 ]\n",
      "root mean square error (RMSE)\n",
      "about  159\n",
      "advantage  160\n",
      "roundness  42\n",
      "S\n",
      "save() function  206\n",
      "scale-invariant feature transform (SIFT)  219\n",
      "scikit-learn classification\n",
      "about  43, 44\n",
      "decision boundaries, examining  45-47\n",
      "SciKit library  52\n",
      "SciPy\n",
      "about  6\n",
      "learning  12, 13\n",
      "toolboxes  12, 13\n",
      "URL  6\n",
      "Securities and Exchange  \n",
      "Commission (SEC)  168\n",
      "Seeds dataset\n",
      "about  41\n",
      "features  41\n",
      "sentiment analysis\n",
      "about  123\n",
      "first classifier, creating  134\n",
      "Nave Bayes classifier  124\n",
      "roadmap, sketching  123\n",
      "tweets, cleaning  146\n",
      "Twitter data, fetching  124\n",
      "SentiWordNet\n",
      "URL  150\n",
      "similarity measuring\n",
      "about  52\n",
      "bag of word approach  53\n",
      "SoX\n",
      "URL  200\n",
      "sparse  165\n",
      "sparsity  83\n",
      "specgram function  201\n",
      "Speeded Up Robust Features (SURF)  235\n",
      "stacked learning  186\n",
      "Stack Overflow\n",
      "URL  5\n",
      "StarCluster\n",
      "about  288\n",
      "URL  288\n",
      "used, for automating cluster  \n",
      "generation  284-287\n",
      "stemming  60\n",
      "T\n",
      "Talkbox SciKit\n",
      "URL  214\n",
      "task  265\n",
      "testing accuracy  36\n",
      "TfidfVectorizer parameter  141\n",
      "thresholding  222\n",
      "TimeToAnswer  101\n",
      "tiny application, machine learning\n",
      "about  13\n",
      "data, cleaning  15, 16\n",
      "data, preprocessing  15, 16\n",
      "data, reading in  14, 15\n",
      "learning algorithm, selecting  17\n",
      "model, selecting  17, 18\n",
      "Title attribute  99\n",
      "toolboxes, SciPy\n",
      "cluster  12\n",
      "constants  13\n",
      "fftpack  13\n",
      "integrate  13\n",
      "interpolate  13\n",
      "io  13\n",
      "linalg  13\n",
      "ndimage  13\n",
      "odr  13\n",
      "optimize  13\n",
      "signal  13\n",
      "sparse  13\n",
      "spatial  13\n",
      "special  13\n",
      "stats  13\n",
      "topic modeling  79\n",
      "topics\n",
      "about  79\n",
      "documents comparing by  86-89\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 321, 'page_label': '301'}\n",
      "[ 301 ]\n",
      "number of topics, selecting  92, 93\n",
      "training accuracy  36\n",
      "train_model()function  136\n",
      "transform(documents) method  152\n",
      "tweets\n",
      "cleaning  146-148\n",
      "Twitter data\n",
      "fetching  124\n",
      "two-levels of cross-validation  171\n",
      "TwoToReal\n",
      "URL  292\n",
      "U\n",
      "underfitting  24\n",
      "V\n",
      "ViewCount  99\n",
      "virtual machines, AWS\n",
      "creating  276-282\n",
      "jug, running on cloud machine  283, 284\n",
      "Python packages, installing on Amazon \n",
      "Linux  282\n",
      "visual words  237\n",
      "W\n",
      "Wikipedia dump\n",
      "URL  89\n",
      "word types\n",
      "about  148\n",
      "determining  148\n",
      "estimator  152\n",
      "implementing  155, 156\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 322, 'page_label': '302'}\n",
      "\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 323, 'page_label': '303'}\n",
      "Thank you for buying  \n",
      "Building Machine Learning Systems  \n",
      "with Python\n",
      "Second Edition\n",
      "About Packt Publishing\n",
      "Packt, pronounced 'packed', published its first book, Mastering phpMyAdmin for Effective \n",
      "MySQL Management, in April 2004, and subsequently continued to specialize in publishing \n",
      "highly focused books on specific technologies and solutions.\n",
      "Our books and publications share the experiences of your fellow IT professionals in adapting \n",
      "and customizing today's systems, applications, and frameworks. Our solution-based books \n",
      "give you the knowledge and power to customize the software and technologies you're using \n",
      "to get the job done. Packt books are more specific and less general than the IT books you have \n",
      "seen in the past. Our unique business model allows us to bring you more focused information, \n",
      "giving you more of what you need to know, and less of what you don't.\n",
      "Packt is a modern yet unique publishing company that focuses on producing quality,  \n",
      "cutting-edge books for communities of developers, administrators, and newbies alike.  \n",
      "For more information, please visit our website at www.packtpub.com.\n",
      "About Packt Open Source\n",
      "In 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order  \n",
      "to continue its focus on specialization. This book is part of the Packt Open Source brand,  \n",
      "home to books published on software built around open source licenses, and offering \n",
      "information to anybody from advanced developers to budding web designers. The Open \n",
      "Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty \n",
      "to each open source project about whose software a book is sold.\n",
      "Writing for Packt\n",
      "We welcome all inquiries from people who are interested in authoring. Book proposals should \n",
      "be sent to author@packtpub.com. If your book idea is still at an early stage and you would \n",
      "like to discuss it first before writing a formal book proposal, then please contact us; one of our \n",
      "commissioning editors will get in touch with you. \n",
      "We're not just looking for published authors; if you have strong technical skills but no writing \n",
      "experience, our experienced editors can help you develop a writing career, or simply get some \n",
      "additional reward for your expertise.\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 324, 'page_label': '304'}\n",
      "Mastering Machine Learning with \n",
      "scikit-learn\n",
      "ISBN: 978-1-78398-836-5              Paperback: 238 pages\n",
      "Apply effective learning algorithms to real-world \n",
      "problems using scikit-learn\n",
      "1. Design and troubleshoot machine learning \n",
      "systems for common tasks including regression, \n",
      "classification, and clustering.\n",
      "2. Acquaint yourself with popular machine \n",
      "learning algorithms, including decision trees, \n",
      "logistic regression, and support vector machines.\n",
      "3. A practical example-based guide to help you \n",
      "gain expertise in implementing and evaluating \n",
      "machine learning systems using scikit-learn.\n",
      "Scala for Machine Learning\n",
      "ISBN: 978-1-78355-874-2             Paperback: 520 pages\n",
      "Leverage Scala and Machine Learning to construct \n",
      "and study systems that can learn from data\n",
      "1. Explore a broad variety of data processing, \n",
      "machine learning, and genetic algorithms \n",
      "through diagrams, mathematical formulation, \n",
      "and source code.\n",
      "2. Leverage your expertise in Scala programming \n",
      "to create and customize AI applications with \n",
      "your own scalable machine learning algorithms.\n",
      "3. Experiment with different techniques,  \n",
      "and evaluate their benefits and limitations \n",
      "using real-world financial applications, in a \n",
      "tutorial style.\n",
      " \n",
      "Please check www.PacktPub.com for information on our titles\n",
      "{'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2015-03-24T13:14:02+05:30', 'moddate': '2015-03-25T17:33:08+05:30', 'trapped': '/False', 'source': 'books\\\\Building Machine Learning Systems with Python - Second Edition.pdf', 'total_pages': 326, 'page': 325, 'page_label': '305'}\n",
      "R Machine Learning Essentials\n",
      "ISBN: 978-1-78398-774-0            Paperback: 218 pages\n",
      "Gain quick access to the machine learning concepts \n",
      "and practical applications using the R development \n",
      "environment\n",
      "1. Build machine learning algorithms using the \n",
      "most powerful tools in R.\n",
      "2. Identify business problems and solve them by \n",
      "developing effective solutions.\n",
      "3. Hands-on tutorial explaining the concepts \n",
      "through lots of practical examples, tips  \n",
      "and tricks.\n",
      "Clojure for Machine Learning\n",
      "ISBN: 978-1-78328-435-1             Paperback: 292 pages\n",
      "Successfully leverage advanced machine learning \n",
      "techniques using the Clojure ecosystem\n",
      "1. Covers a lot of machine learning techniques \n",
      "with Clojure programming.\n",
      "2. Encompasses precise patterns in data to \n",
      "predict future outcomes using various machine \n",
      "learning techniques.\n",
      "3. Packed with several machine learning libraries \n",
      "available in the Clojure ecosystem.\n",
      " \n",
      "Please check www.PacktPub.com for information on our titles\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='books',\n",
    "    glob='*.pdf',\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "\n",
    "docs = loader.lazy_load()  # here we use lazy_load() for fast loading \n",
    "\n",
    "for document in docs:\n",
    "    print(document.metadata)\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d10a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
