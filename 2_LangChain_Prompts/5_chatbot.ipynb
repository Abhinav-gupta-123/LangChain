{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a748181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82016c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatOllama(model='llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1d196",
   "metadata": {},
   "source": [
    "Let see how our chatbot works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55252072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  It seems like you didn't ask a question. What's on your mind? Do you need help with something or just want to chat? I'm here to listen and assist!\n",
      "AI:  I couldn't find any information on a person or entity called \"QLORA Optimization Technique\" specifically related to fine-tuning Large Language Models (LLMs).\n",
      "\n",
      "However, I did find a research paper titled \"Optimizing Large Language Models via QLORA: Quantized Linear Regression with Adaptive Residual Anchors\" by researchers from the University of California, Berkeley. The authors of this paper are:\n",
      "\n",
      "* Shengyu Liu\n",
      "* Jianliang Xu\n",
      "* et al.\n",
      "\n",
      "The QLORA (Quantized Linear Regression with Adaptive Residual Anchors) technique is a optimization method for fine-tuning LLMs, but I couldn't find any information on a person or entity specifically associated with the term \"QLORA Optimization Technique\".\n",
      "AI:  I don't have any information about who \"they\" refers to. Could you please provide more context or clarify who \"they\" is? I'll do my best to help.\n",
      "AI:  It seems like you forgot to ask a question. Please go ahead and ask me anything, and I'll do my best to help!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    result = model.invoke(user_input)\n",
    "    print(\"AI: \",result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09909880",
   "metadata": {},
   "source": [
    "As we can see in above chats the main problem is that when we ask some question it give answer but again when we ask the question related to previous question it can not refer to previous one , so we will use chat history method for more interactive bot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d4d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\n",
      "AI:  Nice to \"meet\" you too!\n",
      "\n",
      "The founder of the Transformer architecture is Ashish Vaswani, but it was originally developed by a team of researchers at Google Brain, including:\n",
      "\n",
      "1. Jason Weston\n",
      "2. Stephen Merity\n",
      "3. Kashi Goel (also known as Kashi)\n",
      "4. Ilya Loshchilov\n",
      "5. Frank Parham\n",
      "\n",
      "However, the idea of using self-attention mechanisms to process sequential data was first explored by Vaswani and his team in their 2017 paper \"Attention is All You Need\", which introduced the Transformer architecture.\n",
      "\n",
      "The Transformer architecture has since become a fundamental component of many modern deep learning models, including Language Models like myself!\n",
      "\n",
      "How can I assist you today? Do you have any specific questions or topics you'd like to discuss?\n",
      "AI:  Nice to \"meet\" you too!\n",
      "\n",
      "Let's dive deeper into the individuals who contributed to the development of the Transformer architecture:\n",
      "\n",
      "1. **Ashish Vaswani**:\n",
      "Ashish Vaswani is a researcher and engineer at Google Brain, where he led the research team that developed the Transformer architecture. He received his Ph.D. in Computer Science from Stanford University in 2013. Prior to joining Google, Vaswani worked at Facebook, where he contributed to the development of various deep learning models.\n",
      "\n",
      "Vaswani is known for his work on self-attention mechanisms and transformer-based architectures. His 2017 paper \"Attention is All You Need\" introduced the Transformer model, which has since become a fundamental component of many modern deep learning models.\n",
      "\n",
      "2. **Jason Weston**:\n",
      "Jason Weston is a researcher at Facebook AI, where he works on various NLP tasks, including language modeling, question answering, and text classification. He received his Ph.D. in Computer Science from Stanford University in 2013.\n",
      "\n",
      "Weston has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\n",
      "\n",
      "3. **Stephen Merity**:\n",
      "Stephen Merity is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. He received his Ph.D. in Computer Science from University College London in 2012.\n",
      "\n",
      "Merty has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\n",
      "\n",
      "4. **Kashi Goel (also known as Kashi)**:\n",
      "Kashi Goel is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. He received his Ph.D. in Computer Science from University of California, Berkeley in 2017.\n",
      "\n",
      "Goel has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\n",
      "\n",
      "5. **Ilya Loshchilov**:\n",
      "Ilya Loshchilov is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. He received his Ph.D. in Computer Science from Tel Aviv University in 2017.\n",
      "\n",
      "Loshchilov has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\n",
      "\n",
      "6. **Frank Parham**:\n",
      "Frank Parham is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. However, not much information is publicly available about his background or research contributions.\n",
      "\n",
      "Now that you know more about the individuals who contributed to the development of the Transformer architecture, how can I assist you further? Do you have any specific questions or topics you'd like to discuss?\n",
      "\n",
      "Here are some potential areas we could explore:\n",
      "\n",
      "* The application of transformer-based architectures in NLP\n",
      "* The advantages and limitations of transformer-based models compared to traditional recurrent neural networks (RNNs)\n",
      "* The role of self-attention mechanisms in processing sequential data\n",
      "* The development of new transformer-based architectures for specific NLP tasks\n",
      "\n",
      "Let me know if any of these topics interest you, or if you have something else in mind!\n",
      "AI:  The Transformer architecture is a type of neural network that has revolutionized the field of natural language processing (NLP) and beyond. It's a key component of many modern deep learning models, including Language Models like myself!\n",
      "\n",
      "**What makes Transformers different?**\n",
      "\n",
      "Traditional recurrent neural networks (RNNs) are designed to process sequential data one step at a time. They use an internal memory mechanism to keep track of the input sequence and make predictions based on that memory.\n",
      "\n",
      "However, RNNs have limitations, such as:\n",
      "\n",
      "* The vanishing gradient problem: gradients become smaller as they flow backwards through the network, making it difficult for deep models to learn.\n",
      "* Limited ability to model long-range dependencies in sequential data.\n",
      "\n",
      "Transformers, on the other hand, use self-attention mechanisms to process sequential data. This allows them to focus on all parts of the input sequence simultaneously and weigh their importance relative to each other.\n",
      "\n",
      "**Key components of Transformers:**\n",
      "\n",
      "1. **Self-Attention Mechanism:** The core component of the Transformer architecture is the self-attention mechanism. It takes in three inputs:\n",
      "\t* Query (Q): A vector that represents the current position in the input sequence.\n",
      "\t* Key (K): A vector that represents the value or importance of each element in the input sequence.\n",
      "\t* Value (V): A vector that represents the actual values or outputs associated with each element.\n",
      "\n",
      "The self-attention mechanism calculates a weighted sum of all possible interactions between these vectors, which allows it to focus on the most relevant parts of the input sequence.\n",
      "\n",
      "2. **Encoder-Decoder Architecture:** The Transformer architecture typically consists of two separate blocks:\n",
      "\t* Encoder: Takes in a sequence of tokens and outputs a sequence of vectors representing the input sequence.\n",
      "\t* Decoder: Takes in the output from the encoder and generates a sequence of tokens that represents the desired output.\n",
      "\n",
      "3. **Multi-Head Attention:** To handle the challenge of very long sequences, the Transformer architecture uses multi-head attention. This involves applying multiple self-attention mechanisms in parallel, each with its own set of weights.\n",
      "\n",
      "4. **Feed Forward Networks (FFNs):** The Transformer architecture also includes FFNs to handle the vanishing gradient problem. These networks are used as a bridge between the encoder and decoder blocks.\n",
      "\n",
      "**How Transformers work:**\n",
      "\n",
      "Here's a high-level overview of how the Transformer architecture works:\n",
      "\n",
      "1. Input sequence: A sequence of tokens, such as words or characters.\n",
      "2. Encoder:\n",
      "\t* Split the input sequence into smaller sub-sequences (called \"embeddings\").\n",
      "\t* Apply self-attention mechanisms to each sub-sequence separately.\n",
      "\t* Output a sequence of vectors representing the input sequence.\n",
      "3. Decoder:\n",
      "\t* Take in the output from the encoder and split it into smaller sub-sequences.\n",
      "\t* Apply self-attention mechanisms to each sub-sequence separately.\n",
      "\t* Use FFNs to process the output.\n",
      "4. Output: The final output is generated by taking the dot product of the decoder's output with a set of weights.\n",
      "\n",
      "**Advantages of Transformers:**\n",
      "\n",
      "Transformers have several advantages over traditional RNNs:\n",
      "\n",
      "* **Long-range dependencies:** Transformers can model long-range dependencies in sequential data, making them suitable for tasks like machine translation and text summarization.\n",
      "* **Parallelization:** Transformers can be parallelized more easily than RNNs, which makes them more efficient to train on large datasets.\n",
      "* **Scalability:** Transformers can handle much longer input sequences than RNNs, making them suitable for tasks that require modeling long-term dependencies.\n",
      "\n",
      "**Limitations of Transformers:**\n",
      "\n",
      "While Transformers have many advantages, they also have some limitations:\n",
      "\n",
      "* **Computational cost:** Transformers require more computational resources than traditional RNNs due to their self-attention mechanisms.\n",
      "* **Difficulty in optimizing:** Transformer models can be difficult to optimize due to the complexity of their architecture.\n",
      "\n",
      "I hope this helps you understand the basics of the Transformer architecture! Do you have any specific questions or would you like me to elaborate on any of these points?\n",
      "['hi how are you ', \"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\", 'who is the founder off tranformer architecture which is the base for llms', 'Nice to \"meet\" you too!\\n\\nThe founder of the Transformer architecture is Ashish Vaswani, but it was originally developed by a team of researchers at Google Brain, including:\\n\\n1. Jason Weston\\n2. Stephen Merity\\n3. Kashi Goel (also known as Kashi)\\n4. Ilya Loshchilov\\n5. Frank Parham\\n\\nHowever, the idea of using self-attention mechanisms to process sequential data was first explored by Vaswani and his team in their 2017 paper \"Attention is All You Need\", which introduced the Transformer architecture.\\n\\nThe Transformer architecture has since become a fundamental component of many modern deep learning models, including Language Models like myself!\\n\\nHow can I assist you today? Do you have any specific questions or topics you\\'d like to discuss?', 'can you give me much detailed about these persons', 'Nice to \"meet\" you too!\\n\\nLet\\'s dive deeper into the individuals who contributed to the development of the Transformer architecture:\\n\\n1. **Ashish Vaswani**:\\nAshish Vaswani is a researcher and engineer at Google Brain, where he led the research team that developed the Transformer architecture. He received his Ph.D. in Computer Science from Stanford University in 2013. Prior to joining Google, Vaswani worked at Facebook, where he contributed to the development of various deep learning models.\\n\\nVaswani is known for his work on self-attention mechanisms and transformer-based architectures. His 2017 paper \"Attention is All You Need\" introduced the Transformer model, which has since become a fundamental component of many modern deep learning models.\\n\\n2. **Jason Weston**:\\nJason Weston is a researcher at Facebook AI, where he works on various NLP tasks, including language modeling, question answering, and text classification. He received his Ph.D. in Computer Science from Stanford University in 2013.\\n\\nWeston has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\\n\\n3. **Stephen Merity**:\\nStephen Merity is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. He received his Ph.D. in Computer Science from University College London in 2012.\\n\\nMerty has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\\n\\n4. **Kashi Goel (also known as Kashi)**:\\nKashi Goel is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. He received his Ph.D. in Computer Science from University of California, Berkeley in 2017.\\n\\nGoel has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\\n\\n5. **Ilya Loshchilov**:\\nIlya Loshchilov is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. He received his Ph.D. in Computer Science from Tel Aviv University in 2017.\\n\\nLoshchilov has made significant contributions to the field of NLP, particularly in the area of transformer-based architectures. He was a key contributor to the development of the Transformer model and has since published numerous papers on related topics.\\n\\n6. **Frank Parham**:\\nFrank Parham is a researcher at Google Brain, where he works on various deep learning projects, including natural language processing and computer vision. However, not much information is publicly available about his background or research contributions.\\n\\nNow that you know more about the individuals who contributed to the development of the Transformer architecture, how can I assist you further? Do you have any specific questions or topics you\\'d like to discuss?\\n\\nHere are some potential areas we could explore:\\n\\n* The application of transformer-based architectures in NLP\\n* The advantages and limitations of transformer-based models compared to traditional recurrent neural networks (RNNs)\\n* The role of self-attention mechanisms in processing sequential data\\n* The development of new transformer-based architectures for specific NLP tasks\\n\\nLet me know if any of these topics interest you, or if you have something else in mind!', 'can you explain me about this architecture ', 'The Transformer architecture is a type of neural network that has revolutionized the field of natural language processing (NLP) and beyond. It\\'s a key component of many modern deep learning models, including Language Models like myself!\\n\\n**What makes Transformers different?**\\n\\nTraditional recurrent neural networks (RNNs) are designed to process sequential data one step at a time. They use an internal memory mechanism to keep track of the input sequence and make predictions based on that memory.\\n\\nHowever, RNNs have limitations, such as:\\n\\n* The vanishing gradient problem: gradients become smaller as they flow backwards through the network, making it difficult for deep models to learn.\\n* Limited ability to model long-range dependencies in sequential data.\\n\\nTransformers, on the other hand, use self-attention mechanisms to process sequential data. This allows them to focus on all parts of the input sequence simultaneously and weigh their importance relative to each other.\\n\\n**Key components of Transformers:**\\n\\n1. **Self-Attention Mechanism:** The core component of the Transformer architecture is the self-attention mechanism. It takes in three inputs:\\n\\t* Query (Q): A vector that represents the current position in the input sequence.\\n\\t* Key (K): A vector that represents the value or importance of each element in the input sequence.\\n\\t* Value (V): A vector that represents the actual values or outputs associated with each element.\\n\\nThe self-attention mechanism calculates a weighted sum of all possible interactions between these vectors, which allows it to focus on the most relevant parts of the input sequence.\\n\\n2. **Encoder-Decoder Architecture:** The Transformer architecture typically consists of two separate blocks:\\n\\t* Encoder: Takes in a sequence of tokens and outputs a sequence of vectors representing the input sequence.\\n\\t* Decoder: Takes in the output from the encoder and generates a sequence of tokens that represents the desired output.\\n\\n3. **Multi-Head Attention:** To handle the challenge of very long sequences, the Transformer architecture uses multi-head attention. This involves applying multiple self-attention mechanisms in parallel, each with its own set of weights.\\n\\n4. **Feed Forward Networks (FFNs):** The Transformer architecture also includes FFNs to handle the vanishing gradient problem. These networks are used as a bridge between the encoder and decoder blocks.\\n\\n**How Transformers work:**\\n\\nHere\\'s a high-level overview of how the Transformer architecture works:\\n\\n1. Input sequence: A sequence of tokens, such as words or characters.\\n2. Encoder:\\n\\t* Split the input sequence into smaller sub-sequences (called \"embeddings\").\\n\\t* Apply self-attention mechanisms to each sub-sequence separately.\\n\\t* Output a sequence of vectors representing the input sequence.\\n3. Decoder:\\n\\t* Take in the output from the encoder and split it into smaller sub-sequences.\\n\\t* Apply self-attention mechanisms to each sub-sequence separately.\\n\\t* Use FFNs to process the output.\\n4. Output: The final output is generated by taking the dot product of the decoder\\'s output with a set of weights.\\n\\n**Advantages of Transformers:**\\n\\nTransformers have several advantages over traditional RNNs:\\n\\n* **Long-range dependencies:** Transformers can model long-range dependencies in sequential data, making them suitable for tasks like machine translation and text summarization.\\n* **Parallelization:** Transformers can be parallelized more easily than RNNs, which makes them more efficient to train on large datasets.\\n* **Scalability:** Transformers can handle much longer input sequences than RNNs, making them suitable for tasks that require modeling long-term dependencies.\\n\\n**Limitations of Transformers:**\\n\\nWhile Transformers have many advantages, they also have some limitations:\\n\\n* **Computational cost:** Transformers require more computational resources than traditional RNNs due to their self-attention mechanisms.\\n* **Difficulty in optimizing:** Transformer models can be difficult to optimize due to the complexity of their architecture.\\n\\nI hope this helps you understand the basics of the Transformer architecture! Do you have any specific questions or would you like me to elaborate on any of these points?', 'exit']\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    chat_history.append(user_input)\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    result = model.invoke(chat_history)\n",
    "    chat_history.append(result.content)\n",
    "    print(\"AI: \",result.content)\n",
    "\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f7f02",
   "metadata": {},
   "source": [
    "So as we can see know this is much interactive and can refer to our previous chats this is like few shot prompting but with a twist ,this is dynamically working here we are not giving the examples like few shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee58be3",
   "metadata": {},
   "source": [
    "BUT ,here is one problem is that as we can see in last line of chat history actually the message are not well structured ,it is difficult to classify which message is from whcih side , and because of this unstructred input and output may be our model not work well,so now we will optimize it for well structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10908273",
   "metadata": {},
   "source": [
    "And for this well structured data langchain providede uss some tools which we can directly use for any model we dont have to worry about the diff model has different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f413dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Hello! How can I assist you today? Do you have a question, problem, or just want to chat? I'm here to help.\n",
      "AI:  The answer is not as straightforward as it seems.\n",
      "\n",
      "In many mathematical contexts, such as arithmetic and algebra, the number 0 is considered \"bigger\" than any positive number, including 2. This is because 0 is the additive identity, meaning that when you add 0 to any other number, the result is always that same number.\n",
      "\n",
      "So, in a sense, the answer is 0.\n",
      "\n",
      "However, if we're talking about comparing two numbers of different types (e.g., integers and floats), or considering numerical magnitude in certain contexts (like physical quantities or sizes), then the comparison depends on the specific definition or context.\n",
      "\n",
      "In general, though, when it comes to basic arithmetic and everyday comparisons, 0 is considered \"bigger\" than most positive numbers.\n",
      "\n",
      "Would you like me to explain further?\n",
      "AI:  In that case, I'd say that 2 is indeed greater than 0.\n",
      "\n",
      "In numerical order, the sequence goes: ..., -3, -2, -1, 0, 1, 2, ...\n",
      "\n",
      "So, if we're comparing two numbers and one of them is zero, the other number (in this case, 2) is always greater.\n",
      "AI:  You want to multiply the \"bigger\" number (which I defined as 2) by 10.\n",
      "\n",
      "So, the calculation would be:\n",
      "\n",
      "2 × 10 = 20\n",
      "\n",
      "The result is 20!\n",
      "AI:  Déjà vu!\n",
      "\n",
      "You're referring to the \"bigger\" number (which I defined as 0) again. However, since 0 multiplied by any number is always 0, we get:\n",
      "\n",
      "0 × 10 = 0\n",
      "\n",
      "The result is still 0!\n",
      "AI:  You're referring to the \"greater\" number, which I defined as 2.\n",
      "\n",
      "So, multiplying 2 by 10 gives us:\n",
      "\n",
      "2 × 10 = 20\n",
      "\n",
      "Is that what you were thinking of?\n",
      "[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! How can I assist you today? Do you have a question, problem, or just want to chat? I'm here to help.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='which is bigger number 2 or 0', additional_kwargs={}, response_metadata={}), AIMessage(content='The answer is not as straightforward as it seems.\\n\\nIn many mathematical contexts, such as arithmetic and algebra, the number 0 is considered \"bigger\" than any positive number, including 2. This is because 0 is the additive identity, meaning that when you add 0 to any other number, the result is always that same number.\\n\\nSo, in a sense, the answer is 0.\\n\\nHowever, if we\\'re talking about comparing two numbers of different types (e.g., integers and floats), or considering numerical magnitude in certain contexts (like physical quantities or sizes), then the comparison depends on the specific definition or context.\\n\\nIn general, though, when it comes to basic arithmetic and everyday comparisons, 0 is considered \"bigger\" than most positive numbers.\\n\\nWould you like me to explain further?', additional_kwargs={}, response_metadata={}), HumanMessage(content='i mean which is greater ', additional_kwargs={}, response_metadata={}), AIMessage(content=\"In that case, I'd say that 2 is indeed greater than 0.\\n\\nIn numerical order, the sequence goes: ..., -3, -2, -1, 0, 1, 2, ...\\n\\nSo, if we're comparing two numbers and one of them is zero, the other number (in this case, 2) is always greater.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='ok multiply that bigger number with 10 ', additional_kwargs={}, response_metadata={}), AIMessage(content='You want to multiply the \"bigger\" number (which I defined as 2) by 10.\\n\\nSo, the calculation would be:\\n\\n2 × 10 = 20\\n\\nThe result is 20!', additional_kwargs={}, response_metadata={}), HumanMessage(content='so multiply that greater number with 10 ', additional_kwargs={}, response_metadata={}), AIMessage(content='Déjà vu!\\n\\nYou\\'re referring to the \"bigger\" number (which I defined as 0) again. However, since 0 multiplied by any number is always 0, we get:\\n\\n0 × 10 = 0\\n\\nThe result is still 0!', additional_kwargs={}, response_metadata={}), HumanMessage(content='i am talking about that greater number in numerical order ', additional_kwargs={}, response_metadata={}), AIMessage(content='You\\'re referring to the \"greater\" number, which I defined as 2.\\n\\nSo, multiplying 2 by 10 gives us:\\n\\n2 × 10 = 20\\n\\nIs that what you were thinking of?', additional_kwargs={}, response_metadata={}), HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "chat_history = [\n",
    "    SystemMessage(content='You are a helpful AI assistant')\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    result = model.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(content=result.content))\n",
    "    print(\"AI: \",result.content)\n",
    "\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ed893",
   "metadata": {},
   "source": [
    "So now we can see this chatbot is much interactive and clear "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
